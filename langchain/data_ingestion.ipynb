{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading a txt file\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "text_loader = TextLoader(\"data/text-ingestion-hamlet.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_documents = text_loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 0, 'page_label': 'Cover'}, page_content='Tunstall, \\n von Werra \\n & Wolf\\nLewis Tunstall,\\nLeandro von Werra  \\n& Thomas Wolf\\nNatural Language \\nProcessing with  \\n Transformers\\nBuilding Language Applications  \\nwith Hugging Face\\nRevised\\n \\n Edition'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 1, 'page_label': 'A'}, page_content='MACHINE LEARNING\\n“The preeminent book \\nfor the preeminent \\ntransformers library— \\na model of clarity!”\\n—Jeremy Howard\\nCofounder of fast.ai and professor at \\nUniversity of Queensland\\n“A wonderfully clear and \\nincisive guide to modern \\nNLP’s most essential \\nlibrary. Recommended!”\\n—Christopher Manning\\nThomas M. Siebel Professor in Machine \\nLearning, Stanford University\\nNatural Language Processing  \\nwith Transformers\\nISBN: 978-1-098-10324-8\\nUS $59 .99  CAN $79 .99\\nTwitter: @oreillymedia\\nlinkedin.com/company/oreilly-media\\nyoutube.com/oreillymedia \\nSince their introduction in 2017, transformers have quickly \\nbecome the dominant architecture for achieving state-of-the-\\nart results on a variety of natural language processing tasks. \\nIf you’re a data scientist or coder, this practical book—now \\nrevised in full color—shows you how to train and scale these \\nlarge models using Hugging Face Transformers, a Python-\\nbased deep learning library.\\nTransformers have been used to write realistic news stories, \\nimprove Google Search queries, and even create chatbots \\nthat tell corny jokes. In this guide, authors Lewis Tunstall, \\nLeandro von Werra, and Thomas Wolf, among the creators \\nof Hugging Face Transformers, use a hands-on approach to \\nteach you how transformers work and how to integrate them \\nin your applications. You’ll quickly learn a variety of tasks they \\ncan help you solve.\\n• Build, debug, and optimize transformer models for core NLP \\ntasks, such as text classification, named entity recognition, \\nand question answering\\n• Learn how transformers can be used for cross-lingual \\ntransfer learning\\n• Apply transformers in real-world scenarios where labeled \\ndata is scarce\\n• Make transformer models efficient for deployment using \\ntechniques such as distillation, pruning, and quantization \\n• Train transformers from scratch and learn how to scale to \\nmultiple GPUs and distributed environments\\nLewis Tunstall   is a machine learning \\nengineer at Hugging Face. His current \\nwork focuses on developing tools \\nfor the NLP community and teaching \\npeople to use them effectively.\\nLeandro von Werra  is a machine \\nlearning engineer in the open source \\nteam at Hugging Face, where he \\nprimarily works on code generation \\nmodels and community outreach. \\nThomas Wolf is chief science officer \\nand cofounder of Hugging Face. His \\nteam is on a mission to catalyze and \\ndemocratize AI research.\\nTunstall, \\n von Werra \\n & Wolf\\nISBN: 978-1-098-13679-6\\nUS $59.99  CAN $74.99'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 2, 'page_label': 'i'}, page_content='Praise for Natural Language Processing\\nwith Transformers\\nPretrained transformer language models have taken the NLP world by storm, while\\nlibraries such as \\n  Transformers have made them much easier to use. Who better to\\nteach you how to leverage the latest breakthroughs in NLP than the creators of said\\nlibrary? Natural Language Processing with Transformers is a tour de force, reflecting the\\ndeep subject matter expertise of its authors in both engineering and research. It is the rare\\nbook that offers both substantial breadth and depth of insight and deftly mixes research\\nadvances with real-world applications in an accessible way. The book gives informed\\ncoverage of the most important methods and applications in current NLP , from\\nmultilingual to efficient models and from question answering to text generation. Each\\nchapter provides a nuanced overview grounded in rich code examples that highlights best\\npractices as well as practical considerations and enables you to put research-focused\\nmodels to impactful real-world use. Whether you’re new to NLP or a veteran, this book\\nwill improve your understanding and fast-track your development and deployment\\nof state-of-the-art models.\\n—Sebastian Ruder, Google DeepMind\\nTransformers have changed how we do NLP , and Hugging Face has pioneered how we use\\ntransformers in product and research. Lewis Tunstall, Leandro von Werra, and Thomas\\nWolf from Hugging Face have written a timely volume providing a convenient and\\nhands-on introduction to this critical topic. The book offers a solid conceptual grounding\\nof transformer mechanics, a tour of the transformer menagerie, applications of\\ntransformers, and practical issues in training and bringing transformers to production.\\nHaving read chapters in this book, with the depth of its content and lucid presentation, I\\nam confident that this will be the number one resource for anyone interested in learning\\ntransformers, particularly for natural language processing.\\n—Delip Rao, Author of Natural Language Processing and\\nDeep Learning with PyTorch'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 3, 'page_label': 'ii'}, page_content='Complexity made simple. This is a rare and precious book about NLP , transformers, and\\nthe growing ecosystem around them, Hugging Face. Whether these are still buzzwords to\\nyou or you already have a solid grasp of it all, the authors will navigate you with humor,\\nscientific rigor, and plenty of code examples into the deepest secrets of the coolest\\ntechnology around. From “off-the-shelf pretrained” to “from-scratch custom” models,\\nand from performance to missing labels issues, the authors address practically every real-\\nlife struggle of a ML engineer and provide state-of-the-art solutions, making this book\\ndestined to dictate the standards in the field for years to come.\\n—Luca Perrozzi, PhD, Data Science and Machine Learning\\nAssociate Manager at Accenture'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 4, 'page_label': 'iii'}, page_content='Lewis Tunstall, Leandro von Werra, and Thomas Wolf\\nForeword by Aurélien Géron\\nNatural Language Processing\\nwith Transformers\\nBuilding Language Applications\\nwith Hugging Face\\nREVISED EDITION\\nBoston Farnham Sebastopol TokyoBeijing Boston Farnham Sebastopol TokyoBeijing'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 5, 'page_label': 'iv'}, page_content='978-1-098-13679-6\\n[LSI]\\nNatural Language Processing with Transformers\\nby Lewis Tunstall, Leandro von Werra, and Thomas Wolf\\nCopyright © 2022 Lewis Tunstall, Leandro von Werra, and Thomas Wolf. All rights reserved.\\nPrinted in the United States of America.\\nPublished by O’Reilly Media, Inc., 1005 Gravenstein Highway North, Sebastopol, CA 95472.\\nO’Reilly books may be purchased for educational, business, or sales promotional use. Online editions are\\nalso available for most titles (http://oreilly.com). For more information, contact our corporate/institutional\\nsales department: 800-998-9938 or corporate@oreilly.com.\\nAcquisitions Editor: Rebecca Novack\\nDevelopment Editor: Melissa Potter\\nProduction Editor: Katherine Tozer\\nCopyeditor: Rachel Head\\nProofreader: Kim Cofer\\nIndexer: Potomac Indexing, LLC\\nInterior Designer: David Futato\\nCover Designer: Karen Montgomery\\nIllustrator: Christa Lanz\\nFebruary 2022:  First Edition\\nMay 2022:  Revised Color Edition\\nRevision History for the Revised Edition\\n2022-05-27: First Release\\nSee http://oreilly.com/catalog/errata.csp?isbn=9781098136796 for release details.\\nThe O’Reilly logo is a registered trademark of O’Reilly Media, Inc. Natural Language Processing with\\nTransformers, the cover image, and related trade dress are trademarks of O’Reilly Media, Inc.\\nThe views expressed in this work are those of the authors and do not represent the publisher’s views.\\nWhile the publisher and the authors have used good faith efforts to ensure that the information and\\ninstructions contained in this work are accurate, the publisher and the authors disclaim all responsibility\\nfor errors or omissions, including without limitation responsibility for damages resulting from the use of\\nor reliance on this work. Use of the information and instructions contained in this work is at your own\\nrisk. If any code samples or other technology this work contains or describes is subject to open source\\nlicenses or the intellectual property rights of others, it is your responsibility to ensure that your use\\nthereof complies with such licenses and/or rights.'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 6, 'page_label': 'v'}, page_content='Table of Contents\\nForeword. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  xi\\nPreface. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  xv\\n1. Hello Transformers. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  1\\nThe Encoder-Decoder Framework                                                                                  2\\nAttention Mechanisms                                                                                                      4\\nTransfer Learning in NLP                                                                                                 6\\nHugging Face Transformers: Bridging the Gap                                                             9\\nA Tour of Transformer Applications                                                                             10\\nText Classification                                                                                                        10\\nNamed Entity Recognition                                                                                         11\\nQuestion Answering                                                                                                    12\\nSummarization                                                                                                             13\\nTranslation                                                                                                                     13\\nText Generation                                                                                                            14\\nThe Hugging Face Ecosystem                                                                                        15\\nThe Hugging Face Hub                                                                                                16\\nHugging Face Tokenizers                                                                                            17\\nHugging Face Datasets                                                                                                 18\\nHugging Face Accelerate                                                                                             18\\nMain Challenges with Transformers                                                                             19\\nConclusion                                                                                                                        20\\n2. Text Classification. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  21\\nThe Dataset                                                                                                                       22\\nA First Look at Hugging Face Datasets                                                                     23\\nFrom Datasets to DataFrames                                                                                    26\\nv'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 7, 'page_label': 'vi'}, page_content='Looking at the Class Distribution                                                                              27\\nHow Long Are Our Tweets?                                                                                       28\\nFrom Text to Tokens                                                                                                        29\\nCharacter Tokenization                                                                                               29\\nWord Tokenization                                                                                                       31\\nSubword Tokenization                                                                                                 33\\nTokenizing the Whole Dataset                                                                                   35\\nTraining a Text Classifier                                                                                                36\\nTransformers as Feature Extractors                                                                           38\\nFine-Tuning Transformers                                                                                          45\\nConclusion                                                                                                                        54\\n3. Transformer Anatomy. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  57\\nThe Transformer Architecture                                                                                       57\\nThe Encoder                                                                                                                     60\\nSelf-Attention                                                                                                                61\\nThe Feed-Forward Layer                                                                                             70\\nAdding Layer Normalization                                                                                      71\\nPositional Embeddings                                                                                                73\\nAdding a Classification Head                                                                                     75\\nThe Decoder                                                                                                                     76\\nMeet the Transformers                                                                                                    78\\nThe Transformer Tree of Life                                                                                     78\\nThe Encoder Branch                                                                                                    79\\nThe Decoder Branch                                                                                                    82\\nThe Encoder-Decoder Branch                                                                                    83\\nConclusion                                                                                                                        84\\n4. Multilingual Named Entity Recognition. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  87\\nThe Dataset                                                                                                                       88\\nMultilingual Transformers                                                                                             92\\nA Closer Look at Tokenization                                                                                       93\\nThe Tokenizer Pipeline                                                                                                94\\nThe SentencePiece Tokenizer                                                                                     95\\nTransformers for Named Entity Recognition                                                              96\\nThe Anatomy of the Transformers Model Class                                                         98\\nBodies and Heads                                                                                                         98\\nCreating a Custom Model for Token Classification                                                99\\nLoading a Custom Model                                                                                          101\\nTokenizing Texts for NER                                                                                             103\\nPerformance Measures                                                                                                  105\\nFine-Tuning XLM-RoBERTa                                                                                       106\\nvi | Table of Contents'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 8, 'page_label': 'vii'}, page_content='Error Analysis                                                                                                                 108\\nCross-Lingual Transfer                                                                                                 115\\nWhen Does Zero-Shot Transfer Make Sense?                                                        116\\nFine-Tuning on Multiple Languages at Once                                                         118\\nInteracting with Model Widgets                                                                                  121\\nConclusion                                                                                                                      121\\n5. Text Generation. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  123\\nThe Challenge with Generating Coherent Text                                                         125\\nGreedy Search Decoding                                                                                              127\\nBeam Search Decoding                                                                                                 130\\nSampling Methods                                                                                                         134\\nTop-k and Nucleus Sampling                                                                                       136\\nWhich Decoding Method Is Best?                                                                               140\\nConclusion                                                                                                                      140\\n6. Summarization. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  141\\nThe CNN/DailyMail Dataset                                                                                        141\\nText Summarization Pipelines                                                                                     143\\nSummarization Baseline                                                                                            143\\nGPT-2                                                                                                                           144\\nT5                                                                                                                                  144\\nBART                                                                                                                            145\\nPEGASUS                                                                                                                    145\\nComparing Different Summaries                                                                                146\\nMeasuring the Quality of Generated Text                                                                  148\\nBLEU                                                                                                                            148\\nROUGE                                                                                                                        152\\nEvaluating PEGASUS on the CNN/DailyMail Dataset                                            154\\nTraining a Summarization Model                                                                               157\\nEvaluating PEGASUS on SAMSum                                                                         158\\nFine-Tuning PEGASUS                                                                                             158\\nGenerating Dialogue Summaries                                                                             162\\nConclusion                                                                                                                      163\\n7. Question Answering. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  165\\nBuilding a Review-Based QA System                                                                          166\\nThe Dataset                                                                                                                 167\\nExtracting Answers from Text                                                                                  173\\nUsing Haystack to Build a QA Pipeline                                                                   181\\nImproving Our QA Pipeline                                                                                        189\\nEvaluating the Retriever                                                                                            189\\nTable of Contents | vii'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 9, 'page_label': 'viii'}, page_content='Evaluating the Reader                                                                                                196\\nDomain Adaptation                                                                                                   199\\nEvaluating the Whole QA Pipeline                                                                          203\\nGoing Beyond Extractive QA                                                                                       205\\nConclusion                                                                                                                      207\\n8. Making Transformers Efficient in Production. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  209\\nIntent Detection as a Case Study                                                                                 210\\nCreating a Performance Benchmark                                                                           212\\nMaking Models Smaller via Knowledge Distillation                                                217\\nKnowledge Distillation for Fine-Tuning                                                                 217\\nKnowledge Distillation for Pretraining                                                                  220\\nCreating a Knowledge Distillation Trainer                                                             220\\nChoosing a Good Student Initialization                                                                 222\\nFinding Good Hyperparameters with Optuna                                                      226\\nBenchmarking Our Distilled Model                                                                        229\\nMaking Models Faster with Quantization                                                                  230\\nBenchmarking Our Quantized Model                                                                        236\\nOptimizing Inference with ONNX and the ONNX Runtime                                 237\\nMaking Models Sparser with Weight Pruning                                                          243\\nSparsity in Deep Neural Networks                                                                          244\\nWeight Pruning Methods                                                                                          244\\nConclusion                                                                                                                      248\\n9. Dealing with Few to No Labels. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  249\\nBuilding a GitHub Issues Tagger                                                                                 251\\nGetting the Data                                                                                                         252\\nPreparing the Data                                                                                                     253\\nCreating Training Sets                                                                                               257\\nCreating Training Slices                                                                                            259\\nImplementing a Naive Bayesline                                                                                 260\\nWorking with No Labeled Data                                                                                   263\\nWorking with a Few Labels                                                                                          271\\nData Augmentation                                                                                                    271\\nUsing Embeddings as a Lookup Table                                                                    275\\nFine-Tuning a Vanilla Transformer                                                                         284\\nIn-Context and Few-Shot Learning with Prompts                                                288\\nLeveraging Unlabeled Data                                                                                          289\\nFine-Tuning a Language Model                                                                               289\\nFine-Tuning a Classifier                                                                                            293\\nAdvanced Methods                                                                                                    295\\nConclusion                                                                                                                      297\\nviii | Table of Contents'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 10, 'page_label': 'ix'}, page_content='10. Training Transformers from Scratch. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  299\\nLarge Datasets and Where to Find Them                                                                  300\\nChallenges of Building a Large-Scale Corpus                                                        300\\nBuilding a Custom Code Dataset                                                                             303\\nWorking with Large Datasets                                                                                   306\\nAdding Datasets to the Hugging Face Hub                                                            309\\nBuilding a Tokenizer                                                                                                     310\\nThe Tokenizer Model                                                                                                 312\\nMeasuring Tokenizer Performance                                                                         312\\nA Tokenizer for Python                                                                                             313\\nTraining a Tokenizer                                                                                                  318\\nSaving a Custom Tokenizer on the Hub                                                                 322\\nTraining a Model from Scratch                                                                                    323\\nA Tale of Pretraining Objectives                                                                              323\\nInitializing the Model                                                                                                325\\nImplementing the Dataloader                                                                                  326\\nDefining the Training Loop                                                                                      330\\nThe Training Run                                                                                                       337\\nResults and Analysis                                                                                                      338\\nConclusion                                                                                                                      343\\n11. Future Directions. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  345\\nScaling Transformers                                                                                                     345\\nScaling Laws                                                                                                                347\\nChallenges with Scaling                                                                                             349\\nAttention Please!                                                                                                        351\\nSparse Attention                                                                                                         352\\nLinearized Attention                                                                                                  353\\nGoing Beyond Text                                                                                                        354\\nVision                                                                                                                           355\\nTables                                                                                                                           359\\nMultimodal Transformers                                                                                            361\\nSpeech-to-Text                                                                                                            361\\nVision and Text                                                                                                           364\\nWhere to from Here?                                                                                                    370\\nIndex. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  371\\nTable of Contents | ix'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 11, 'page_label': 'x'}, page_content=''), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 12, 'page_label': 'xi'}, page_content='1 For brain hygiene tips, see CGP Grey’s excellent video on memes.\\nForeword\\nA miracle is taking place as you read these lines: the squiggles on this page are trans‐\\nforming into words and concepts and emotions as they navigate their way through\\nyour cortex. My thoughts from November 2021 have now successfully invaded your\\nbrain. If they manage to catch your attention and survive long enough in this harsh\\nand highly competitive environment, they may have a chance to reproduce again as\\nyou share these thoughts with others. Thanks to language, thoughts have become air‐\\nborne and highly contagious brain germs—and no vaccine is coming.\\nLuckily, most brain germs are harmless, 1 and a few are wonderfully useful. In fact,\\nhumanity’s brain germs constitute two of our most precious treasures: knowledge and\\nculture. Much as we can’t digest properly without healthy gut bacteria, we cannot\\nthink properly without healthy brain germs. Most of your thoughts are not actually\\nyours: they arose and grew and evolved in many other brains before they infected\\nyou. So if we want to build intelligent machines, we will need to find a way to infect\\nthem too.\\nThe good news is that another miracle has been unfolding over the last few years: sev‐\\neral breakthroughs in deep learning have given birth to powerful language models.\\nSince you are reading this book, you have probably seen some astonishing demos of\\nthese language models, such as GPT-3, which given a short prompt such as “a frog\\nmeets a crocodile” can write a whole story. Although it’s not quite Shakespeare yet, it’s\\nsometimes hard to believe that these texts were written by an artificial neural net‐\\nwork. In fact, GitHub’s Copilot system is helping me write these lines: you’ll never\\nknow how much I really wrote.\\nThe revolution goes far beyond text generation. It encompasses the whole realm of\\nnatural language processing (NLP), from text classification to summarization, trans‐\\nlation, question answering, chatbots, natural language understanding (NLU), and\\nxi'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 13, 'page_label': 'xii'}, page_content='more. Wherever there’s language, speech or text, there’s an application for NLP . Y ou\\ncan already ask your phone for tomorrow’s weather, or chat with a virtual help desk\\nassistant to troubleshoot a problem, or get meaningful results from search engines\\nthat seem to truly understand your query. But the technology is so new that the best\\nis probably yet to come.\\nLike most advances in science, this recent revolution in NLP rests upon the hard\\nwork of hundreds of unsung heroes. But three key ingredients of its success do stand\\nout:\\n• The transformer is a neural network architecture proposed in 2017 in a ground‐\\nbreaking paper called “ Attention Is All Y ou Need”, published by a team of Google\\nresearchers. In just a few years it swept across the field, crushing previous archi‐\\ntectures that were typically based on recurrent neural networks (RNNs). The\\nTransformer architecture is excellent at capturing patterns in long sequences of\\ndata and dealing with huge datasets—so much so that its use is now extending\\nwell beyond NLP , for example to image processing tasks.\\n• In most projects, you won’t have access to a huge dataset to train a model from\\nscratch. Luckily, it’s often possible to download a model that was pretrained on a\\ngeneric dataset: all you need to do then is fine-tune it on your own (much\\nsmaller) dataset. Pretraining has been mainstream in image processing since the\\nearly 2010s, but in NLP it was restricted to contextless word embeddings (i.e.,\\ndense vector representations of individual words). For example, the word “bear”\\nhad the same pretrained embedding in “teddy bear” and in “to bear. ” Then, in\\n2018, several papers proposed full-blown language models that could be pre‐\\ntrained and fine-tuned for a variety of NLP tasks; this completely changed the\\ngame.\\n• Model hubs like Hugging Face’s have also been a game-changer. In the early days,\\npretrained models were just posted anywhere, so it wasn’t easy to find what you\\nneeded. Murphy’s law guaranteed that PyTorch users would only find Tensor‐\\nFlow models, and vice versa. And when you did find a model, figuring out how\\nto fine-tune it wasn’t always easy. This is where Hugging Face’s Transformers\\nlibrary comes in: it’s open source, it supports both TensorFlow and PyTorch, and\\nit makes it easy to download a state-of-the-art pretrained model from the Hug‐\\nging Face Hub, configure it for your task, fine-tune it on your dataset, and evalu‐\\nate it. Use of the library is growing quickly: in Q4 2021 it was used by over five\\nthousand organizations and was installed using pip over four million times per\\nmonth. Moreover, the library and its ecosystem are expanding beyond NLP:\\nimage processing models are available too. Y ou can also download numerous\\ndatasets from the Hub to train or evaluate your models.\\nSo what more can you ask for? Well, this book! It was written by open source devel‐\\nopers at Hugging Face—including the creator of the Transformers library!—and it\\nxii | Foreword'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 14, 'page_label': 'xiii'}, page_content='shows: the breadth and depth of the information you will find in these pages is\\nastounding. It covers everything from the Transformer architecture itself, to the\\nTransformers library and the entire ecosystem around it. I particularly appreciated\\nthe hands-on approach: you can follow along in Jupyter notebooks, and all the code\\nexamples are straight to the point and simple to understand. The authors have exten‐\\nsive experience in training very large transformer models, and they provide a wealth\\nof tips and tricks for getting everything to work efficiently. Last but not least, their\\nwriting style is direct and lively: it reads like a novel.\\nIn short, I thoroughly enjoyed this book, and I’m certain you will too. Anyone inter‐\\nested in building products with state-of-the-art language-processing features needs to\\nread it. It’s packed to the brim with all the right brain germs!\\n— Aurélien Géron\\nNovember 2021, Auckland, NZ\\nForeword | xiii'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 15, 'page_label': 'xiv'}, page_content=''), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 16, 'page_label': 'xv'}, page_content='1 NLP researchers tend to name their creations after characters in Sesame Street. We’ll explain what all these\\nacronyms mean in Chapter 1.\\nPreface\\nSince their introduction in 2017, transformers have become the de facto standard for\\ntackling a wide range of natural language processing (NLP) tasks in both academia\\nand industry. Without noticing it, you probably interacted with a transformer today:\\nGoogle now uses BERT to enhance its search engine by better understanding users’\\nsearch queries. Similarly, the GPT family of models from OpenAI have repeatedly\\nmade headlines in mainstream media for their ability to generate human-like text and\\nimages.1 These transformers now power applications like GitHub’s Copilot, which, as\\nshown in Figure P-1, can convert a comment into source code that automatically cre‐\\nates a neural network for you!\\nSo what is it about transformers that changed the field almost overnight? Like many\\ngreat scientific breakthroughs, it was the synthesis of several ideas, like attention,\\ntransfer learning, and scaling up neural networks, that were percolating in the research\\ncommunity at the time.\\nBut however useful it is, to gain traction in industry any fancy new method needs\\ntools to make it accessible. The \\n  Transformers library and its surrounding ecosys‐\\ntem answered that call by making it easy for practitioners to use, train, and share\\nmodels. This greatly accelerated the adoption of transformers, and the library is now\\nused by over five thousand organizations. Throughout this book we’ll guide you on\\nhow to train and optimize these models for practical applications.\\nxv'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 17, 'page_label': 'xvi'}, page_content='Figure P-1. An example from GitHub Copilot where, given a brief description of the\\ntask, the application provides a suggestion for the entire class (everything following\\nclass is autogenerated)\\nWho Is This Book For?\\nThis book is written for data scientists and machine learning engineers who may have\\nheard about the recent breakthroughs involving transformers, but are lacking an in-\\ndepth guide to help them adapt these models to their own use cases. The book is not\\nmeant to be an introduction to machine learning, and we assume you are comfortable\\nprogramming in Python and has a basic understanding of deep learning frameworks\\nlike PyTorch and TensorFlow. We also assume you have some practical experience\\nwith training models on GPUs. Although the book focuses on the PyTorch API of \\nTransformers, Chapter 2 shows you how to translate all the examples to TensorFlow.\\nThe following resources provide a good foundation for the topics covered in this\\nbook. We assume your technical knowledge is roughly at their level:\\n• Hands-On Machine Learning with Scikit-Learn and TensorFlow , by Aurélien\\nGéron (O’Reilly)\\n• Deep Learning for Coders with fastai and PyTorch, by Jeremy Howard and Sylvain\\nGugger (O’Reilly)\\nxvi | Preface'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 18, 'page_label': 'xvii'}, page_content='• Natural Language Processing with PyTorch , by Delip Rao and Brian McMahan\\n(O’Reilly)\\n• The Hugging Face Course, by the open source team at Hugging Face\\nWhat You Will Learn\\nThe goal of this book is to enable you to build your own language applications. To\\nthat end, it focuses on practical use cases, and delves into theory only where neces‐\\nsary. The style of the book is hands-on, and we highly recommend you experiment by\\nrunning the code examples yourself.\\nThe book covers all the major applications of transformers in NLP by having each\\nchapter (with a few exceptions) dedicated to one task, combined with a realistic use\\ncase and dataset. Each chapter also introduces some additional concepts. Here’s a\\nhigh-level overview of the tasks and topics we’ll cover:\\n• Chapter 1, Hello Transformers, introduces transformers and puts them into con‐\\ntext. It also provides an introduction to the Hugging Face ecosystem.\\n• Chapter 2, Text Classification, focuses on the task of sentiment analysis (a com‐\\nmon text classification problem) and introduces the Trainer API.\\n• Chapter 3 , Transformer Anatomy, dives into the Transformer architecture in\\nmore depth, to prepare you for the chapters that follow.\\n• Chapter 4, Multilingual Named Entity Recognition, focuses on the task of identify‐\\ning entities in texts in multiple languages (a token classification problem).\\n• Chapter 5, Text Generation, explores the ability of transformer models to gener‐\\nate text, and introduces decoding strategies and metrics.\\n• Chapter 6, Summarization, digs into the complex sequence-to-sequence task of\\ntext summarization and explores the metrics used for this task.\\n• Chapter 7 , Question Answering , focuses on building a review-based question\\nanswering system and introduces retrieval with Haystack.\\n• Chapter 8, Making Transformers Efficient in Production, focuses on model perfor‐\\nmance. We’ll look at the task of intent detection (a type of sequence classification\\nproblem) and explore techniques such a knowledge distillation, quantization,\\nand pruning.\\n• Chapter 9, Dealing with Few to No Labels, looks at ways to improve model perfor‐\\nmance in the absence of large amounts of labeled data. We’ll build a GitHub\\nissues tagger and explore techniques such as zero-shot classification and data\\naugmentation.\\nPreface | xvii'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 19, 'page_label': 'xviii'}, page_content='• Chapter 10 , Training Transformers from Scratch , shows you how to build and\\ntrain a model for autocompleting Python source code from scratch. We’ll look at\\ndataset streaming and large-scale training, and build our own tokenizer.\\n• Chapter 11 , Future Directions , explores the challenges transformers face and\\nsome of the exciting new directions that research in this area is going into.\\n Transformers offers several layers of abstraction for using and training trans‐\\nformer models. We’ll start with the easy-to-use pipelines that allow us to pass text\\nexamples through the models and investigate the predictions in just a few lines of\\ncode. Then we’ll move on to tokenizers, model classes, and the Trainer API, which\\nallow us to train models for our own use cases. Later, we’ll show you how to replace\\nthe Trainer with the \\n  Accelerate library, which gives us full control over the train‐\\ning loop and allows us to train large-scale transformers entirely from scratch!\\nAlthough each chapter is mostly self-contained, the difficulty of the tasks increases in\\nthe later chapters. For this reason, we recommend starting with Chapters 1 and 2,\\nbefore branching off into the topic of most interest.\\nBesides \\n  Transformers and \\n  Accelerate, we will also make extensive use of\\n Datasets, which seamlessly integrates with other libraries. \\n  Datasets offers similar\\nfunctionality for data processing as Pandas but is designed from the ground up for\\ntackling large datasets and machine learning.\\nWith these tools, you have everything you need to tackle almost any NLP challenge!\\nSoftware and Hardware Requirements\\nDue to the hands-on approach of this book, we highly recommend that you run the\\ncode examples while you read each chapter. Since we’re dealing with transformers,\\nyou’ll need access to a computer with an NVIDIA GPU to train these models. Fortu‐\\nnately, there are several free online options that you can use, including:\\n• Google Colaboratory\\n• Kaggle Notebooks\\n• Paperspace Gradient Notebooks\\nTo run the examples, you’ll need to follow the installation guide that we provide in\\nthe book’s GitHub repository. Y ou can find this guide and the code examples at\\nhttps://github.com/nlp-with-transformers/notebooks.\\nWe developed most of the chapters using NVIDIA Tesla P100\\nGPUs, which have 16GB of memory. Some of the free platforms\\nprovide GPUs with less memory, so you may need to reduce the\\nbatch size when training the models.\\nxviii | Preface'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 20, 'page_label': 'xix'}, page_content='Conventions Used in This Book\\nThe following typographical conventions are used in this book:\\nItalic\\nIndicates new terms, URLs, email addresses, filenames, and file extensions.\\nConstant width\\nUsed for program listings, as well as within paragraphs to refer to program ele‐\\nments such as variable or function names, databases, data types, environment\\nvariables, statements, and keywords.\\nConstant width bold\\nShows commands or other text that should be typed literally by the user.\\nConstant width italic\\nShows text that should be replaced with user-supplied values or by values deter‐\\nmined by context.\\nThis element signifies a tip or suggestion.\\nThis element signifies a general note.\\nThis element indicates a warning or caution.\\nUsing Code Examples\\nSupplemental material (code examples, exercises, etc.) is available for download at\\nhttps://github.com/nlp-with-transformers/notebooks.\\nIf you have a technical question or a problem using the code examples, please send\\nemail to bookquestions@oreilly.com.\\nThis book is here to help you get your job done. In general, if example code is offered\\nwith this book, you may use it in your programs and documentation. Y ou do not\\nPreface | xix'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 21, 'page_label': 'xx'}, page_content='need to contact us for permission unless you’re reproducing a significant portion of\\nthe code. For example, writing a program that uses several chunks of code from this\\nbook does not require permission. Selling or distributing examples from O’Reilly\\nbooks does require permission. Answering a question by citing this book and quoting\\nexample code does not require permission. Incorporating a significant amount of\\nexample code from this book into your product’s documentation does require\\npermission.\\nWe appreciate, but generally do not require, attribution. An attribution usually\\nincludes the title, author, publisher, and ISBN. For example: “ Natural Language Pro‐\\ncessing with Transformers by Lewis Tunstall, Leandro von Werra, and Thomas Wolf\\n(O’Reilly). Copyright 2022 Lewis Tunstall, Leandro von Werra, and Thomas Wolf,\\n978-1-098-13679-6. ”\\nIf you feel your use of code examples falls outside fair use or the permission given\\nabove, feel free to contact us at permissions@oreilly.com.\\nO’Reilly Online Learning\\nFor more than 40 years, O’Reilly Media has provided technol‐\\nogy and business training, knowledge, and insight to help\\ncompanies succeed.\\nOur unique network of experts and innovators share their knowledge and expertise\\nthrough books, articles, and our online learning platform. O’Reilly’s online learning\\nplatform gives you on-demand access to live training courses, in-depth learning\\npaths, interactive coding environments, and a vast collection of text and video from\\nO’Reilly and 200+ other publishers. For more information, visit http://oreilly.com.\\nHow to Contact Us\\nPlease address comments and questions concerning this book to the publisher:\\nO’Reilly Media, Inc.\\n1005 Gravenstein Highway North\\nSebastopol, CA 95472\\n800-998-9938 (in the United States or Canada)\\n707-829-0515 (international or local)\\n707-829-0104 (fax)\\nWe have a web page for this book, where we list errata, examples, and any additional\\ninformation. Y ou can access this page at https://oreil.ly/nlp-with-transformers.\\nxx | Preface'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 22, 'page_label': 'xxi'}, page_content='Email bookquestions@oreilly.com to comment or ask technical questions about this\\nbook.\\nFor news and information about our books and courses, visit http://oreilly.com.\\nFind us on Facebook: http://facebook.com/oreilly\\nFollow us on Twitter: http://twitter.com/oreillymedia\\nWatch us on Y ouTube: http://youtube.com/oreillymedia\\nAcknowledgments\\nWriting a book about one of the fastest-moving fields in machine learning would not\\nhave been possible without the help of many people. We thank the wonderful O’Reilly\\nteam, and especially Melissa Potter, Rebecca Novack, and Katherine Tozer for their\\nsupport and advice. The book has also benefited from amazing reviewers who spent\\ncountless hours to provide us with invaluable feedback. We are especially grateful to\\nLuca Perozzi, Hamel Husain, Shabie Iqbal, Umberto Lupo, Malte Pietsch, Timo Möl‐\\nler, and Aurélien Géron for their detailed reviews. We thank Branden Chan at deepset\\nfor his help with extending the Haystack library to support the use case in Chapter 7.\\nThe beautiful illustrations in this book are due to the amazing Christa Lanz—thank\\nyou for making this book extra special. We were also fortunate enough to have the\\nsupport of the whole Hugging Face team. Many thanks to Quentin Lhoest for answer‐\\ning countless questions on \\n  Datasets, to Lysandre Debut for help on everything\\nrelated to the Hugging Face Hub, Sylvain Gugger for his help with \\n  Accelerate, and\\nJoe Davison for his inspiration for Chapter 9 with regard to zero-shot learning. We\\nalso thank Sidd Karamcheti and the whole Mistral team for adding stability tweaks\\nfor GPT-2 to make Chapter 10  possible. This book was written entirely in Jupyter\\nNotebooks, and we thank Jeremy Howard and Sylvain Gugger for creating delightful\\ntools like fastdoc that made this possible.\\nLewis\\nTo Sofia, thank you for being a constant source of support and encouragement—\\nwithout both, this book would not exist. After a long stretch of writing, we can finally\\nenjoy our weekends again!\\nLeandro\\nThank you Janine, for your patience and encouraging support during this long year\\nwith many late nights and busy weekends.\\nPreface | xxi'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 23, 'page_label': 'xxii'}, page_content='Thomas\\nI would like to thank first and foremost Lewis and Leandro for coming up with the\\nidea of this book and pushing strongly to produce it in such a beautiful and accessible\\nformat. I would also like to thank all the Hugging Face team for believing in the mis‐\\nsion of AI as a community effort, and the whole NLP/AI community for building and\\nusing the libraries and research we describe in this book together with us.\\nMore than what we build, the journey we take is what really matters, and we have the\\nprivilege to travel this path with thousands of community members and readers like\\nyou today. Thank you all from the bottom of our hearts.\\nxxii | Preface'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 24, 'page_label': '1'}, page_content='1 A. Vaswani et al., “ Attention Is All Y ou Need”, (2017). This title was so catchy that no less than 50 follow-up\\npapers have included “all you need” in their titles!\\n2 J. Howard and S. Ruder, “Universal Language Model Fine-Tuning for Text Classification”, (2018).\\n3 A. Radford et al., “Improving Language Understanding by Generative Pre-Training”, (2018).\\n4 J. Devlin et al., “BERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding”,\\n(2018).\\nCHAPTER 1\\nHello Transformers\\nIn 2017, researchers at Google published a paper that proposed a novel neural net‐\\nwork architecture for sequence modeling. 1 Dubbed the Transformer, this architecture\\noutperformed recurrent neural networks (RNNs) on machine translation tasks, both\\nin terms of translation quality and training cost.\\nIn parallel, an effective transfer learning method called ULMFiT showed that training\\nlong short-term memory (LSTM) networks on a very large and diverse corpus could\\nproduce state-of-the-art text classifiers with little labeled data.2\\nThese advances were the catalysts for two of today’s most well-known transformers:\\nthe Generative Pretrained Transformer (GPT) 3 and Bidirectional Encoder Represen‐\\ntations from Transformers (BERT).4 By combining the Transformer architecture with\\nunsupervised learning, these models removed the need to train task-specific architec‐\\ntures from scratch and broke almost every benchmark in NLP by a significant mar‐\\ngin. Since the release of GPT and BERT, a zoo of transformer models has emerged; a\\ntimeline of the most prominent entries is shown in Figure 1-1.\\n1'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 25, 'page_label': '2'}, page_content='Figure 1-1. The transformers timeline\\nBut we’re getting ahead of ourselves. To understand what is novel about transformers,\\nwe first need to explain:\\n• The encoder-decoder framework\\n• Attention mechanisms\\n• Transfer learning\\nIn this chapter we’ll introduce the core concepts that underlie the pervasiveness of\\ntransformers, take a tour of some of the tasks that they excel at, and conclude with a\\nlook at the Hugging Face ecosystem of tools and libraries.\\nLet’s start by exploring the encoder-decoder framework and the architectures that\\npreceded the rise of transformers.\\nThe Encoder-Decoder Framework\\nPrior to transformers, recurrent architectures such as LSTMs were the state of the art\\nin NLP . These architectures contain a feedback loop in the network connections that\\nallows information to propagate from one step to another, making them ideal for\\nmodeling sequential data like text. As illustrated on the left side of Figure 1-2 , an\\nRNN receives some input (which could be a word or character), feeds it through the\\nnetwork, and outputs a vector called the hidden state. At the same time, the model\\nfeeds some information back to itself through the feedback loop, which it can then\\nuse in the next step. This can be more clearly seen if we “unroll” the loop as shown on\\nthe right side of Figure 1-2: the RNN passes information about its state at each step to\\nthe next operation in the sequence. This allows an RNN to keep track of information\\nfrom previous steps, and use it for its output predictions.\\n2 | Chapter 1: Hello Transformers'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 26, 'page_label': '3'}, page_content='5 I. Sutskever, O. Vinyals, and Q.V . Le, “Sequence to Sequence Learning with Neural Networks”, (2014).\\nFigure 1-2. Unrolling an RNN in time\\nThese architectures were (and continue to be) widely used for NLP tasks, speech pro‐\\ncessing, and time series. Y ou can find a wonderful exposition of their capabilities in\\nAndrej Karpathy’s blog post, “The Unreasonable Effectiveness of Recurrent Neural\\nNetworks”.\\nOne area where RNNs played an important role was in the development of machine\\ntranslation systems, where the objective is to map a sequence of words in one lan‐\\nguage to another. This kind of task is usually tackled with an encoder-decoder or\\nsequence-to-sequence architecture,5 which is well suited for situations where the input\\nand output are both sequences of arbitrary length. The job of the encoder is to\\nencode the information from the input sequence into a numerical representation that\\nis often called the last hidden state . This state is then passed to the decoder, which\\ngenerates the output sequence.\\nIn general, the encoder and decoder components can be any kind of neural network\\narchitecture that can model sequences. This is illustrated for a pair of RNNs in\\nFigure 1-3, where the English sentence “Transformers are great!” is encoded as a hid‐\\nden state vector that is then decoded to produce the German translation “Trans‐\\nformer sind grossartig!” The input words are fed sequentially through the encoder\\nand the output words are generated one at a time, from top to bottom.\\nThe Encoder-Decoder Framework | 3'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 27, 'page_label': '4'}, page_content='6 D. Bahdanau, K. Cho, and Y . Bengio, “Neural Machine Translation by Jointly Learning to Align and Trans‐\\nlate”, (2014).\\nFigure 1-3. An encoder-decoder architecture with a pair of RNNs (in general, there are\\nmany more recurrent layers than those shown here)\\nAlthough elegant in its simplicity, one weakness of this architecture is that the final\\nhidden state of the encoder creates an information bottleneck: it has to represent the\\nmeaning of the whole input sequence because this is all the decoder has access to\\nwhen generating the output. This is especially challenging for long sequences, where\\ninformation at the start of the sequence might be lost in the process of compressing\\neverything to a single, fixed representation.\\nFortunately, there is a way out of this bottleneck by allowing the decoder to have\\naccess to all of the encoder’s hidden states. The general mechanism for this is called\\nattention,6 and it is a key component in many modern neural network architectures.\\nUnderstanding how attention was developed for RNNs will put us in good shape to\\nunderstand one of the main building blocks of the Transformer architecture. Let’s\\ntake a deeper look.\\nAttention Mechanisms\\nThe main idea behind attention is that instead of producing a single hidden state for\\nthe input sequence, the encoder outputs a hidden state at each step that the decoder\\ncan access. However, using all the states at the same time would create a huge input\\nfor the decoder, so some mechanism is needed to prioritize which states to use. This\\nis where attention comes in: it lets the decoder assign a different amount of weight, or\\n“attention, ” to each of the encoder states at every decoding timestep. This process is\\nillustrated in Figure 1-4, where the role of attention is shown for predicting the third\\ntoken in the output sequence.\\n4 | Chapter 1: Hello Transformers'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 28, 'page_label': '5'}, page_content='Figure 1-4. An encoder-decoder architecture with an attention mechanism for a pair of\\nRNNs\\nBy focusing on which input tokens are most relevant at each timestep, these\\nattention-based models are able to learn nontrivial alignments between the words in a\\ngenerated translation and those in a source sentence. For example, Figure 1-5 visual‐\\nizes the attention weights for an English to French translation model, where each\\npixel denotes a weight. The figure shows how the decoder is able to correctly align the\\nwords “zone” and “ Area” , which are ordered differently in the two languages.\\nFigure 1-5. RNN encoder-decoder alignment of words in English and the generated\\ntranslation in French (courtesy of Dzmitry Bahdanau)\\nAttention Mechanisms | 5'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 29, 'page_label': '6'}, page_content='7 Weights are the learnable parameters of a neural network.\\nAlthough attention enabled the production of much better translations, there was still\\na major shortcoming with using recurrent models for the encoder and decoder: the\\ncomputations are inherently sequential and cannot be parallelized across the input\\nsequence.\\nWith the transformer, a new modeling paradigm was introduced: dispense with\\nrecurrence altogether, and instead rely entirely on a special form of attention called\\nself-attention. We’ll cover self-attention in more detail in Chapter 3, but the basic idea\\nis to allow attention to operate on all the states in the same layer of the neural net‐\\nwork. This is shown in Figure 1-6, where both the encoder and the decoder have their\\nown self-attention mechanisms, whose outputs are fed to feed-forward neural net‐\\nworks (FF NNs). This architecture can be trained much faster than recurrent models\\nand paved the way for many of the recent breakthroughs in NLP .\\nFigure 1-6. Encoder-decoder architecture of the original Transformer\\nIn the original Transformer paper, the translation model was trained from scratch on\\na large corpus of sentence pairs in various languages. However, in many practical\\napplications of NLP we do not have access to large amounts of labeled text data to\\ntrain our models on. A final piece was missing to get the transformer revolution\\nstarted: transfer learning.\\nTransfer Learning in NLP\\nIt is nowadays common practice in computer vision to use transfer learning to train a\\nconvolutional neural network like ResNet on one task, and then adapt it to or fine-\\ntune it on a new task. This allows the network to make use of the knowledge learned\\nfrom the original task. Architecturally, this involves splitting the model into of a body\\nand a head, where the head is a task-specific network. During training, the weights of\\nthe body learn broad features of the source domain, and these weights are used to ini‐\\ntialize a new model for the new task. 7 Compared to traditional supervised learning,\\nthis approach typically produces high-quality models that can be trained much more\\n6 | Chapter 1: Hello Transformers'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 30, 'page_label': '7'}, page_content='efficiently on a variety of downstream tasks, and with much less labeled data. A com‐\\nparison of the two approaches is shown in Figure 1-7.\\nFigure 1-7. Comparison of traditional supervised learning (left) and transfer learning\\n(right)\\nIn computer vision, the models are first trained on large-scale datasets such as Image‐\\nNet, which contain millions of images. This process is called pretraining and its main\\npurpose is to teach the models the basic features of images, such as edges or colors.\\nThese pretrained models can then be fine-tuned on a downstream task such as classi‐\\nfying flower species with a relatively small number of labeled examples (usually a few\\nhundred per class). Fine-tuned models typically achieve a higher accuracy than\\nsupervised models trained from scratch on the same amount of labeled data.\\nAlthough transfer learning became the standard approach in computer vision, for\\nmany years it was not clear what the analogous pretraining process was for NLP . As a\\nresult, NLP applications typically required large amounts of labeled data to achieve\\nhigh performance. And even then, that performance did not compare to what was\\nachieved in the vision domain.\\nTransfer Learning in NLP | 7'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 31, 'page_label': '8'}, page_content='8 A. Radford, R. Jozefowicz, and I. Sutskever, “Learning to Generate Reviews and Discovering Sentiment”,\\n(2017).\\n9 A related work at this time was ELMo (Embeddings from Language Models), which showed how pretraining\\nLSTMs could produce high-quality word embeddings for downstream tasks.\\n10 This is more true for English than for most of the world’s languages, where obtaining a large corpus of digi‐\\ntized text can be difficult. Finding ways to bridge this gap is an active area of NLP research and activism.\\nIn 2017 and 2018, several research groups proposed new approaches that finally\\nmade transfer learning work for NLP . It started with an insight from researchers at\\nOpenAI who obtained strong performance on a sentiment classification task by using\\nfeatures extracted from unsupervised pretraining. 8 This was followed by ULMFiT,\\nwhich introduced a general framework to adapt pretrained LSTM models for various\\ntasks.9\\nAs illustrated in Figure 1-8, ULMFiT involves three main steps:\\nPretraining\\nThe initial training objective is quite simple: predict the next word based on the\\nprevious words. This task is referred to as language modeling. The elegance of this\\napproach lies in the fact that no labeled data is required, and one can make use of\\nabundantly available text from sources such as Wikipedia.10\\nDomain adaptation\\nOnce the language model is pretrained on a large-scale corpus, the next step is to\\nadapt it to the in-domain corpus (e.g., from Wikipedia to the IMDb corpus of\\nmovie reviews, as in Figure 1-8). This stage still uses language modeling, but now\\nthe model has to predict the next word in the target corpus.\\nFine-tuning\\nIn this step, the language model is fine-tuned with a classification layer for the\\ntarget task (e.g., classifying the sentiment of movie reviews in Figure 1-8).\\nFigure 1-8. The ULMFiT process (courtesy of Jeremy Howard)\\nBy introducing a viable framework for pretraining and transfer learning in NLP ,\\nULMFiT provided the missing piece to make transformers take off. In 2018, two\\ntransformers were released that combined self-attention with transfer learning:\\n8 | Chapter 1: Hello Transformers'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 32, 'page_label': '9'}, page_content='11 Y . Zhu et al., “ Aligning Books and Movies: Towards Story-Like Visual Explanations by Watching Movies and\\nReading Books”, (2015).\\nGPT\\nUses only the decoder part of the Transformer architecture, and the same lan‐\\nguage modeling approach as ULMFiT. GPT was pretrained on the BookCorpus,11\\nwhich consists of 7,000 unpublished books from a variety of genres including\\nAdventure, Fantasy, and Romance.\\nBERT\\nUses the encoder part of the Transformer architecture, and a special form of lan‐\\nguage modeling called masked language modeling. The objective of masked lan‐\\nguage modeling is to predict randomly masked words in a text. For example,\\ngiven a sentence like “I looked at my [MASK] and saw that [MASK] was late. ” the\\nmodel needs to predict the most likely candidates for the masked words that are\\ndenoted by [MASK]. BERT was pretrained on the BookCorpus and English\\nWikipedia.\\nGPT and BERT set a new state of the art across a variety of NLP benchmarks and\\nushered in the age of transformers.\\nHowever, with different research labs releasing their models in incompatible frame‐\\nworks (PyTorch or TensorFlow), it wasn’t always easy for NLP practitioners to port\\nthese models to their own applications. With the release of \\n  Transformers, a unified\\nAPI across more than 50 architectures was progressively built. This library catalyzed\\nthe explosion of research into transformers and quickly trickled down to NLP practi‐\\ntioners, making it easy to integrate these models into many real-life applications\\ntoday. Let’s have a look!\\nHugging Face Transformers: Bridging the Gap\\nApplying a novel machine learning architecture to a new task can be a complex\\nundertaking, and usually involves the following steps:\\n1. Implement the model architecture in code, typically based on PyTorch or\\nTensorFlow.\\n2. Load the pretrained weights (if available) from a server.\\n3. Preprocess the inputs, pass them through the model, and apply some task-\\nspecific postprocessing.\\n4. Implement dataloaders and define loss functions and optimizers to train the\\nmodel.\\nHugging Face Transformers: Bridging the Gap | 9'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 33, 'page_label': '10'}, page_content='Each of these steps requires custom logic for each model and task. Traditionally (but\\nnot always!), when research groups publish a new article, they will also release the\\ncode along with the model weights. However, this code is rarely standardized and\\noften requires days of engineering to adapt to new use cases.\\nThis is where \\n  Transformers comes to the NLP practitioner’s rescue! It provides a\\nstandardized interface to a wide range of transformer models as well as code and\\ntools to adapt these models to new use cases. The library currently supports three\\nmajor deep learning frameworks (PyTorch, TensorFlow, and JAX) and allows you to\\neasily switch between them. In addition, it provides task-specific heads so you can\\neasily fine-tune transformers on downstream tasks such as text classification, named\\nentity recognition, and question answering. This reduces the time it takes a practi‐\\ntioner to train and test a handful of models from a week to a single afternoon!\\nY ou’ll see this for yourself in the next section, where we show that with just a few lines\\nof code, \\n  Transformers can be applied to tackle some of the most common NLP\\napplications that you’re likely to encounter in the wild.\\nA Tour of Transformer Applications\\nEvery NLP task starts with a piece of text, like the following made-up customer feed‐\\nback about a certain online order:\\ntext = \"\"\"Dear Amazon, last week I ordered an Optimus Prime action figure\\nfrom your online store in Germany. Unfortunately, when I opened the package,\\nI discovered to my horror that I had been sent an action figure of Megatron\\ninstead! As a lifelong enemy of the Decepticons, I hope you can understand my\\ndilemma. To resolve the issue, I demand an exchange of Megatron for the\\nOptimus Prime figure I ordered. Enclosed are copies of my records concerning\\nthis purchase. I expect to hear from you soon. Sincerely, Bumblebee.\"\"\"\\nDepending on your application, the text you’re working with could be a legal con‐\\ntract, a product description, or something else entirely. In the case of customer feed‐\\nback, you would probably like to know whether the feedback is positive or negative.\\nThis task is called sentiment analysis and is part of the broader topic of text classifica‐\\ntion that we’ll explore in Chapter 2 . For now, let’s have a look at what it takes to\\nextract the sentiment from our piece of text using \\n  Transformers.\\nText Classification\\nAs we’ll see in later chapters, \\n  Transformers has a layered API that allows you to\\ninteract with the library at various levels of abstraction. In this chapter we’ll start with\\npipelines, which abstract away all the steps needed to convert raw text into a set of\\npredictions from a fine-tuned model.\\nIn \\n  Transformers, we instantiate a pipeline by calling the pipeline() function and\\nproviding the name of the task we are interested in:\\n10 | Chapter 1: Hello Transformers'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 34, 'page_label': '11'}, page_content='from transformers import pipeline\\nclassifier = pipeline(\"text-classification\")\\nThe first time you run this code you’ll see a few progress bars appear because the\\npipeline automatically downloads the model weights from the Hugging Face Hub .\\nThe second time you instantiate the pipeline, the library will notice that you’ve\\nalready downloaded the weights and will use the cached version instead. By default,\\nthe text-classification pipeline uses a model that’s designed for sentiment analy‐\\nsis, but it also supports multiclass and multilabel classification.\\nNow that we have our pipeline, let’s generate some predictions! Each pipeline takes a\\nstring of text (or a list of strings) as input and returns a list of predictions. Each pre‐\\ndiction is a Python dictionary, so we can use Pandas to display them nicely as a\\nDataFrame:\\nimport pandas as pd\\noutputs = classifier(text)\\npd.DataFrame(outputs)\\nlabel score\\n0 NEGATIVE 0.901546\\nIn this case the model is very confident that the text has a negative sentiment, which\\nmakes sense given that we’re dealing with a complaint from an angry customer! Note\\nthat for sentiment analysis tasks the pipeline only returns one of the POSITIVE or NEG\\nATIVE labels, since the other can be inferred by computing 1-score.\\nLet’s now take a look at another common task, identifying named entities in text.\\nNamed Entity Recognition\\nPredicting the sentiment of customer feedback is a good first step, but you often want\\nto know if the feedback was about a particular item or service. In NLP , real-world\\nobjects like products, places, and people are called named entities, and extracting\\nthem from text is called named entity recognition (NER). We can apply NER by load‐\\ning the corresponding pipeline and feeding our customer review to it:\\nner_tagger = pipeline(\"ner\", aggregation_strategy=\"simple\")\\noutputs = ner_tagger(text)\\npd.DataFrame(outputs)\\nentity_group score word start end\\n0 ORG 0.879010 Amazon 5 11\\n1 MISC 0.990859 Optimus Prime 36 49\\nA Tour of Transformer Applications | 11'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 35, 'page_label': '12'}, page_content='entity_group score word start end\\n2 LOC 0.999755 Germany 90 97\\n3 MISC 0.556569 Mega 208 212\\n4 PER 0.590256 ##tron 212 216\\n5 ORG 0.669692 Decept 253 259\\n6 MISC 0.498350 ##icons 259 264\\n7 MISC 0.775361 Megatron 350 358\\n8 MISC 0.987854 Optimus Prime 367 380\\n9 PER 0.812096 Bumblebee 502 511\\nY ou can see that the pipeline detected all the entities and also assigned a category\\nsuch as ORG (organization), LOC (location), or PER (person) to each of them. Here we\\nused the aggregation_strategy argument to group the words according to the mod‐\\nel’s predictions. For example, the entity “Optimus Prime” is composed of two words,\\nbut is assigned a single category: MISC (miscellaneous). The scores tell us how confi‐\\ndent the model was about the entities it identified. We can see that it was least confi‐\\ndent about “Decepticons” and the first occurrence of “Megatron” , both of which it\\nfailed to group as a single entity.\\nSee those weird hash symbols ( #) in the word column in the previ‐\\nous table? These are produced by the model’s tokenizer, which\\nsplits words into atomic units called tokens. Y ou’ll learn all about\\ntokenization in Chapter 2.\\nExtracting all the named entities in a text is nice, but sometimes we would like to ask\\nmore targeted questions. This is where we can use question answering.\\nQuestion Answering\\nIn question answering, we provide the model with a passage of text called the context,\\nalong with a question whose answer we’ d like to extract. The model then returns the\\nspan of text corresponding to the answer. Let’s see what we get when we ask a specific\\nquestion about our customer feedback:\\nreader = pipeline(\"question-answering\")\\nquestion = \"What does the customer want?\"\\noutputs = reader(question=question, context=text)\\npd.DataFrame([outputs])\\nscore start end answer\\n0 0.631291 335 358 an exchange of Megatron\\n12 | Chapter 1: Hello Transformers'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 36, 'page_label': '13'}, page_content='We can see that along with the answer, the pipeline also returned start and end inte‐\\ngers that correspond to the character indices where the answer span was found (just\\nlike with NER tagging). There are several flavors of question answering that we will\\ninvestigate in Chapter 7, but this particular kind is called extractive question answer‐\\ning because the answer is extracted directly from the text.\\nWith this approach you can read and extract relevant information quickly from a cus‐\\ntomer’s feedback. But what if you get a mountain of long-winded complaints and you\\ndon’t have the time to read them all? Let’s see if a summarization model can help!\\nSummarization\\nThe goal of text summarization is to take a long text as input and generate a short\\nversion with all the relevant facts. This is a much more complicated task than the pre‐\\nvious ones since it requires the model to generate coherent text. In what should be a\\nfamiliar pattern by now, we can instantiate a summarization pipeline as follows:\\nsummarizer = pipeline(\"summarization\")\\noutputs = summarizer(text, max_length=45, clean_up_tokenization_spaces=True)\\nprint(outputs[0][\\'summary_text\\'])\\n Bumblebee ordered an Optimus Prime action figure from your online store in\\nGermany. Unfortunately, when I opened the package, I discovered to my horror\\nthat I had been sent an action figure of Megatron instead.\\nThis summary isn’t too bad! Although parts of the original text have been copied, the\\nmodel was able to capture the essence of the problem and correctly identify that\\n“Bumblebee” (which appeared at the end) was the author of the complaint. In this\\nexample you can also see that we passed some keyword arguments like max_length\\nand clean_up_tokenization_spaces to the pipeline; these allow us to tweak the out‐\\nputs at runtime.\\nBut what happens when you get feedback that is in a language you don’t understand?\\nY ou could use Google Translate, or you can use your very own transformer to trans‐\\nlate it for you!\\nTranslation\\nLike summarization, translation is a task where the output consists of generated text.\\nLet’s use a translation pipeline to translate an English text to German:\\ntranslator = pipeline(\"translation_en_to_de\",\\n                      model=\"Helsinki-NLP/opus-mt-en-de\")\\noutputs = translator(text, clean_up_tokenization_spaces=True, min_length=100)\\nprint(outputs[0][\\'translation_text\\'])\\nSehr geehrter Amazon, letzte Woche habe ich eine Optimus Prime Action Figur aus\\nIhrem Online-Shop in Deutschland bestellt. Leider, als ich das Paket öffnete,\\nentdeckte ich zu meinem Entsetzen, dass ich stattdessen eine Action Figur von\\nA Tour of Transformer Applications | 13'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 37, 'page_label': '14'}, page_content='Megatron geschickt worden war! Als lebenslanger Feind der Decepticons, Ich\\nhoffe, Sie können mein Dilemma verstehen. Um das Problem zu lösen, Ich fordere\\neinen Austausch von Megatron für die Optimus Prime Figur habe ich bestellt.\\nAnbei sind Kopien meiner Aufzeichnungen über diesen Kauf. Ich erwarte, bald von\\nIhnen zu hören. Aufrichtig, Bumblebee.\\nAgain, the model produced a very good translation that correctly uses German’s for‐\\nmal pronouns, like “Ihrem” and “Sie. ” Here we’ve also shown how you can override\\nthe default model in the pipeline to pick the best one for your application—and you\\ncan find models for thousands of language pairs on the Hugging Face Hub. Before we\\ntake a step back and look at the whole Hugging Face ecosystem, let’s examine one last\\napplication.\\nText Generation\\nLet’s say you would like to be able to provide faster replies to customer feedback by\\nhaving access to an autocomplete function. With a text generation model you can do\\nthis as follows:\\ngenerator = pipeline(\"text-generation\")\\nresponse = \"Dear Bumblebee, I am sorry to hear that your order was mixed up.\"\\nprompt = text + \"\\\\n\\\\nCustomer service response:\\\\n\" + response\\noutputs = generator(prompt, max_length=200)\\nprint(outputs[0][\\'generated_text\\'])\\nDear Amazon, last week I ordered an Optimus Prime action figure from your online\\nstore in Germany. Unfortunately, when I opened the package, I discovered to my\\nhorror that I had been sent an action figure of Megatron instead! As a lifelong\\nenemy of the Decepticons, I hope you can understand my dilemma. To resolve the\\nissue, I demand an exchange of Megatron for the Optimus Prime figure I ordered.\\nEnclosed are copies of my records concerning this purchase. I expect to hear\\nfrom you soon. Sincerely, Bumblebee.\\nCustomer service response:\\nDear Bumblebee, I am sorry to hear that your order was mixed up. The order was\\ncompletely mislabeled, which is very common in our online store, but I can\\nappreciate it because it was my understanding from this site and our customer\\nservice of the previous day that your order was not made correct in our mind and\\nthat we are in a process of resolving this matter. We can assure you that your\\norder\\nOK, maybe we wouldn’t want to use this completion to calm Bumblebee down, but\\nyou get the general idea.\\nNow that you’ve seen a few cool applications of transformer models, you might be\\nwondering where the training happens. All of the models that we’ve used in this chap‐\\nter are publicly available and already fine-tuned for the task at hand. In general, how‐\\never, you’ll want to fine-tune models on your own data, and in the following chapters\\nyou will learn how to do just that.\\n14 | Chapter 1: Hello Transformers'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 38, 'page_label': '15'}, page_content='But training a model is just a small piece of any NLP project—being able to efficiently\\nprocess data, share results with colleagues, and make your work reproducible are key\\ncomponents too. Fortunately, \\n  Transformers is surrounded by a big ecosystem of\\nuseful tools that support much of the modern machine learning workflow. Let’s take a\\nlook.\\nThe Hugging Face Ecosystem\\nWhat started with \\n  Transformers has quickly grown into a whole ecosystem con‐\\nsisting of many libraries and tools to accelerate your NLP and machine learning\\nprojects. The Hugging Face ecosystem consists of mainly two parts: a family of libra‐\\nries and the Hub, as shown in Figure 1-9 . The libraries provide the code while the\\nHub provides the pretrained model weights, datasets, scripts for the evaluation met‐\\nrics, and more. In this section we’ll have a brief look at the various components. We’ll\\nskip \\n  Transformers, as we’ve already discussed it and we will see a lot more of it\\nthroughout the course of the book.\\nFigure 1-9. An overview of the Hugging Face ecosystem\\nThe Hugging Face Ecosystem | 15'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 39, 'page_label': '16'}, page_content='The Hugging Face Hub\\nAs outlined earlier, transfer learning is one of the key factors driving the success of\\ntransformers because it makes it possible to reuse pretrained models for new tasks.\\nConsequently, it is crucial to be able to load pretrained models quickly and run\\nexperiments with them.\\nThe Hugging Face Hub hosts over 20,000 freely available models. As shown in\\nFigure 1-10 , there are filters for tasks, frameworks, datasets, and more that are\\ndesigned to help you navigate the Hub and quickly find promising candidates. As\\nwe’ve seen with the pipelines, loading a promising model in your code is then literally\\njust one line of code away. This makes experimenting with a wide range of models\\nsimple, and allows you to focus on the domain-specific parts of your project.\\nFigure 1-10. The Models page of the Hugging Face Hub, showing filters on the left and a\\nlist of models on the right\\nIn addition to model weights, the Hub also hosts datasets and scripts for computing\\nmetrics, which let you reproduce published results or leverage additional data for\\nyour application.\\nThe Hub also provides model and dataset cards to document the contents of models\\nand datasets and help you make an informed decision about whether they’re the right\\nones for you. One of the coolest features of the Hub is that you can try out any model\\ndirectly through the various task-specific interactive widgets as shown in Figure 1-11.\\n16 | Chapter 1: Hello Transformers'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 40, 'page_label': '17'}, page_content='12 Rust is a high-performance programming language.\\nFigure 1-11. An example model card from the Hugging Face Hub: the inference widget,\\nwhich allows you to interact with the model, is shown on the right\\nLet’s continue our tour with \\n  Tokenizers.\\nPyTorch and TensorFlow also offer hubs of their own and are\\nworth checking out if a particular model or dataset is not available\\non the Hugging Face Hub.\\nHugging Face Tokenizers\\nBehind each of the pipeline examples that we’ve seen in this chapter is a tokenization\\nstep that splits the raw text into smaller pieces called tokens. We’ll see how this works\\nin detail in Chapter 2 , but for now it’s enough to understand that tokens may be\\nwords, parts of words, or just characters like punctuation. Transformer models are\\ntrained on numerical representations of these tokens, so getting this step right is\\npretty important for the whole NLP project!\\n Tokenizers provides many tokenization strategies and is extremely fast at tokeniz‐\\ning text thanks to its Rust backend.12 It also takes care of all the pre- and postprocess‐\\ning steps, such as normalizing the inputs and transforming the model outputs to the\\nrequired format. With \\n  Tokenizers, we can load a tokenizer in the same way we can\\nload pretrained model weights with \\n  Transformers.\\nThe Hugging Face Ecosystem | 17'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 41, 'page_label': '18'}, page_content='We need a dataset and metrics to train and evaluate models, so let’s take a look at \\n Datasets, which is in charge of that aspect.\\nHugging Face Datasets\\nLoading, processing, and storing datasets can be a cumbersome process, especially\\nwhen the datasets get too large to fit in your laptop’s RAM. In addition, you usually\\nneed to implement various scripts to download the data and transform it into a stan‐\\ndard format.\\n Datasets simplifies this process by providing a standard interface for thousands of\\ndatasets that can be found on the Hub. It also provides smart caching (so you don’t\\nhave to redo your preprocessing each time you run your code) and avoids RAM limi‐\\ntations by leveraging a special mechanism called memory mapping  that stores the\\ncontents of a file in virtual memory and enables multiple processes to modify a file\\nmore efficiently. The library is also interoperable with popular frameworks like Pan‐\\ndas and NumPy, so you don’t have to leave the comfort of your favorite data wran‐\\ngling tools.\\nHaving a good dataset and powerful model is worthless, however, if you can’t reliably\\nmeasure the performance. Unfortunately, classic NLP metrics come with many differ‐\\nent implementations that can vary slightly and lead to deceptive results. By providing\\nthe scripts for many metrics, \\n  Datasets helps make experiments more reproducible\\nand the results more trustworthy.\\nWith the \\n  Transformers, \\n  Tokenizers, and \\n  Datasets libraries we have every‐\\nthing we need to train our very own transformer models! However, as we’ll see in\\nChapter 10 there are situations where we need fine-grained control over the training\\nloop. That’s where the last library of the ecosystem comes into play: \\n  Accelerate.\\nHugging Face Accelerate\\nIf you’ve ever had to write your own training script in PyTorch, chances are that\\nyou’ve had some headaches when trying to port the code that runs on your laptop to\\nthe code that runs on your organization’s cluster. \\n  Accelerate adds a layer of abstrac‐\\ntion to your normal training loops that takes care of all the custom logic necessary for\\nthe training infrastructure. This literally accelerates your workflow by simplifying the\\nchange of infrastructure when necessary.\\nThis sums up the core components of Hugging Face’s open source ecosystem. But\\nbefore wrapping up this chapter, let’s take a look at a few of the common challenges\\nthat come with trying to deploy transformers in the real world.\\n18 | Chapter 1: Hello Transformers'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 42, 'page_label': '19'}, page_content='Main Challenges with Transformers\\nIn this chapter we’ve gotten a glimpse of the wide range of NLP tasks that can be tack‐\\nled with transformer models. Reading the media headlines, it can sometimes sound\\nlike their capabilities are limitless. However, despite their usefulness, transformers are\\nfar from being a silver bullet. Here are a few challenges associated with them that we\\nwill explore throughout the book:\\nLanguage\\nNLP research is dominated by the English language. There are several models for\\nother languages, but it is harder to find pretrained models for rare or low-\\nresource languages. In Chapter 4 , we’ll explore multilingual transformers and\\ntheir ability to perform zero-shot cross-lingual transfer.\\nData availability\\nAlthough we can use transfer learning to dramatically reduce the amount of\\nlabeled training data our models need, it is still a lot compared to how much a\\nhuman needs to perform the task. Tackling scenarios where you have little to no\\nlabeled data is the subject of Chapter 9.\\nWorking with long documents\\nSelf-attention works extremely well on paragraph-long texts, but it becomes very\\nexpensive when we move to longer texts like whole documents. Approaches to\\nmitigate this are discussed in Chapter 11.\\nOpacity\\nAs with other deep learning models, transformers are to a large extent opaque. It\\nis hard or impossible to unravel “why” a model made a certain prediction. This is\\nan especially hard challenge when these models are deployed to make critical\\ndecisions. We’ll explore some ways to probe the errors of transformer models in\\nChapters 2 and 4.\\nBias\\nTransformer models are predominantly pretrained on text data from the internet.\\nThis imprints all the biases that are present in the data into the models. Making\\nsure that these are neither racist, sexist, or worse is a challenging task. We discuss\\nsome of these issues in more detail in Chapter 10.\\nAlthough daunting, many of these challenges can be overcome. As well as in the spe‐\\ncific chapters mentioned, we will touch on these topics in almost every chapter ahead.\\nMain Challenges with Transformers | 19'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 43, 'page_label': '20'}, page_content='Conclusion\\nHopefully, by now you are excited to learn how to start training and integrating these\\nversatile models into your own applications! Y ou’ve seen in this chapter that with just\\na few lines of code you can use state-of-the-art models for classification, named entity\\nrecognition, question answering, translation, and summarization, but this is really\\njust the “tip of the iceberg. ”\\nIn the following chapters you will learn how to adapt transformers to a wide range of\\nuse cases, such as building a text classifier, or a lightweight model for production, or\\neven training a language model from scratch. We’ll be taking a hands-on approach,\\nwhich means that for every concept covered there will be accompanying code that\\nyou can run on Google Colab or your own GPU machine.\\nNow that we’re armed with the basic concepts behind transformers, it’s time to get\\nour hands dirty with our first application: text classification. That’s the topic of the\\nnext chapter!\\n20 | Chapter 1: Hello Transformers'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 44, 'page_label': '21'}, page_content='CHAPTER 2\\nText Classification\\nText classification is one of the most common tasks in NLP; it can be used for a broad\\nrange of applications, such as tagging customer feedback into categories or routing\\nsupport tickets according to their language. Chances are that your email program’s\\nspam filter is using text classification to protect your inbox from a deluge of unwan‐\\nted junk!\\nAnother common type of text classification is sentiment analysis, which (as we saw in\\nChapter 1) aims to identify the polarity of a given text. For example, a company like\\nTesla might analyze Twitter posts like the one in Figure 2-1  to determine whether\\npeople like its new car roofs or not.\\nFigure 2-1. Analyzing Twitter content can yield useful feedback from customers (cour‐\\ntesy of Aditya Veluri)\\n21'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 45, 'page_label': '22'}, page_content='1 V . Sanh et al., “DistilBERT, a Distilled Version of BERT: Smaller, Faster, Cheaper and Lighter”, (2019).\\n2 Optimus Prime is the leader of a race of robots in the popular Transformers franchise for children (and for\\nthose who are young at heart!).\\n3 E. Saravia et al., “CARER: Contextualized Affect Representations for Emotion Recognition, ” Proceedings of the\\n2018 Conference on Empirical Methods in Natural Language Processing (Oct–Nov 2018): 3687–3697, http://\\ndx.doi.org/10.18653/v1/D18-1404.\\nNow imagine that you are a data scientist who needs to build a system that can auto‐\\nmatically identify emotional states such as “anger” or “joy” that people express about\\nyour company’s product on Twitter. In this chapter, we’ll tackle this task using a var‐\\niant of BERT called DistilBERT.1 The main advantage of this model is that it achieves\\ncomparable performance to BERT, while being significantly smaller and more effi‐\\ncient. This enables us to train a classifier in a few minutes, and if you want to train a\\nlarger BERT model you can simply change the checkpoint of the pretrained model. A\\ncheckpoint corresponds to the set of weights that are loaded into a given transformer\\narchitecture.\\nThis will also be our first encounter with three of the core libraries from the Hugging\\nFace ecosystem: \\n  Datasets, \\n  Tokenizers, and \\n  Transformers. As shown in\\nFigure 2-2 , these libraries will allow us to quickly go from raw text to a fine-tuned\\nmodel that can be used for inference on new tweets. So, in the spirit of Optimus\\nPrime, let’s dive in, “transform, and roll out!”2\\nFigure 2-2. A typical pipeline for training transformer models with the \\n  Datasets, \\nTokenizers, and \\n  Transformers libraries\\nThe Dataset\\nTo build our emotion detector we’ll use a great dataset from an article that explored\\nhow emotions are represented in English Twitter messages. 3 Unlike most sentiment\\nanalysis datasets that involve just “positive” and “negative” polarities, this dataset con‐\\ntains six basic emotions: anger, disgust, fear, joy, sadness, and surprise. Given a tweet,\\nour task will be to train a model that can classify it into one of these emotions.\\n22 | Chapter 2: Text Classification'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 46, 'page_label': '23'}, page_content='A First Look at Hugging Face Datasets\\nWe will use \\n  Datasets to download the data from the Hugging Face Hub. We can\\nuse the list_datasets() function to see what datasets are available on the Hub:\\nfrom datasets import list_datasets\\nall_datasets = list_datasets()\\nprint(f\"There are {len(all_datasets)} datasets currently available on the Hub\")\\nprint(f\"The first 10 are: {all_datasets[:10]}\")\\nThere are 1753 datasets currently available on the Hub\\nThe first 10 are: [\\'acronym_identification\\', \\'ade_corpus_v2\\', \\'adversarial_qa\\',\\n\\'aeslc\\', \\'afrikaans_ner_corpus\\', \\'ag_news\\', \\'ai2_arc\\', \\'air_dialogue\\',\\n\\'ajgt_twitter_ar\\', \\'allegro_reviews\\']\\nWe see that each dataset is given a name, so let’s load the emotion dataset with the\\nload_dataset() function:\\nfrom datasets import load_dataset\\nemotions = load_dataset(\"emotion\")\\nIf we look inside our emotions object:\\nemotions\\nDatasetDict({\\n    train: Dataset({\\n        features: [\\'text\\', \\'label\\'],\\n        num_rows: 16000\\n    })\\n    validation: Dataset({\\n        features: [\\'text\\', \\'label\\'],\\n        num_rows: 2000\\n    })\\n    test: Dataset({\\n        features: [\\'text\\', \\'label\\'],\\n        num_rows: 2000\\n    })\\n})\\nwe see it is similar to a Python dictionary, with each key corresponding to a different\\nsplit. And we can use the usual dictionary syntax to access an individual split:\\ntrain_ds = emotions[\"train\"]\\ntrain_ds\\nDataset({\\n    features: [\\'text\\', \\'label\\'],\\n    num_rows: 16000\\n})\\nThe Dataset | 23'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 47, 'page_label': '24'}, page_content='which returns an instance of the Dataset class. The Dataset object is one of the core\\ndata structures in \\n  Datasets, and we’ll be exploring many of its features throughout\\nthe course of this book. For starters, it behaves like an ordinary Python array or list,\\nso we can query its length:\\nlen(train_ds)\\n16000\\nor access a single example by its index:\\ntrain_ds[0]\\n{\\'label\\': 0, \\'text\\': \\'i didnt feel humiliated\\'}\\nHere we see that a single row is represented as a dictionary, where the keys corre‐\\nspond to the column names:\\ntrain_ds.column_names\\n[\\'text\\', \\'label\\']\\nand the values are the tweet and the emotion. This reflects the fact that \\n  Datasets is\\nbased on Apache Arrow, which defines a typed columnar format that is more memory\\nefficient than native Python. We can see what data types are being used under the\\nhood by accessing the features attribute of a Dataset object:\\nprint(train_ds.features)\\n{\\'text\\': Value(dtype=\\'string\\', id=None), \\'label\\': ClassLabel(num_classes=6,\\nnames=[\\'sadness\\', \\'joy\\', \\'love\\', \\'anger\\', \\'fear\\', \\'surprise\\'], names_file=None,\\nid=None)}\\nIn this case, the data type of the text column is string, while the label column is a\\nspecial ClassLabel object that contains information about the class names and their\\nmapping to integers. We can also access several rows with a slice:\\nprint(train_ds[:5])\\n{\\'text\\': [\\'i didnt feel humiliated\\', \\'i can go from feeling so hopeless to so\\ndamned hopeful just from being around someone who cares and is awake\\', \\'im\\ngrabbing a minute to post i feel greedy wrong\\', \\'i am ever feeling nostalgic\\nabout the fireplace i will know that it is still on the property\\', \\'i am feeling\\ngrouchy\\'], \\'label\\': [0, 0, 3, 2, 3]}\\nNote that in this case, the dictionary values are now lists instead of individual ele‐\\nments. We can also get the full column by name:\\nprint(train_ds[\"text\"][:5])\\n[\\'i didnt feel humiliated\\', \\'i can go from feeling so hopeless to so damned\\nhopeful just from being around someone who cares and is awake\\', \\'im grabbing a\\nminute to post i feel greedy wrong\\', \\'i am ever feeling nostalgic about the\\nfireplace i will know that it is still on the property\\', \\'i am feeling grouchy\\']\\n24 | Chapter 2: Text Classification'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 48, 'page_label': '25'}, page_content='Now that we’ve seen how to load and inspect data with \\n  Datasets, let’s do a few\\nchecks about the content of our tweets.\\nWhat If My Dataset Is Not on the Hub?\\nWe’ll be using the Hugging Face Hub to download datasets for most of the examples\\nin this book. But in many cases, you’ll find yourself working with data that is either\\nstored on your laptop or on a remote server in your organization. \\n  Datasets pro‐\\nvides several loading scripts to handle local and remote datasets. Examples for the\\nmost common data formats are shown in Table 2-1.\\nTable 2-1. How to load datasets in various formats\\nData format Loading script Example\\nCSV csv load_dataset(\"csv\", data_files=\"my_file.csv\")\\nText text load_dataset(\"text\", data_files=\"my_file.txt\")\\nJSON json load_dataset(\"json\", data_files=\"my_file.jsonl\")\\nAs you can see, for each data format, we just need to pass the relevant loading script\\nto the load_dataset() function, along with a data_files argument that specifies the\\npath or URL to one or more files. For example, the source files for the emotion dataset\\nare actually hosted on Dropbox, so an alternative way to load the dataset is to first\\ndownload one of the splits:\\ndataset_url = \"https://www.dropbox.com/s/1pzkadrvffbqw6o/train.txt\"\\n!wget {dataset_url}\\nIf you’re wondering why there’s a ! character in the preceding shell command, that’s\\nbecause we’re running the commands in a Jupyter notebook. Simply remove the pre‐\\nfix if you want to download and unzip the dataset within a terminal. Now, if we peek\\nat the first row of the train.txt file:\\n!head -n 1 train.txt\\ni didnt feel humiliated;sadness\\nwe can see that here are no column headers and each tweet and emotion are separated\\nby a semicolon. Nevertheless, this is quite similar to a CSV file, so we can load the\\ndataset locally by using the csv script and pointing the data_files argument to the\\ntrain.txt file:\\nemotions_local = load_dataset(\"csv\", data_files=\"train.txt\", sep=\";\",\\n                              names=[\"text\", \"label\"])\\nHere we’ve also specified the type of delimiter and the names of the columns. An even\\nsimpler approach is to just point the data_files argument to the URL itself:\\nThe Dataset | 25'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 49, 'page_label': '26'}, page_content='dataset_url = \"https://www.dropbox.com/s/1pzkadrvffbqw6o/train.txt?dl=1\"\\nemotions_remote = load_dataset(\"csv\", data_files=dataset_url, sep=\";\",\\n                               names=[\"text\", \"label\"])\\nwhich will automatically download and cache the dataset for you. As you can see, the\\nload_dataset() function is very versatile. We recommend checking out the \\n  Data‐\\nsets documentation to get a complete overview.\\nFrom Datasets to DataFrames\\nAlthough \\n  Datasets provides a lot of low-level functionality to slice and dice our\\ndata, it is often convenient to convert a Dataset object to a Pandas DataFrame so we\\ncan access high-level APIs for data visualization. To enable the conversion, \\n  Data‐\\nsets provides a set_format() method that allows us to change the output format of\\nthe Dataset. Note that this does not change the underlying data format (which is an\\nArrow table), and you can switch to another format later if needed:\\nimport pandas as pd\\nemotions.set_format(type=\"pandas\")\\ndf = emotions[\"train\"][:]\\ndf.head()\\ntext label\\n0 i didnt feel humiliated 0\\n1 i can go from feeling so hopeless to so damned... 0\\n2 im grabbing a minute to post i feel greedy wrong 3\\n3 i am ever feeling nostalgic about the fireplac... 2\\n4 i am feeling grouchy 3\\nAs you can see, the column headers have been preserved and the first few rows match\\nour previous views of the data. However, the labels are represented as integers, so let’s\\nuse the int2str() method of the label feature to create a new column in our\\nDataFrame with the corresponding label names:\\ndef label_int2str(row):\\n    return emotions[\"train\"].features[\"label\"].int2str(row)\\ndf[\"label_name\"] = df[\"label\"].apply(label_int2str)\\ndf.head()\\ntext label label_name\\n0 i didnt feel humiliated 0 sadness\\n1 i can go from feeling so hopeless to so damned... 0 sadness\\n26 | Chapter 2: Text Classification'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 50, 'page_label': '27'}, page_content='text label label_name\\n2 im grabbing a minute to post i feel greedy wrong 3 anger\\n3 i am ever feeling nostalgic about the fireplac... 2 love\\n4 i am feeling grouchy 3 anger\\nBefore diving into building a classifier, let’s take a closer look at the dataset. As Andrej\\nKarpathy notes in his famous blog post “ A Recipe for Training Neural Networks” ,\\nbecoming “one with the data” is an essential step for training great models!\\nLooking at the Class Distribution\\nWhenever you are working on text classification problems, it is a good idea to exam‐\\nine the distribution of examples across the classes. A dataset with a skewed class dis‐\\ntribution might require a different treatment in terms of the training loss and\\nevaluation metrics than a balanced one.\\nWith Pandas and Matplotlib, we can quickly visualize the class distribution as follows:\\nimport matplotlib.pyplot as plt\\ndf[\"label_name\"].value_counts(ascending=True).plot.barh()\\nplt.title(\"Frequency of Classes\")\\nplt.show()\\nIn this case, we can see that the dataset is heavily imbalanced; the joy and sadness\\nclasses appear frequently, whereas love and surprise are about 5–10 times rarer.\\nThere are several ways to deal with imbalanced data, including:\\nThe Dataset | 27'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 51, 'page_label': '28'}, page_content='• Randomly oversample the minority class.\\n• Randomly undersample the majority class.\\n• Gather more labeled data from the underrepresented classes.\\nTo keep things simple in this chapter, we’ll work with the raw, unbalanced class fre‐\\nquencies. If you want to learn more about these sampling techniques, we recommend\\nchecking out the Imbalanced-learn library. Just make sure that you don’t apply sam‐\\npling methods before creating your train/test splits, or you’ll get plenty of leakage\\nbetween them!\\nNow that we’ve looked at the classes, let’s take a look at the tweets themselves.\\nHow Long Are Our Tweets?\\nTransformer models have a maximum input sequence length that is referred to as the\\nmaximum context size. For applications using DistilBERT, the maximum context size\\nis 512 tokens, which amounts to a few paragraphs of text. As we’ll see in the next sec‐\\ntion, a token is an atomic piece of text; for now, we’ll treat a token as a single word.\\nWe can get a rough estimate of tweet lengths per emotion by looking at the distribu‐\\ntion of words per tweet:\\ndf[\"Words Per Tweet\"] = df[\"text\"].str.split().apply(len)\\ndf.boxplot(\"Words Per Tweet\", by=\"label_name\", grid=False,\\n          showfliers=False, color=\"black\")\\nplt.suptitle(\"\")\\nplt.xlabel(\"\")\\nplt.show()\\n28 | Chapter 2: Text Classification'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 52, 'page_label': '29'}, page_content='From the plot we see that for each emotion, most tweets are around 15 words long\\nand the longest tweets are well below DistilBERT’s maximum context size. Texts that\\nare longer than a model’s context size need to be truncated, which can lead to a loss in\\nperformance if the truncated text contains crucial information; in this case, it looks\\nlike that won’t be an issue.\\nLet’s now figure out how we can convert these raw texts into a format suitable for \\n Transformers! While we’re at it, let’s also reset the output format of our dataset\\nsince we don’t need the DataFrame format anymore:\\nemotions.reset_format()\\nFrom Text to Tokens\\nTransformer models like DistilBERT cannot receive raw strings as input; instead, they\\nassume the text has been tokenized and encoded as numerical vectors. Tokenization is\\nthe step of breaking down a string into the atomic units used in the model. There are\\nseveral tokenization strategies one can adopt, and the optimal splitting of words into\\nsubunits is usually learned from the corpus. Before looking at the tokenizer used for\\nDistilBERT, let’s consider two extreme cases: character and word tokenization.\\nCharacter Tokenization\\nThe simplest tokenization scheme is to feed each character individually to the model.\\nIn Python, str objects are really arrays under the hood, which allows us to quickly\\nimplement character-level tokenization with just one line of code:\\ntext = \"Tokenizing text is a core task of NLP.\"\\ntokenized_text = list(text)\\nprint(tokenized_text)\\n[\\'T\\', \\'o\\', \\'k\\', \\'e\\', \\'n\\', \\'i\\', \\'z\\', \\'i\\', \\'n\\', \\'g\\', \\' \\', \\'t\\', \\'e\\', \\'x\\', \\'t\\', \\' \\',\\n\\'i\\', \\'s\\', \\' \\', \\'a\\', \\' \\', \\'c\\', \\'o\\', \\'r\\', \\'e\\', \\' \\', \\'t\\', \\'a\\', \\'s\\', \\'k\\', \\' \\', \\'o\\',\\n\\'f\\', \\' \\', \\'N\\', \\'L\\', \\'P\\', \\'.\\']\\nThis is a good start, but we’re not done yet. Our model expects each character to be\\nconverted to an integer, a process sometimes called numericalization. One simple way\\nto do this is by encoding each unique token (which are characters in this case) with a\\nunique integer:\\ntoken2idx = {ch: idx for idx, ch in enumerate(sorted(set(tokenized_text)))}\\nprint(token2idx)\\n{\\' \\': 0, \\'.\\': 1, \\'L\\': 2, \\'N\\': 3, \\'P\\': 4, \\'T\\': 5, \\'a\\': 6, \\'c\\': 7, \\'e\\': 8, \\'f\\': 9,\\n\\'g\\': 10, \\'i\\': 11, \\'k\\': 12, \\'n\\': 13, \\'o\\': 14, \\'r\\': 15, \\'s\\': 16, \\'t\\': 17, \\'x\\': 18,\\n\\'z\\': 19}\\nThis gives us a mapping from each character in our vocabulary to a unique integer.\\nWe can now use token2idx to transform the tokenized text to a list of integers:\\nFrom Text to Tokens | 29'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 53, 'page_label': '30'}, page_content='input_ids = [token2idx[token] for token in tokenized_text]\\nprint(input_ids)\\n[5, 14, 12, 8, 13, 11, 19, 11, 13, 10, 0, 17, 8, 18, 17, 0, 11, 16, 0, 6, 0, 7,\\n14, 15, 8, 0, 17, 6, 16, 12, 0, 14, 9, 0, 3, 2, 4, 1]\\nEach token has now been mapped to a unique numerical identifier (hence the name\\ninput_ids). The last step is to convert input_ids to a 2D tensor of one-hot vectors.\\nOne-hot vectors are frequently used in machine learning to encode categorical data,\\nwhich can be either ordinal or nominal. For example, suppose we wanted to encode\\nthe names of characters in the Transformers TV series. One way to do this would be\\nto map each name to a unique ID, as follows:\\ncategorical_df = pd.DataFrame(\\n    {\"Name\": [\"Bumblebee\", \"Optimus Prime\", \"Megatron\"], \"Label ID\": [0,1,2]})\\ncategorical_df\\nName Label ID\\n0 Bumblebee 0\\n1 Optimus Prime 1\\n2 Megatron 2\\nThe problem with this approach is that it creates a fictitious ordering between the\\nnames, and neural networks are really good at learning these kinds of relationships.\\nSo instead, we can create a new column for each category and assign a 1 where the\\ncategory is true, and a 0 otherwise. In Pandas, this can be implemented with the\\nget_dummies() function as follows:\\npd.get_dummies(categorical_df[\"Name\"])\\nBumblebee Megatron Optimus Prime\\n0 1 0 0\\n1 0 0 1\\n2 0 1 0\\nThe rows of this DataFrame are the one-hot vectors, which have a single “hot” entry\\nwith a 1 and 0s everywhere else. Now, looking at our input_ids, we have a similar\\nproblem: the elements create an ordinal scale. This means that adding or subtracting\\ntwo IDs is a meaningless operation, since the result is a new ID that represents\\nanother random token.\\nOn the other hand, the result of adding two one-hot encodings can easily be inter‐\\npreted: the two entries that are “hot” indicate that the corresponding tokens co-occur.\\nWe can create the one-hot encodings in PyTorch by converting input_ids to a tensor\\nand applying the one_hot() function as follows:\\n30 | Chapter 2: Text Classification'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 54, 'page_label': '31'}, page_content='import torch\\nimport torch.nn.functional as F\\ninput_ids = torch.tensor(input_ids)\\none_hot_encodings = F.one_hot(input_ids, num_classes=len(token2idx))\\none_hot_encodings.shape\\ntorch.Size([38, 20])\\nFor each of the 38 input tokens we now have a one-hot vector with 20 dimensions,\\nsince our vocabulary consists of 20 unique characters.\\nIt’s important to always set num_classes in the one_hot() function\\nbecause otherwise the one-hot vectors may end up being shorter\\nthan the length of the vocabulary (and need to be padded with\\nzeros manually). In TensorFlow, the equivalent function is\\ntf.one_hot(), where the depth argument plays the role of\\nnum_classes.\\nBy examining the first vector, we can verify that a 1 appears in the location indicated\\nby input_ids[0]:\\nprint(f\"Token: {tokenized_text[0]}\")\\nprint(f\"Tensor index: {input_ids[0]}\")\\nprint(f\"One-hot: {one_hot_encodings[0]}\")\\nToken: T\\nTensor index: 5\\nOne-hot: tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\\nFrom our simple example we can see that character-level tokenization ignores any\\nstructure in the text and treats the whole string as a stream of characters. Although\\nthis helps deal with misspellings and rare words, the main drawback is that linguistic\\nstructures such as words need to be learned from the data. This requires significant\\ncompute, memory, and data. For this reason, character tokenization is rarely used in\\npractice. Instead, some structure of the text is preserved during the tokenization step.\\nWord tokenization is a straightforward approach to achieve this, so let’s take a look at\\nhow it works.\\nWord Tokenization\\nInstead of splitting the text into characters, we can split it into words and map each\\nword to an integer. Using words from the outset enables the model to skip the step of\\nlearning words from characters, and thereby reduces the complexity of the training\\nprocess.\\nFrom Text to Tokens | 31'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 55, 'page_label': '32'}, page_content=\"4 GPT-2 is the successor of GPT, and it captivated the public’s attention with its impressive ability to generate\\nrealistic text. We’ll explore GPT-2 in detail in Chapter 6.\\nOne simple class of word tokenizers uses whitespace to tokenize the text. We can do\\nthis by applying Python’s split() function directly on the raw text (just like we did to\\nmeasure the tweet lengths):\\ntokenized_text = text.split()\\nprint(tokenized_text)\\n['Tokenizing', 'text', 'is', 'a', 'core', 'task', 'of', 'NLP.']\\nFrom here we can take the same steps we took for the character tokenizer to map\\neach word to an ID. However, we can already see one potential problem with this\\ntokenization scheme: punctuation is not accounted for, so NLP. is treated as a single\\ntoken. Given that words can include declinations, conjugations, or misspellings, the\\nsize of the vocabulary can easily grow into the millions!\\nSome word tokenizers have extra rules for punctuation. One can\\nalso apply stemming or lemmatization, which normalizes words to\\ntheir stem (e.g., “great” , “greater” , and “greatest” all become “great”),\\nat the expense of losing some information in the text.\\nHaving a large vocabulary is a problem because it requires neural networks to have an\\nenormous number of parameters. To illustrate this, suppose we have 1 million unique\\nwords and want to compress the 1-million-dimensional input vectors to 1-thousand-\\ndimensional vectors in the first layer of our neural network. This is a standard step in\\nmost NLP architectures, and the resulting weight matrix of this first layer would con‐\\ntain 1 million × 1 thousand = 1 billion weights. This is already comparable to the\\nlargest GPT-2 model,4 which has around 1.5 billion parameters in total!\\nNaturally, we want to avoid being so wasteful with our model parameters since mod‐\\nels are expensive to train, and larger models are more difficult to maintain. A com‐\\nmon approach is to limit the vocabulary and discard rare words by considering, say,\\nthe 100,000 most common words in the corpus. Words that are not part of the\\nvocabulary are classified as “unknown” and mapped to a shared UNK token. This\\nmeans that we lose some potentially important information in the process of word\\ntokenization, since the model has no information about words associated with UNK.\\nWouldn’t it be nice if there was a compromise between character and word tokeniza‐\\ntion that preserved all the input information and some of the input structure? There\\nis: subword tokenization.\\n32 | Chapter 2: Text Classification\"), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 56, 'page_label': '33'}, page_content='5 M. Schuster and K. Nakajima, “Japanese and Korean Voice Search, ” 2012 IEEE International Conference on\\nAcoustics, Speech and Signal Processing (2012): 5149–5152, https://doi.org/10.1109/ICASSP .2012.6289079.\\nSubword Tokenization\\nThe basic idea behind subword tokenization is to combine the best aspects of charac‐\\nter and word tokenization. On the one hand, we want to split rare words into smaller\\nunits to allow the model to deal with complex words and misspellings. On the other\\nhand, we want to keep frequent words as unique entities so that we can keep the\\nlength of our inputs to a manageable size. The main distinguishing feature of\\nsubword tokenization (as well as word tokenization) is that it is learned from the pre‐\\ntraining corpus using a mix of statistical rules and algorithms.\\nThere are several subword tokenization algorithms that are commonly used in NLP ,\\nbut let’s start with WordPiece,5 which is used by the BERT and DistilBERT tokenizers.\\nThe easiest way to understand how WordPiece works is to see it in action. \\n  Trans‐\\nformers provides a convenient AutoTokenizer class that allows you to quickly load\\nthe tokenizer associated with a pretrained model—we just call its from_pretrained()\\nmethod, providing the ID of a model on the Hub or a local file path. Let’s start by\\nloading the tokenizer for DistilBERT:\\nfrom transformers import AutoTokenizer\\nmodel_ckpt = \"distilbert-base-uncased\"\\ntokenizer = AutoTokenizer.from_pretrained(model_ckpt)\\nThe AutoTokenizer class belongs to a larger set of “auto” classes whose job is to auto‐\\nmatically retrieve the model’s configuration, pretrained weights, or vocabulary from\\nthe name of the checkpoint. This allows you to quickly switch between models, but if\\nyou wish to load the specific class manually you can do so as well. For example, we\\ncould have loaded the DistilBERT tokenizer as follows:\\nfrom transformers import DistilBertTokenizer\\ndistilbert_tokenizer = DistilBertTokenizer.from_pretrained(model_ckpt)\\nWhen you run the AutoTokenizer.from_pretrained() method\\nfor the first time you will see a progress bar that shows which\\nparameters of the pretrained tokenizer are loaded from the Hug‐\\nging Face Hub. When you run the code a second time, it will load\\nthe tokenizer from the cache, usually at ~/.cache/huggingface.\\nLet’s examine how this tokenizer works by feeding it our simple “Tokenizing text is a\\ncore task of NLP . ” example text:\\nFrom Text to Tokens | 33'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 57, 'page_label': '34'}, page_content=\"encoded_text = tokenizer(text)\\nprint(encoded_text)\\n{'input_ids': [101, 19204, 6026, 3793, 2003, 1037, 4563, 4708, 1997, 17953,\\n2361, 1012, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\\nJust as with character tokenization, we can see that the words have been mapped to\\nunique integers in the input_ids field. We’ll discuss the role of the attention_mask\\nfield in the next section. Now that we have the input_ids, we can convert them back\\ninto tokens by using the tokenizer’s convert_ids_to_tokens() method:\\ntokens = tokenizer.convert_ids_to_tokens(encoded_text.input_ids)\\nprint(tokens)\\n['[CLS]', 'token', '##izing', 'text', 'is', 'a', 'core', 'task', 'of', 'nl',\\n'##p', '.', '[SEP]']\\nWe can observe three things here. First, some special [CLS] and [SEP] tokens have\\nbeen added to the start and end of the sequence. These tokens differ from model to\\nmodel, but their main role is to indicate the start and end of a sequence. Second, the\\ntokens have each been lowercased, which is a feature of this particular checkpoint.\\nFinally, we can see that “tokenizing” and “NLP” have been split into two tokens,\\nwhich makes sense since they are not common words. The ## prefix in ##izing and\\n##p means that the preceding string is not whitespace; any token with this prefix\\nshould be merged with the previous token when you convert the tokens back to a\\nstring. The AutoTokenizer class has a convert_tokens_to_string() method for\\ndoing just that, so let’s apply it to our tokens:\\nprint(tokenizer.convert_tokens_to_string(tokens))\\n[CLS] tokenizing text is a core task of nlp. [SEP]\\nThe AutoTokenizer class also has several attributes that provide information about\\nthe tokenizer. For example, we can inspect the vocabulary size:\\ntokenizer.vocab_size\\n30522\\nand the corresponding model’s maximum context size:\\ntokenizer.model_max_length\\n512\\nAnother interesting attribute to know about is the names of the fields that the model\\nexpects in its forward pass:\\ntokenizer.model_input_names\\n['input_ids', 'attention_mask']\\nNow that we have a basic understanding of the tokenization process for a single\\nstring, let’s see how we can tokenize the whole dataset!\\n34 | Chapter 2: Text Classification\"), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 58, 'page_label': '35'}, page_content='When using pretrained models, it is really important to make sure\\nthat you use the same tokenizer that the model was trained with.\\nFrom the model’s perspective, switching the tokenizer is like shuf‐\\nfling the vocabulary. If everyone around you started swapping\\nrandom words like “house” for “cat, ” you’ d have a hard time under‐\\nstanding what was going on too!\\nTokenizing the Whole Dataset\\nTo tokenize the whole corpus, we’ll use the map() method of our DatasetDict object.\\nWe’ll encounter this method many times throughout this book, as it provides a con‐\\nvenient way to apply a processing function to each element in a dataset. As we’ll soon\\nsee, the map() method can also be used to create new rows and columns.\\nTo get started, the first thing we need is a processing function to tokenize our exam‐\\nples with:\\ndef tokenize(batch):\\n    return tokenizer(batch[\"text\"], padding=True, truncation=True)\\nThis function applies the tokenizer to a batch of examples; padding=True will pad the\\nexamples with zeros to the size of the longest one in a batch, and truncation=True\\nwill truncate the examples to the model’s maximum context size. To see tokenize()\\nin action, let’s pass a batch of two examples from the training set:\\nprint(tokenize(emotions[\"train\"][:2]))\\n{\\'input_ids\\': [[101, 1045, 2134, 2102, 2514, 26608, 102, 0, 0, 0, 0, 0, 0, 0, 0,\\n0, 0, 0, 0, 0, 0, 0, 0], [101, 1045, 2064, 2175, 2013, 3110, 2061, 20625, 2000,\\n2061, 9636, 17772, 2074, 2013, 2108, 2105, 2619, 2040, 14977, 1998, 2003, 8300,\\n102]], \\'attention_mask\\': [[1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\\n0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\\n1, 1]]}\\nHere we can see the result of padding: the first element of input_ids is shorter than\\nthe second, so zeros have been added to that element to make them the same length.\\nThese zeros have a corresponding [PAD] token in the vocabulary, and the set of spe‐\\ncial tokens also includes the [CLS] and [SEP] tokens that we encountered earlier:\\nSpecial Token [PAD] [UNK] [CLS] [SEP] [MASK]\\nSpecial Token ID 0 100 101 102 103\\nAlso note that in addition to returning the encoded tweets as input_ids, the token‐\\nizer returns a list of attention_mask arrays. This is because we do not want the\\nmodel to get confused by the additional padding tokens: the attention mask allows\\nthe model to ignore the padded parts of the input. Figure 2-3  provides a visual\\nexplanation of how the input IDs and attention masks are padded.\\nFrom Text to Tokens | 35'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 59, 'page_label': '36'}, page_content='Figure 2-3. For each batch, the input sequences are padded to the maximum sequence\\nlength in the batch; the attention mask is used in the model to ignore the padded areas of\\nthe input tensors\\nOnce we’ve defined a processing function, we can apply it across all the splits in the\\ncorpus in a single line of code:\\nemotions_encoded = emotions.map(tokenize, batched=True, batch_size=None)\\nBy default, the map() method operates individually on every example in the corpus,\\nso setting batched=True will encode the tweets in batches. Because we’ve set\\nbatch_size=None, our tokenize() function will be applied on the full dataset as a\\nsingle batch. This ensures that the input tensors and attention masks have the same\\nshape globally, and we can see that this operation has added new input_ids and\\nattention_mask columns to the dataset:\\nprint(emotions_encoded[\"train\"].column_names)\\n[\\'attention_mask\\', \\'input_ids\\', \\'label\\', \\'text\\']\\nIn later chapters, we’ll see how data collators can be used to dynam‐\\nically pad the tensors in each batch. Padding globally will come in\\nhandy in the next section, where we extract a feature matrix from\\nthe whole corpus.\\nTraining a Text Classifier\\nAs discussed in Chapter 1, models like DistilBERT are pretrained to predict masked\\nwords in a sequence of text. However, we can’t use these language models directly for\\ntext classification; we need to modify them slightly. To understand what modifica‐\\ntions are necessary, let’s take a look at the architecture of an encoder-based model like\\nDistilBERT, which is depicted in Figure 2-4.\\n36 | Chapter 2: Text Classification'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 60, 'page_label': '37'}, page_content='6 In the case of DistilBERT, it’s guessing the masked tokens.\\nFigure 2-4. The architecture used for sequence classification with an encoder-based\\ntransformer; it consists of the model’s pretrained body combined with a custom classifi‐\\ncation head\\nFirst, the text is tokenized and represented as one-hot vectors called token encodings.\\nThe size of the tokenizer vocabulary determines the dimension of the token encod‐\\nings, and it usually consists of 20k–200k unique tokens. Next, these token encodings\\nare converted to token embeddings, which are vectors living in a lower-dimensional\\nspace. The token embeddings are then passed through the encoder block layers to\\nyield a hidden state for each input token. For the pretraining objective of language\\nmodeling,6 each hidden state is fed to a layer that predicts the masked input tokens.\\nFor the classification task, we replace the language modeling layer with a classifica‐\\ntion layer.\\nIn practice, PyTorch skips the step of creating one-hot vectors for\\ntoken encodings because multiplying a matrix with a one-hot vec‐\\ntor is the same as selecting a column from the matrix. This can be\\ndone directly by getting the column with the token ID from the\\nmatrix. We’ll see this in Chapter 3 when we use the nn.Embedding\\nclass.\\nWe have two options to train such a model on our Twitter dataset:\\nFeature extraction\\nWe use the hidden states as features and just train a classifier on them, without\\nmodifying the pretrained model.\\nTraining a Text Classifier | 37'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 61, 'page_label': '38'}, page_content='Fine-tuning\\nWe train the whole model end-to-end, which also updates the parameters of the\\npretrained model.\\nIn the following sections we explore both options for DistilBERT and examine their\\ntrade-offs.\\nTransformers as Feature Extractors\\nUsing a transformer as a feature extractor is fairly simple. As shown in Figure 2-5, we\\nfreeze the body’s weights during training and use the hidden states as features for the\\nclassifier. The advantage of this approach is that we can quickly train a small or shal‐\\nlow model. Such a model could be a neural classification layer or a method that does\\nnot rely on gradients, such as a random forest. This method is especially convenient if\\nGPUs are unavailable, since the hidden states only need to be precomputed once.\\nFigure 2-5. In the feature-based approach, the DistilBERT model is frozen and just pro‐\\nvides features for a classifier\\nUsing pretrained models\\nWe will use another convenient auto class from \\n  Transformers called AutoModel.\\nSimilar to the AutoTokenizer class, AutoModel has a from_pretrained() method to\\nload the weights of a pretrained model. Let’s use this method to load the DistilBERT\\ncheckpoint:\\nfrom transformers import AutoModel\\nmodel_ckpt = \"distilbert-base-uncased\"\\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\\nmodel = AutoModel.from_pretrained(model_ckpt).to(device)\\nHere we’ve used PyTorch to check whether a GPU is available or not, and then\\nchained the PyTorch nn.Module.to() method to the model loader. This ensures that\\n38 | Chapter 2: Text Classification'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 62, 'page_label': '39'}, page_content='the model will run on the GPU if we have one. If not, the model will run on the CPU,\\nwhich can be considerably slower.\\nThe AutoModel class converts the token encodings to embeddings, and then feeds\\nthem through the encoder stack to return the hidden states. Let’s take a look at how\\nwe can extract these states from our corpus.\\nInteroperability Between Frameworks\\nAlthough the code in this book is mostly written in PyTorch, \\n  Transformers pro‐\\nvides tight interoperability with TensorFlow and JAX. This means that you only need\\nto change a few lines of code to load a pretrained model in your favorite deep learn‐\\ning framework! For example, we can load DistilBERT in TensorFlow by using the\\nTFAutoModel class as follows:\\nfrom transformers import TFAutoModel\\ntf_model = TFAutoModel.from_pretrained(model_ckpt)\\nThis interoperability is especially useful when a model is only released in one frame‐\\nwork, but you’ d like to use it in another. For example, the XLM-RoBERTa model that\\nwe’ll encounter in Chapter 4  only has PyTorch weights, so if you try to load it in\\nTensorFlow as we did before:\\ntf_xlmr = TFAutoModel.from_pretrained(\"xlm-roberta-base\")\\nyou’ll get an error. In these cases, you can specify a from_pt=True argument to the\\nTfAutoModel.from_pretrained() function, and the library will automatically down‐\\nload and convert the PyTorch weights for you:\\ntf_xlmr = TFAutoModel.from_pretrained(\"xlm-roberta-base\", from_pt=True)\\nAs you can see, it is very simple to switch between frameworks in \\n  Transformers! In\\nmost cases, you can just add a “TF” prefix to the classes and you’ll get the equivalent\\nTensorFlow 2.0 classes. When we use the \"pt\" string (e.g., in the following section),\\nwhich is short for PyTorch, just replace it with \"tf\", which is short for TensorFlow.\\nExtracting the last hidden states\\nTo warm up, let’s retrieve the last hidden states for a single string. The first thing we\\nneed to do is encode the string and convert the tokens to PyTorch tensors. This can\\nbe done by providing the return_tensors=\"pt\" argument to the tokenizer as follows:\\ntext = \"this is a test\"\\ninputs = tokenizer(text, return_tensors=\"pt\")\\nprint(f\"Input tensor shape: {inputs[\\'input_ids\\'].size()}\")\\nInput tensor shape: torch.Size([1, 6])\\nTraining a Text Classifier | 39'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 63, 'page_label': '40'}, page_content=\"As we can see, the resulting tensor has the shape [batch_size, n_tokens]. Now that\\nwe have the encodings as a tensor, the final step is to place them on the same device\\nas the model and pass the inputs as follows:\\ninputs = {k:v.to(device) for k,v in inputs.items()}\\nwith torch.no_grad():\\n    outputs = model(**inputs)\\nprint(outputs)\\nBaseModelOutput(last_hidden_state=tensor([[[-0.1565, -0.1862,  0.0528,  ...,\\n-0.1188,  0.0662,  0.5470],\\n         [-0.3575, -0.6484, -0.0618,  ..., -0.3040,  0.3508,  0.5221],\\n         [-0.2772, -0.4459,  0.1818,  ..., -0.0948, -0.0076,  0.9958],\\n         [-0.2841, -0.3917,  0.3753,  ..., -0.2151, -0.1173,  1.0526],\\n         [ 0.2661, -0.5094, -0.3180,  ..., -0.4203,  0.0144, -0.2149],\\n         [ 0.9441,  0.0112, -0.4714,  ...,  0.1439, -0.7288, -0.1619]]],\\n       device='cuda:0'), hidden_states=None, attentions=None)\\nHere we’ve used the torch.no_grad() context manager to disable the automatic cal‐\\nculation of the gradient. This is useful for inference since it reduces the memory foot‐\\nprint of the computations. Depending on the model configuration, the output can\\ncontain several objects, such as the hidden states, losses, or attentions, arranged in a\\nclass similar to a namedtuple in Python. In our example, the model output is an\\ninstance of BaseModelOutput, and we can simply access its attributes by name. The\\ncurrent model returns only one attribute, which is the last hidden state, so let’s exam‐\\nine its shape:\\noutputs.last_hidden_state.size()\\ntorch.Size([1, 6, 768])\\nLooking at the hidden state tensor, we see that it has the shape [batch_size,\\nn_tokens, hidden_dim]. In other words, a 768-dimensional vector is returned for\\neach of the 6 input tokens. For classification tasks, it is common practice to just use\\nthe hidden state associated with the [CLS] token as the input feature. Since this token\\nappears at the start of each sequence, we can extract it by simply indexing into\\noutputs.last_hidden_state as follows:\\noutputs.last_hidden_state[:,0].size()\\ntorch.Size([1, 768])\\nNow we know how to get the last hidden state for a single string; let’s do the same for\\nthe whole dataset by creating a new hidden_state column that stores all these vec‐\\ntors. As we did with the tokenizer, we’ll use the map() method of DatasetDict to\\nextract all the hidden states in one go. The first thing we need to do is wrap the previ‐\\nous steps in a processing function:\\ndef extract_hidden_states(batch):\\n    # Place model inputs on the GPU\\n    inputs = {k:v.to(device) for k,v in batch.items()\\n40 | Chapter 2: Text Classification\"), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 64, 'page_label': '41'}, page_content='if k in tokenizer.model_input_names}\\n    # Extract last hidden states\\n    with torch.no_grad():\\n        last_hidden_state = model(**inputs).last_hidden_state\\n    # Return vector for [CLS] token\\n    return {\"hidden_state\": last_hidden_state[:,0].cpu().numpy()}\\nThe only difference between this function and our previous logic is the final step\\nwhere we place the final hidden state back on the CPU as a NumPy array. The map()\\nmethod requires the processing function to return Python or NumPy objects when\\nwe’re using batched inputs.\\nSince our model expects tensors as inputs, the next thing to do is convert the\\ninput_ids and attention_mask columns to the \"torch\" format, as follows:\\nemotions_encoded.set_format(\"torch\",\\n                            columns=[\"input_ids\", \"attention_mask\", \"label\"])\\nWe can then go ahead and extract the hidden states across all splits in one go:\\nemotions_hidden = emotions_encoded.map(extract_hidden_states, batched=True)\\nNotice that we did not set batch_size=None in this case, which means the default\\nbatch_size=1000 is used instead. As expected, applying the extract_ hidden_\\nstates() function has added a new hidden_state column to our dataset:\\nemotions_hidden[\"train\"].column_names\\n[\\'attention_mask\\', \\'hidden_state\\', \\'input_ids\\', \\'label\\', \\'text\\']\\nNow that we have the hidden states associated with each tweet, the next step is to\\ntrain a classifier on them. To do that, we’ll need a feature matrix—let’s take a look.\\nCreating a feature matrix\\nThe preprocessed dataset now contains all the information we need to train a classi‐\\nfier on it. We will use the hidden states as input features and the labels as targets. We\\ncan easily create the corresponding arrays in the well-known Scikit-learn format as\\nfollows:\\nimport numpy as np\\nX_train = np.array(emotions_hidden[\"train\"][\"hidden_state\"])\\nX_valid = np.array(emotions_hidden[\"validation\"][\"hidden_state\"])\\ny_train = np.array(emotions_hidden[\"train\"][\"label\"])\\ny_valid = np.array(emotions_hidden[\"validation\"][\"label\"])\\nX_train.shape, X_valid.shape\\n((16000, 768), (2000, 768))\\nBefore we train a model on the hidden states, it’s good practice to perform a quick\\ncheck to ensure that they provide a useful representation of the emotions we want to\\nTraining a Text Classifier | 41'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 65, 'page_label': '42'}, page_content='7 L. McInnes, J. Healy, and J. Melville, “UMAP: Uniform Manifold Approximation and Projection for Dimen‐\\nsion Reduction”, (2018).\\nclassify. In the next section, we’ll see how visualizing the features provides a fast way\\nto achieve this.\\nVisualizing the training set\\nSince visualizing the hidden states in 768 dimensions is tricky to say the least, we’ll\\nuse the powerful UMAP algorithm to project the vectors down to 2D. 7 Since UMAP\\nworks best when the features are scaled to lie in the [0,1] interval, we’ll first apply a\\nMinMaxScaler and then use the UMAP implementation from the umap-learn library\\nto reduce the hidden states:\\nfrom umap import UMAP\\nfrom sklearn.preprocessing import MinMaxScaler\\n# Scale features to [0,1] range\\nX_scaled = MinMaxScaler().fit_transform(X_train)\\n# Initialize and fit UMAP\\nmapper = UMAP(n_components=2, metric=\"cosine\").fit(X_scaled)\\n# Create a DataFrame of 2D embeddings\\ndf_emb = pd.DataFrame(mapper.embedding_, columns=[\"X\", \"Y\"])\\ndf_emb[\"label\"] = y_train\\ndf_emb.head()\\nX Y label\\n0 4.358075 6.140816 0\\n1 -3.134567 5.329446 0\\n2 5.152230 2.732643 3\\n3 -2.519018 3.067250 2\\n4 -3.364520 3.356613 3\\nThe result is an array with the same number of training samples, but with only 2 fea‐\\ntures instead of the 768 we started with! Let’s investigate the compressed data a little\\nbit further and plot the density of points for each category separately:\\nfig, axes = plt.subplots(2, 3, figsize=(7,5))\\naxes = axes.flatten()\\ncmaps = [\"Greys\", \"Blues\", \"Oranges\", \"Reds\", \"Purples\", \"Greens\"]\\nlabels = emotions[\"train\"].features[\"label\"].names\\nfor i, (label, cmap) in enumerate(zip(labels, cmaps)):\\n    df_emb_sub = df_emb.query(f\"label == {i}\")\\n    axes[i].hexbin(df_emb_sub[\"X\"], df_emb_sub[\"Y\"], cmap=cmap,\\n                   gridsize=20, linewidths=(0,))\\n42 | Chapter 2: Text Classification'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 66, 'page_label': '43'}, page_content='axes[i].set_title(label)\\n    axes[i].set_xticks([]), axes[i].set_yticks([])\\nplt.tight_layout()\\nplt.show()\\nThese are only projections onto a lower-dimensional space. Just\\nbecause some categories overlap does not mean that they are not\\nseparable in the original space. Conversely, if they are separable in\\nthe projected space they will be separable in the original space.\\nFrom this plot we can see some clear patterns: the negative feelings such as sadness,\\nanger, and fear all occupy similar regions with slightly varying distributions. On the\\nother hand, joy and love are well separated from the negative emotions and also\\nshare a similar space. Finally, surprise is scattered all over the place. Although we\\nmay have hoped for some separation, this is in no way guaranteed since the model\\nwas not trained to know the difference between these emotions. It only learned them\\nimplicitly by guessing the masked words in texts.\\nNow that we’ve gained some insight into the features of our dataset, let’s finally train a\\nmodel on it!\\nTraining a simple classifier\\nWe’ve seen that the hidden states are somewhat different between the emotions,\\nalthough for several of them there is no obvious boundary. Let’s use these hidden\\nstates to train a logistic regression model with Scikit-learn. Training such a simple\\nmodel is fast and does not require a GPU:\\nTraining a Text Classifier | 43'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 67, 'page_label': '44'}, page_content='from sklearn.linear_model import LogisticRegression\\n# We increase `max_iter` to guarantee convergence\\nlr_clf = LogisticRegression(max_iter=3000)\\nlr_clf.fit(X_train, y_train)\\nlr_clf.score(X_valid, y_valid)\\n0.633\\nLooking at the accuracy, it might appear that our model is just a bit better than ran‐\\ndom—but since we are dealing with an unbalanced multiclass dataset, it’s actually sig‐\\nnificantly better. We can examine whether our model is any good by comparing it\\nagainst a simple baseline. In Scikit-learn there is a DummyClassifier that can be used\\nto build a classifier with simple heuristics such as always choosing the majority class\\nor always drawing a random class. In this case the best-performing heuristic is to\\nalways choose the most frequent class, which yields an accuracy of about 35%:\\nfrom sklearn.dummy import DummyClassifier\\ndummy_clf = DummyClassifier(strategy=\"most_frequent\")\\ndummy_clf.fit(X_train, y_train)\\ndummy_clf.score(X_valid, y_valid)\\n0.352\\nSo, our simple classifier with DistilBERT embeddings is significantly better than our\\nbaseline. We can further investigate the performance of the model by looking at the\\nconfusion matrix of the classifier, which tells us the relationship between the true and\\npredicted labels:\\nfrom sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\\ndef plot_confusion_matrix(y_preds, y_true, labels):\\n    cm = confusion_matrix(y_true, y_preds, normalize=\"true\")\\n    fig, ax = plt.subplots(figsize=(6, 6))\\n    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\\n    disp.plot(cmap=\"Blues\", values_format=\".2f\", ax=ax, colorbar=False)\\n    plt.title(\"Normalized confusion matrix\")\\n    plt.show()\\ny_preds = lr_clf.predict(X_valid)\\nplot_confusion_matrix(y_preds, y_valid, labels)\\n44 | Chapter 2: Text Classification'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 68, 'page_label': '45'}, page_content='We can see that anger and fear are most often confused with sadness, which agrees\\nwith the observation we made when visualizing the embeddings. Also, love and\\nsurprise are frequently mistaken for joy.\\nIn the next section we will explore the fine-tuning approach, which leads to superior\\nclassification performance. It is, however, important to note that doing this requires\\nmore computational resources, such as GPUs, that might not be available in your\\norganization. In cases like these, a feature-based approach can be a good compromise\\nbetween doing traditional machine learning and deep learning.\\nFine-Tuning Transformers\\nLet’s now explore what it takes to fine-tune a transformer end-to-end. With the fine-\\ntuning approach we do not use the hidden states as fixed features, but instead train\\nthem as shown in Figure 2-6. This requires the classification head to be differentiable,\\nwhich is why this method usually uses a neural network for classification.\\nTraining a Text Classifier | 45'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 69, 'page_label': '46'}, page_content='Figure 2-6. When using the fine-tuning approach the whole DistilBERT model is trained\\nalong with the classification head\\nTraining the hidden states that serve as inputs to the classification model will help us\\navoid the problem of working with data that may not be well suited for the classifica‐\\ntion task. Instead, the initial hidden states adapt during training to decrease the\\nmodel loss and thus increase its performance.\\nWe’ll be using the Trainer API from \\n  Transformers to simplify the training loop.\\nLet’s look at the ingredients we need to set one up!\\nLoading a pretrained model\\nThe first thing we need is a pretrained DistilBERT model like the one we used in the\\nfeature-based approach. The only slight modification is that we use the AutoModelFor\\nSequenceClassification model instead of AutoModel. The difference is that the\\nAutoModelForSequenceClassification model has a classification head on top of the\\npretrained model outputs, which can be easily trained with the base model. We just\\nneed to specify how many labels the model has to predict (six in our case), since this\\ndictates the number of outputs the classification head has:\\nfrom transformers import AutoModelForSequenceClassification\\nnum_labels = 6\\nmodel = (AutoModelForSequenceClassification\\n         .from_pretrained(model_ckpt, num_labels=num_labels)\\n         .to(device))\\nY ou will see a warning that some parts of the model are randomly initialized. This\\nis normal since the classification head has not yet been trained. The next step is to\\ndefine the metrics that we’ll use to evaluate our model’s performance during\\nfine-tuning.\\n46 | Chapter 2: Text Classification'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 70, 'page_label': '47'}, page_content='Defining the performance metrics\\nTo monitor metrics during training, we need to define a compute_metrics() function\\nfor the Trainer. This function receives an EvalPrediction object (which is a named\\ntuple with predictions and label_ids attributes) and needs to return a dictionary\\nthat maps each metric’s name to its value. For our application, we’ll compute the\\nF1-score and the accuracy of the model as follows:\\nfrom sklearn.metrics import accuracy_score, f1_score\\ndef compute_metrics(pred):\\n    labels = pred.label_ids\\n    preds = pred.predictions.argmax(-1)\\n    f1 = f1_score(labels, preds, average=\"weighted\")\\n    acc = accuracy_score(labels, preds)\\n    return {\"accuracy\": acc, \"f1\": f1}\\nWith the dataset and metrics ready, we just have two final things to take care of before\\nwe define the Trainer class:\\n1. Log in to our account on the Hugging Face Hub. This will allow us to push our\\nfine-tuned model to our account on the Hub and share it with the community.\\n2. Define all the hyperparameters for the training run.\\nWe’ll tackle these steps in the next section.\\nTraining the model\\nIf you’re running this code in a Jupyter notebook, you can log in to the Hub with the\\nfollowing helper function:\\nfrom huggingface_hub import notebook_login\\nnotebook_login()\\nThis will display a widget in which you can enter your username and password, or an\\naccess token with write privileges. Y ou can find details on how to create access tokens\\nin the Hub documentation. If you’re working in the terminal, you can log in by run‐\\nning the following command:\\n$ huggingface-cli login\\nTo define the training parameters, we use the TrainingArguments class. This class\\nstores a lot of information and gives you fine-grained control over the training and\\nevaluation. The most important argument to specify is output_dir, which is where\\nall the artifacts from training are stored. Here is an example of TrainingArguments in\\nall its glory:\\nTraining a Text Classifier | 47'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 71, 'page_label': '48'}, page_content='from transformers import Trainer, TrainingArguments\\nbatch_size = 64\\nlogging_steps = len(emotions_encoded[\"train\"]) // batch_size\\nmodel_name = f\"{model_ckpt}-finetuned-emotion\"\\ntraining_args = TrainingArguments(output_dir=model_name,\\n                                  num_train_epochs=2,\\n                                  learning_rate=2e-5,\\n                                  per_device_train_batch_size=batch_size,\\n                                  per_device_eval_batch_size=batch_size,\\n                                  weight_decay=0.01,\\n                                  evaluation_strategy=\"epoch\",\\n                                  disable_tqdm=False,\\n                                  logging_steps=logging_steps,\\n                                  push_to_hub=True,\\n                                  log_level=\"error\")\\nHere we also set the batch size, learning rate, and number of epochs, and specify to\\nload the best model at the end of the training run. With this final ingredient, we can\\ninstantiate and fine-tune our model with the Trainer:\\nfrom transformers import Trainer\\ntrainer = Trainer(model=model, args=training_args,\\n                  compute_metrics=compute_metrics,\\n                  train_dataset=emotions_encoded[\"train\"],\\n                  eval_dataset=emotions_encoded[\"validation\"],\\n                  tokenizer=tokenizer)\\ntrainer.train();\\nEpoch Training Loss Validation Loss Accuracy F1\\n1 0.840900 0.327445 0.896500 0.892285\\n2 0.255000 0.220472 0.922500 0.922550\\nLooking at the logs, we can see that our model has an F1-score on the validation set of\\naround 92%—this is a significant improvement over the feature-based approach!\\nWe can take a more detailed look at the training metrics by calculating the confusion\\nmatrix. To visualize the confusion matrix, we first need to get the predictions on the\\nvalidation set. The predict() method of the Trainer class returns several useful\\nobjects we can use for evaluation:\\npreds_output = trainer.predict(emotions_encoded[\"validation\"])\\nThe output of the predict() method is a PredictionOutput object that contains\\narrays of predictions and label_ids, along with the metrics we passed to the\\ntrainer. For example, the metrics on the validation set can be accessed as follows:\\npreds_output.metrics\\n48 | Chapter 2: Text Classification'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 72, 'page_label': '49'}, page_content=\"{'test_loss': 0.22047173976898193,\\n 'test_accuracy': 0.9225,\\n 'test_f1': 0.9225500751072866,\\n 'test_runtime': 1.6357,\\n 'test_samples_per_second': 1222.725,\\n 'test_steps_per_second': 19.564}\\nIt also contains the raw predictions for each class. We can decode the predictions\\ngreedily using np.argmax(). This yields the predicted labels and has the same format\\nas the labels returned by the Scikit-learn models in the feature-based approach:\\ny_preds = np.argmax(preds_output.predictions, axis=1)\\nWith the predictions, we can plot the confusion matrix again:\\nplot_confusion_matrix(y_preds, y_valid, labels)\\nThis is much closer to the ideal diagonal confusion matrix. The love category is still\\noften confused with joy, which seems natural. surprise is also frequently mistaken\\nfor joy, or confused with fear. Overall the performance of the model seems quite\\ngood, but before we call it a day, let’s dive a little deeper into the types of errors our\\nmodel is likely to make.\\nTraining a Text Classifier | 49\"), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 73, 'page_label': '50'}, page_content='Fine-Tuning with Keras\\nIf you are using TensorFlow, it’s also possible to fine-tune your models using the\\nKeras API. The main difference from the PyTorch API is that there is no Trainer\\nclass, since Keras models already provide a built-in fit() method. To see how this\\nworks, let’s first load DistilBERT as a TensorFlow model:\\nfrom transformers import TFAutoModelForSequenceClassification\\ntf_model = (TFAutoModelForSequenceClassification\\n            .from_pretrained(model_ckpt, num_labels=num_labels))\\nNext, we’ll convert our datasets into the tf.data.Dataset format. Because we have\\nalready padded our tokenized inputs, we can do this conversion easily by applying the\\nto_tf_dataset() method to emotions_encoded:\\n# The column names to convert to TensorFlow tensors\\ntokenizer_columns = tokenizer.model_input_names\\ntf_train_dataset = emotions_encoded[\"train\"].to_tf_dataset(\\n    columns=tokenizer_columns, label_cols=[\"label\"], shuffle=True,\\n    batch_size=batch_size)\\ntf_eval_dataset = emotions_encoded[\"validation\"].to_tf_dataset(\\n    columns=tokenizer_columns, label_cols=[\"label\"], shuffle=False,\\n    batch_size=batch_size)\\nHere we’ve also shuffled the training set, and defined the batch size for it and the vali‐\\ndation set. The last thing to do is compile and train the model:\\nimport tensorflow as tf\\ntf_model.compile(\\n    optimizer=tf.keras.optimizers.Adam(learning_rate=5e-5),\\n    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\\n    metrics=tf.metrics.SparseCategoricalAccuracy())\\ntf_model.fit(tf_train_dataset, validation_data=tf_eval_dataset, epochs=2)\\nError analysis\\nBefore moving on, we should investigate our model’s predictions a little bit further. A\\nsimple yet powerful technique is to sort the validation samples by the model loss.\\nWhen we pass the label during the forward pass, the loss is automatically calculated\\nand returned. Here’s a function that returns the loss along with the predicted label:\\nfrom torch.nn.functional import cross_entropy\\ndef forward_pass_with_label(batch):\\n    # Place all input tensors on the same device as the model\\n    inputs = {k:v.to(device) for k,v in batch.items()\\n50 | Chapter 2: Text Classification'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 74, 'page_label': '51'}, page_content='if k in tokenizer.model_input_names}\\n    with torch.no_grad():\\n        output = model(**inputs)\\n        pred_label = torch.argmax(output.logits, axis=-1)\\n        loss = cross_entropy(output.logits, batch[\"label\"].to(device),\\n                             reduction=\"none\")\\n    # Place outputs on CPU for compatibility with other dataset columns\\n    return {\"loss\": loss.cpu().numpy(),\\n            \"predicted_label\": pred_label.cpu().numpy()}\\nUsing the map() method once more, we can apply this function to get the losses for all\\nthe samples:\\n# Convert our dataset back to PyTorch tensors\\nemotions_encoded.set_format(\"torch\",\\n                            columns=[\"input_ids\", \"attention_mask\", \"label\"])\\n# Compute loss values\\nemotions_encoded[\"validation\"] = emotions_encoded[\"validation\"].map(\\n    forward_pass_with_label, batched=True, batch_size=16)\\nFinally, we create a DataFrame with the texts, losses, and predicted/true labels:\\nemotions_encoded.set_format(\"pandas\")\\ncols = [\"text\", \"label\", \"predicted_label\", \"loss\"]\\ndf_test = emotions_encoded[\"validation\"][:][cols]\\ndf_test[\"label\"] = df_test[\"label\"].apply(label_int2str)\\ndf_test[\"predicted_label\"] = (df_test[\"predicted_label\"]\\n                              .apply(label_int2str))\\nWe can now easily sort emotions_encoded by the losses in either ascending or\\ndescending order. The goal of this exercise is to detect one of the following:\\nWrong labels\\nEvery process that adds labels to data can be flawed. Annotators can make mis‐\\ntakes or disagree, while labels that are inferred from other features can be wrong.\\nIf it was easy to automatically annotate data, then we would not need a model to\\ndo it. Thus, it is normal that there are some wrongly labeled examples. With this\\napproach, we can quickly find and correct them.\\nQuirks of the dataset\\nDatasets in the real world are always a bit messy. When working with text, special\\ncharacters or strings in the inputs can have a big impact on the model’s predic‐\\ntions. Inspecting the model’s weakest predictions can help identify such features,\\nand cleaning the data or injecting similar examples can make the model more\\nrobust.\\nLet’s first have a look at the data samples with the highest losses:\\ndf_test.sort_values(\"loss\", ascending=False).head(10)\\nTraining a Text Classifier | 51'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 75, 'page_label': '52'}, page_content='text label predicted_label loss\\ni feel that he was being overshadowed by the supporting characters love sadness 5.704531\\ni called myself pro life and voted for perry without knowing this information i\\nwould feel betrayed but moreover i would feel that i had betrayed god by\\nsupporting a man who mandated a barely year old vaccine for little girls putting\\nthem in danger to financially support people close to him\\njoy sadness 5.484461\\ni guess i feel betrayed because i admired him so much and for someone to do this\\nto his wife and kids just goes beyond the pale\\njoy sadness 5.434768\\ni feel badly about reneging on my commitment to bring donuts to the faithful at\\nholy family catholic church in columbus ohio\\nlove sadness 5.257482\\ni as representative of everything thats wrong with corporate america and feel that\\nsending him to washington is a ludicrous idea\\nsurprise sadness 4.827708\\ni guess this is a memoir so it feels like that should be fine too except i dont know\\nsomething about such a deep amount of self absorption made me feel\\nuncomfortable\\njoy fear 4.713047\\ni am going to several holiday parties and i can t wait to feel super awkward i am\\ngoing to several holiday parties and i can t wait to feel super awkward a href http\\nbadplaydate\\njoy sadness 4.704955\\ni felt ashamed of these feelings and was scared because i knew that something\\nwrong with me and thought i might be gay\\nfear sadness 4.656096\\ni guess we would naturally feel a sense of loneliness even the people who said\\nunkind things to you might be missed\\nanger sadness 4.593202\\nim lazy my characters fall into categories of smug and or blas people and their foils\\npeople who feel inconvenienced by smug and or blas people\\njoy fear 4.311287\\nWe can clearly see that the model predicted some of the labels incorrectly. On the\\nother hand, it seems that there are quite a few examples with no clear class, which\\nmight be either mislabeled or require a new class altogether. In particular, joy seems\\nto be mislabeled several times. With this information we can refine the dataset, which\\noften can lead to as big a performance gain (or more) as having more data or larger\\nmodels!\\nWhen looking at the samples with the lowest losses, we observe that the model seems\\nto be most confident when predicting the sadness class. Deep learning models are\\nexceptionally good at finding and exploiting shortcuts to get to a prediction. For this\\nreason, it is also worth investing time into looking at the examples that the model is\\nmost confident about, so that we can be confident that the model does not improp‐\\nerly exploit certain features of the text. So, let’s also look at the predictions with the\\nsmallest loss:\\ndf_test.sort_values(\"loss\", ascending=True).head(10)\\n52 | Chapter 2: Text Classification'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 76, 'page_label': '53'}, page_content='text label predicted_label loss\\ni feel try to tell me im ungrateful tell me im basically the worst daughter sister in\\nthe world\\nsadness sadness 0.017331\\nim kinda relieve but at the same time i feel disheartened sadness sadness 0.017392\\ni and feel quite ungrateful for it but i m looking forward to summer and warmth\\nand light nights\\nsadness sadness 0.017400\\ni remember feeling disheartened one day when we were studying a poem really\\ndissecting it verse by verse stanza by stanza\\nsadness sadness 0.017461\\ni feel like an ungrateful asshole sadness sadness 0.017485\\ni leave the meeting feeling more than a little disheartened sadness sadness 0.017670\\ni am feeling a little disheartened sadness sadness 0.017685\\ni feel like i deserve to be broke with how frivolous i am sadness sadness 0.017888\\ni started this blog with pure intentions i must confess to starting to feel a little\\ndisheartened lately by the knowledge that there doesnt seem to be anybody\\nreading it\\nsadness sadness 0.017899\\ni feel so ungrateful to be wishing this pregnancy over now sadness sadness 0.017913\\nWe now know that the joy is sometimes mislabeled and that the model is most confi‐\\ndent about predicting the label sadness. With this information we can make targeted\\nimprovements to our dataset, and also keep an eye on the class the model seems to be\\nvery confident about.\\nThe last step before serving the trained model is to save it for later usage. \\n  Trans‐\\nformers allows us to do this in a few steps, which we’ll show you in the next section.\\nSaving and sharing the model\\nThe NLP community benefits greatly from sharing pretrained and fine-tuned models,\\nand everybody can share their models with others via the Hugging Face Hub. Any\\ncommunity-generated model can be downloaded from the Hub just like we downloa‐\\nded the DistilBERT model. With the Trainer API, saving and sharing a model is\\nsimple:\\ntrainer.push_to_hub(commit_message=\"Training completed!\")\\nWe can also use the fine-tuned model to make predictions on new tweets. Since we’ve\\npushed our model to the Hub, we can now use it with the pipeline() function, just\\nlike we did in Chapter 1. First, let’s load the pipeline:\\nfrom transformers import pipeline\\n# Change `transformersbook` to your Hub username\\nmodel_id = \"transformersbook/distilbert-base-uncased-finetuned-emotion\"\\nclassifier = pipeline(\"text-classification\", model=model_id)\\nTraining a Text Classifier | 53'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 77, 'page_label': '54'}, page_content='Then let’s test the pipeline with a sample tweet:\\ncustom_tweet = \"I saw a movie today and it was really good.\"\\npreds = classifier(custom_tweet, return_all_scores=True)\\nFinally, we can plot the probability for each class in a bar plot. Clearly, the model esti‐\\nmates that the most likely class is joy, which appears to be reasonable given the tweet:\\npreds_df = pd.DataFrame(preds[0])\\nplt.bar(labels, 100 * preds_df[\"score\"], color=\\'C0\\')\\nplt.title(f\\'\"{custom_tweet}\"\\')\\nplt.ylabel(\"Class probability (%)\")\\nplt.show()\\nConclusion\\nCongratulations, you now know how to train a transformer model to classify the\\nemotions in tweets! We have seen two complementary approaches based on features\\nand fine-tuning, and investigated their strengths and weaknesses.\\nHowever, this is just the first step in building a real-world application with trans‐\\nformer models, and we have a lot more ground to cover. Here’s a list of challenges\\nyou’re likely to experience in your NLP journey:\\nMy boss wants my model in production yesterday!\\nIn most applications, your model doesn’t just sit somewhere gathering dust—you\\nwant to make sure it’s serving predictions! When a model is pushed to the Hub,\\nan inference endpoint is automatically created that can be called with HTTP\\nrequests. We recommend checking out the documentation of the Inference API if\\nyou want to learn more.\\n54 | Chapter 2: Text Classification'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 78, 'page_label': '55'}, page_content='My users want faster predictions!\\nWe’ve already seen one approach to this problem: using DistilBERT. In Chapter 8\\nwe’ll dive into knowledge distillation (the process by which DistilBERT was cre‐\\nated), along with other tricks to speed up your transformer models.\\nCan your model also do X?\\nAs we’ve alluded to in this chapter, transformers are extremely versatile. In the\\nrest of the book we will be exploring a range of tasks, like question answering and\\nnamed entity recognition, all using the same basic architecture.\\nNone of my texts are in English!\\nIt turns out that transformers also come in a multilingual variety, and we’ll use\\nthem in Chapter 4 to tackle several languages at once.\\nI don’t have any labels!\\nIf there is very little labeled data available, fine-tuning may not be an option. In\\nChapter 9, we’ll explore some techniques to deal with this situation.\\nNow that we’ve seen what’s involved in training and sharing a transformer, in the next\\nchapter we’ll explore implementing our very own transformer model from scratch.\\nConclusion | 55'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 79, 'page_label': '56'}, page_content=''), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 80, 'page_label': '57'}, page_content='CHAPTER 3\\nTransformer Anatomy\\nIn Chapter 2, we saw what it takes to fine-tune and evaluate a transformer. Now let’s\\ntake a look at how they work under the hood. In this chapter we’ll explore the main\\nbuilding blocks of transformer models and how to implement them using PyTorch.\\nWe’ll also provide guidance on how to do the same in TensorFlow. We’ll first focus on\\nbuilding the attention mechanism, and then add the bits and pieces necessary to\\nmake a transformer encoder work. We’ll also have a brief look at the architectural dif‐\\nferences between the encoder and decoder modules. By the end of this chapter you\\nwill be able to implement a simple transformer model yourself!\\nWhile a deep technical understanding of the Transformer architecture is generally\\nnot necessary to use \\n  Transformers and fine-tune models for your use case, it can\\nbe helpful for comprehending and navigating the limitations of transformers and\\nusing them in new domains.\\nThis chapter also introduces a taxonomy of transformers to help you understand the\\nzoo of models that have emerged in recent years. Before diving into the code, let’s\\nstart with an overview of the original architecture that kick-started the transformer\\nrevolution.\\nThe Transformer Architecture\\nAs we saw in Chapter 1 , the original Transformer is based on the encoder-decoder\\narchitecture that is widely used for tasks like machine translation, where a sequence\\nof words is translated from one language to another. This architecture consists of two\\ncomponents:\\nEncoder\\nConverts an input sequence of tokens into a sequence of embedding vectors,\\noften called the hidden state or context\\n57'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 81, 'page_label': '58'}, page_content='Decoder\\nUses the encoder’s hidden state to iteratively generate an output sequence of\\ntokens, one token at a time\\nAs illustrated in Figure 3-1 , the encoder and decoder are themselves composed of\\nseveral building blocks.\\nFigure 3-1. Encoder-decoder architecture of the transformer, with the encoder shown in\\nthe upper half of the figure and the decoder in the lower half\\nWe’ll look at each of the components in detail shortly, but we can already see a few\\nthings in Figure 3-1 that characterize the Transformer architecture:\\n• The input text is tokenized and converted to token embeddings using the techni‐\\nques we encountered in Chapter 2. Since the attention mechanism is not aware of\\nthe relative positions of the tokens, we need a way to inject some information\\nabout token positions into the input to model the sequential nature of text. The\\ntoken embeddings are thus combined with positional embeddings that contain\\npositional information for each token.\\n• The encoder is composed of a stack of encoder layers or “blocks, ” which is analo‐\\ngous to stacking convolutional layers in computer vision. The same is true of the\\ndecoder, which has its own stack of decoder layers.\\n• The encoder’s output is fed to each decoder layer, and the decoder then generates\\na prediction for the most probable next token in the sequence. The output of this\\nstep is then fed back into the decoder to generate the next token, and so on until\\na special end-of-sequence (EOS) token is reached. In the example from\\nFigure 3-1, imagine the decoder has already predicted “Die” and “Zeit” . Now it\\n58 | Chapter 3: Transformer Anatomy'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 82, 'page_label': '59'}, page_content='1 Y . Liu and M. Lapata, “Text Summarization with Pretrained Encoder”, (2019).\\ngets these two as an input as well as all the encoder’s outputs to predict the next\\ntoken, “fliegt” . In the next step the decoder gets “fliegt” as an additional input. We\\nrepeat the process until the decoder predicts the EOS token or we reached a max‐\\nimum length.\\nThe Transformer architecture was originally designed for sequence-to-sequence tasks\\nlike machine translation, but both the encoder and decoder blocks were soon adapted\\nas standalone models. Although there are hundreds of different transformer models,\\nmost of them belong to one of three types:\\nEncoder-only\\nThese models convert an input sequence of text into a rich numerical representa‐\\ntion that is well suited for tasks like text classification or named entity recogni‐\\ntion. BERT and its variants, like RoBERTa and DistilBERT, belong to this class of\\narchitectures. The representation computed for a given token in this architecture\\ndepends both on the left (before the token) and the right (after the token) con‐\\ntexts. This is often called bidirectional attention.\\nDecoder-only\\nGiven a prompt of text like “Thanks for lunch, I had a… ” these models will auto-\\ncomplete the sequence by iteratively predicting the most probable next word.\\nThe family of GPT models belong to this class. The representation computed for\\na given token in this architecture depends only on the left context. This is often\\ncalled causal or autoregressive attention.\\nEncoder-decoder\\nThese are used for modeling complex mappings from one sequence of text to\\nanother; they’re suitable for machine translation and summarization tasks. In\\naddition to the Transformer architecture, which as we’ve seen combines an\\nencoder and a decoder, the BART and T5 models belong to this class.\\nIn reality, the distinction between applications for decoder-only\\nversus encoder-only architectures is a bit blurry. For example,\\ndecoder-only models like those in the GPT family can be primed\\nfor tasks like translation that are conventionally thought of as\\nsequence-to-sequence tasks. Similarly, encoder-only models like\\nBERT can be applied to summarization tasks that are usually asso‐\\nciated with encoder-decoder or decoder-only models.1\\nNow that you have a high-level understanding of the Transformer architecture, let’s\\ntake a closer look at the inner workings of the encoder.\\nThe Transformer Architecture | 59'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 83, 'page_label': '60'}, page_content='The Encoder\\nAs we saw earlier, the transformer’s encoder consists of many encoder layers stacked\\nnext to each other. As illustrated in Figure 3-2, each encoder layer receives a sequence\\nof embeddings and feeds them through the following sublayers:\\n• A multi-head self-attention layer\\n• A fully connected feed-forward layer that is applied to each input embedding\\nThe output embeddings of each encoder layer have the same size as the inputs, and\\nwe’ll soon see that the main role of the encoder stack is to “update” the input embed‐\\ndings to produce representations that encode some contextual information in the\\nsequence. For example, the word “apple” will be updated to be more “company-like”\\nand less “fruit-like” if the words “keynote” or “phone” are close to it.\\nFigure 3-2. Zooming into the encoder layer\\nEach of these sublayers also uses skip connections and layer normalization, which are\\nstandard tricks to train deep neural networks effectively. But to truly understand what\\nmakes a transformer work, we have to go deeper. Let’s start with the most important\\nbuilding block: the self-attention layer.\\n60 | Chapter 3: Transformer Anatomy'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 84, 'page_label': '61'}, page_content='2 M.E. Peters et al., “Deep Contextualized Word Representations”, (2017).\\nSelf-Attention\\nAs we discussed in Chapter 1, attention is a mechanism that allows neural networks\\nto assign a different amount of weight or “attention” to each element in a sequence.\\nFor text sequences, the elements are token embeddings like the ones we encountered\\nin Chapter 2, where each token is mapped to a vector of some fixed dimension. For\\nexample, in BERT each token is represented as a 768-dimensional vector. The “self ”\\npart of self-attention refers to the fact that these weights are computed for all hidden\\nstates in the same set—for example, all the hidden states of the encoder. By contrast,\\nthe attention mechanism associated with recurrent models involves computing the\\nrelevance of each encoder hidden state to the decoder hidden state at a given decod‐\\ning timestep.\\nThe main idea behind self-attention is that instead of using a fixed embedding for\\neach token, we can use the whole sequence to compute a weighted average of each\\nembedding. Another way to formulate this is to say that given a sequence of token\\nembeddings x1, ...,xn, self-attention produces a sequence of new embeddings x1′, ...,xn′\\nwhere each xi′ is a linear combination of all the xj:\\nxi′ = ∑\\nj = 1\\nn\\nwjixj\\nThe coefficients wji are called attention weights and are normalized so that ∑j wji = 1.\\nTo see why averaging the token embeddings might be a good idea, consider what\\ncomes to mind when you see the word “flies” . Y ou might think of annoying insects,\\nbut if you were given more context, like “time flies like an arrow” , then you would\\nrealize that “flies” refers to the verb instead. Similarly, we can create a representation\\nfor “flies” that incorporates this context by combining all the token embeddings in\\ndifferent proportions, perhaps by assigning a larger weight wji to the token embed‐\\ndings for “time” and “arrow” . Embeddings that are generated in this way are called\\ncontextualized embeddings and predate the invention of transformers in language\\nmodels like ELMo.2 A diagram of the process is shown in Figure 3-3, where we illus‐\\ntrate how, depending on the context, two different representations for “flies” can be\\ngenerated via self-attention.\\nThe Encoder | 61'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 85, 'page_label': '62'}, page_content='3 A. Vaswani et al., “ Attention Is All Y ou Need”, (2017).\\nFigure 3-3. Diagram showing how self-attention updates raw token embeddings (upper)\\ninto contextualized embeddings (lower) to create representations that incorporate infor‐\\nmation from the whole sequence\\nLet’s now take a look at how we can calculate the attention weights.\\nScaled dot-product attention\\nThere are several ways to implement a self-attention layer, but the most common one\\nis scaled dot-product attention, from the paper introducing the Transformer architec‐\\nture.3 There are four main steps required to implement this mechanism:\\n1. Project each token embedding into three vectors called query, key, and value.\\n2. Compute attention scores. We determine how much the query and key vectors\\nrelate to each other using a similarity function. As the name suggests, the similar‐\\nity function for scaled dot-product attention is the dot product, computed effi‐\\nciently using matrix multiplication of the embeddings. Queries and keys that are\\nsimilar will have a large dot product, while those that don’t share much in com‐\\nmon will have little to no overlap. The outputs from this step are called the atten‐\\ntion scores, and for a sequence with n input tokens there is a corresponding n×n\\nmatrix of attention scores.\\n62 | Chapter 3: Transformer Anatomy'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 86, 'page_label': '63'}, page_content='3. Compute attention weights. Dot products can in general produce arbitrarily large\\nnumbers, which can destabilize the training process. To handle this, the attention\\nscores are first multiplied by a scaling factor to normalize their variance and then\\nnormalized with a softmax to ensure all the column values sum to 1. The result‐\\ning n × n matrix now contains all the attention weights, wji.\\n4. Update the token embeddings. Once the attention weights are computed, we\\nmultiply them by the value vector v1, ...,vn to obtain an updated representation\\nfor embedding xi′ = ∑ j wjivj.\\nWe can visualize how the attention weights are calculated with a nifty library called\\nBertViz for Jupyter. This library provides several functions that can be used to visual‐\\nize different aspects of attention in transformer models. To visualize the attention\\nweights, we can use the neuron_view module, which traces the computation of the\\nweights to show how the query and key vectors are combined to produce the final\\nweight. Since BertViz needs to tap into the attention layers of the model, we’ll instan‐\\ntiate our BERT checkpoint with the model class from BertViz and then use the\\nshow() function to generate the interactive visualization for a specific encoder layer\\nand attention head. Note that you need to click the “+” on the left to activate the\\nattention visualization:\\nfrom transformers import AutoTokenizer\\nfrom bertviz.transformers_neuron_view import BertModel\\nfrom bertviz.neuron_view import show\\nmodel_ckpt = \"bert-base-uncased\"\\ntokenizer = AutoTokenizer.from_pretrained(model_ckpt)\\nmodel = BertModel.from_pretrained(model_ckpt)\\ntext = \"time flies like an arrow\"\\nshow(model, \"bert\", tokenizer, text, display_mode=\"light\", layer=0, head=8)\\nFrom the visualization, we can see the values of the query and key vectors are repre‐\\nsented as vertical bands, where the intensity of each band corresponds to the magni‐\\ntude. The connecting lines are weighted according to the attention between the\\ntokens, and we can see that the query vector for “flies” has the strongest overlap with\\nthe key vector for “arrow” .\\nThe Encoder | 63'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 87, 'page_label': '64'}, page_content='Demystifying Queries, Keys, and Values\\nThe notion of query, key, and value vectors may seem a bit cryptic the first time you\\nencounter them. Their names were inspired by information retrieval systems, but we\\ncan motivate their meaning with a simple analogy. Imagine that you’re at the super‐\\nmarket buying all the ingredients you need for your dinner. Y ou have the dish’s recipe,\\nand each of the required ingredients can be thought of as a query. As you scan the\\nshelves, you look at the labels (keys) and check whether they match an ingredient on\\nyour list (similarity function). If you have a match, then you take the item (value)\\nfrom the shelf.\\nIn this analogy, you only get one grocery item for every label that matches the ingre‐\\ndient. Self-attention is a more abstract and “smooth” version of this: every label in the\\nsupermarket matches the ingredient to the extent to which each key matches the\\nquery. So if your list includes a dozen eggs, then you might end up grabbing 10 eggs,\\nan omelette, and a chicken wing.\\nLet’s take a look at this process in more detail by implementing the diagram of opera‐\\ntions to compute scaled dot-product attention, as shown in Figure 3-4.\\nFigure 3-4. Operations in scaled dot-product attention\\nWe will use PyTorch to implement the Transformer architecture in this chapter, but\\nthe steps in TensorFlow are analogous. We provide a mapping between the most\\nimportant functions in the two frameworks in Table 3-1.\\nTable 3-1. PyTorch and TensorFlow (Keras) classes and methods used in this chapter\\nPyTorch TensorFlow (Keras) Creates/implements\\nnn.Linear keras.layers.Dense A dense neural network layer\\nnn.Module keras.layers.Layer The building blocks of models\\nnn.Dropout keras.layers.Dropout A dropout layer\\nnn.LayerNorm keras.layers.LayerNormalization Layer normalization\\nnn.Embedding keras.layers.Embedding An embedding layer\\nnn.GELU keras.activations.gelu The Gaussian Error Linear Unit activation function\\nnn.bmm tf.matmul Batched matrix multiplication\\nmodel.forward model.call The model’s forward pass\\n64 | Chapter 3: Transformer Anatomy'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 88, 'page_label': '65'}, page_content='The first thing we need to do is tokenize the text, so let’s use our tokenizer to extract\\nthe input IDs:\\ninputs = tokenizer(text, return_tensors=\"pt\", add_special_tokens=False)\\ninputs.input_ids\\ntensor([[ 2051, 10029,  2066,  2019,  8612]])\\nAs we saw in Chapter 2, each token in the sentence has been mapped to a unique ID\\nin the tokenizer’s vocabulary. To keep things simple, we’ve also excluded the [CLS]\\nand [SEP] tokens by setting add_special_tokens=False. Next, we need to create\\nsome dense embeddings. Dense in this context means that each entry in the embed‐\\ndings contains a nonzero value. In contrast, the one-hot encodings we saw in Chap‐\\nter 2 are sparse, since all entries except one are zero. In PyTorch, we can do this by\\nusing a torch.nn.Embedding layer that acts as a lookup table for each input ID:\\nfrom torch import nn\\nfrom transformers import AutoConfig\\nconfig = AutoConfig.from_pretrained(model_ckpt)\\ntoken_emb = nn.Embedding(config.vocab_size, config.hidden_size)\\ntoken_emb\\nEmbedding(30522, 768)\\nHere we’ve used the AutoConfig class to load the config.json file associated with the\\nbert-base-uncased checkpoint. In \\n  Transformers, every checkpoint is assigned a\\nconfiguration file that specifies various hyperparameters like vocab_size and\\nhidden_size, which in our example shows us that each input ID will be mapped to\\none of the 30,522 embedding vectors stored in nn.Embedding, each with a size of 768.\\nThe AutoConfig class also stores additional metadata, such as the label names, which\\nare used to format the model’s predictions.\\nNote that the token embeddings at this point are independent of their context. This\\nmeans that homonyms (words that have the same spelling but different meaning),\\nlike “flies” in the previous example, have the same representation. The role of the sub‐\\nsequent attention layers will be to mix these token embeddings to disambiguate and\\ninform the representation of each token with the content of its context.\\nNow that we have our lookup table, we can generate the embeddings by feeding in the\\ninput IDs:\\ninputs_embeds = token_emb(inputs.input_ids)\\ninputs_embeds.size()\\ntorch.Size([1, 5, 768])\\nThis has given us a tensor of shape [batch_size, seq_len, hidden_dim], just like\\nwe saw in Chapter 2. We’ll postpone the positional encodings, so the next step is to\\nThe Encoder | 65'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 89, 'page_label': '66'}, page_content='create the query, key, and value vectors and calculate the attention scores using the\\ndot product as the similarity function:\\nimport torch\\nfrom math import sqrt\\nquery = key = value = inputs_embeds\\ndim_k = key.size(-1)\\nscores = torch.bmm(query, key.transpose(1,2)) / sqrt(dim_k)\\nscores.size()\\ntorch.Size([1, 5, 5])\\nThis has created a 5 × 5 matrix of attention scores per sample in the batch. We’ll see\\nlater that the query, key, and value vectors are generated by applying independent\\nweight matrices WQ, K, V to the embeddings, but for now we’ve kept them equal for\\nsimplicity. In scaled dot-product attention, the dot products are scaled by the size of\\nthe embedding vectors so that we don’t get too many large numbers during training\\nthat can cause the softmax we will apply next to saturate.\\nThe torch.bmm() function performs a batch matrix-matrix product\\nthat simplifies the computation of the attention scores where the\\nquery and key vectors have the shape [batch_size, seq_len,\\nhidden_dim]. If we ignored the batch dimension we could calculate\\nthe dot product between each query and key vector by simply\\ntransposing the key tensor to have the shape [hidden_dim,\\nseq_len] and then using the matrix product to collect all the dot\\nproducts in a [seq_len, seq_len] matrix. Since we want to do\\nthis for all sequences in the batch independently, we use\\ntorch.bmm(), which takes two batches of matrices and multiplies\\neach matrix from the first batch with the corresponding matrix in\\nthe second batch.\\nLet’s apply the softmax now:\\nimport torch.nn.functional as F\\nweights = F.softmax(scores, dim=-1)\\nweights.sum(dim=-1)\\ntensor([[1., 1., 1., 1., 1.]], grad_fn=<SumBackward1>)\\nThe final step is to multiply the attention weights by the values:\\nattn_outputs = torch.bmm(weights, value)\\nattn_outputs.shape\\ntorch.Size([1, 5, 768])\\n66 | Chapter 3: Transformer Anatomy'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 90, 'page_label': '67'}, page_content='And that’s it—we’ve gone through all the steps to implement a simplified form of self-\\nattention! Notice that the whole process is just two matrix multiplications and a soft‐\\nmax, so you can think of “self-attention” as just a fancy form of averaging.\\nLet’s wrap these steps into a function that we can use later:\\ndef scaled_dot_product_attention(query, key, value):\\n    dim_k = query.size(-1)\\n    scores = torch.bmm(query, key.transpose(1, 2)) / sqrt(dim_k)\\n    weights = F.softmax(scores, dim=-1)\\n    return torch.bmm(weights, value)\\nOur attention mechanism with equal query and key vectors will assign a very large\\nscore to identical words in the context, and in particular to the current word itself: the\\ndot product of a query with itself is always 1. But in practice, the meaning of a word\\nwill be better informed by complementary words in the context than by identical\\nwords—for example, the meaning of “flies” is better defined by incorporating infor‐\\nmation from “time” and “arrow” than by another mention of “flies” . How can we pro‐\\nmote this behavior?\\nLet’s allow the model to create a different set of vectors for the query, key, and value of\\na token by using three different linear projections to project our initial token vector\\ninto three different spaces.\\nMulti-headed attention\\nIn our simple example, we only used the embeddings “as is” to compute the attention\\nscores and weights, but that’s far from the whole story. In practice, the self-attention\\nlayer applies three independent linear transformations to each embedding to generate\\nthe query, key, and value vectors. These transformations project the embeddings and\\neach projection carries its own set of learnable parameters, which allows the self-\\nattention layer to focus on different semantic aspects of the sequence.\\nIt also turns out to be beneficial to have multiple sets of linear projections, each one\\nrepresenting a so-called attention head. The resulting multi-head attention layer  is\\nillustrated in Figure 3-5. But why do we need more than one attention head? The rea‐\\nson is that the softmax of one head tends to focus on mostly one aspect of similarity.\\nHaving several heads allows the model to focus on several aspects at once. For\\ninstance, one head can focus on subject-verb interaction, whereas another finds\\nnearby adjectives. Obviously we don’t handcraft these relations into the model, and\\nthey are fully learned from the data. If you are familiar with computer vision models\\nyou might see the resemblance to filters in convolutional neural networks, where one\\nfilter can be responsible for detecting faces and another one finds wheels of cars in\\nimages.\\nThe Encoder | 67'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 91, 'page_label': '68'}, page_content='Figure 3-5. Multi-head attention\\nLet’s implement this layer by first coding up a single attention head:\\nclass AttentionHead(nn.Module):\\n    def __init__(self, embed_dim, head_dim):\\n        super().__init__()\\n        self.q = nn.Linear(embed_dim, head_dim)\\n        self.k = nn.Linear(embed_dim, head_dim)\\n        self.v = nn.Linear(embed_dim, head_dim)\\n    def forward(self, hidden_state):\\n        attn_outputs = scaled_dot_product_attention(\\n            self.q(hidden_state), self.k(hidden_state), self.v(hidden_state))\\n        return attn_outputs\\nHere we’ve initialized three independent linear layers that apply matrix multiplication\\nto the embedding vectors to produce tensors of shape [batch_size, seq_len,\\nhead_dim], where head_dim is the number of dimensions we are projecting into.\\nAlthough head_dim does not have to be smaller than the number of embedding\\ndimensions of the tokens ( embed_dim), in practice it is chosen to be a multiple of\\nembed_dim so that the computation across each head is constant. For example, BERT\\nhas 12 attention heads, so the dimension of each head is 768/12 = 64 .\\nNow that we have a single attention head, we can concatenate the outputs of each one\\nto implement the full multi-head attention layer:\\nclass MultiHeadAttention(nn.Module):\\n    def __init__(self, config):\\n        super().__init__()\\n        embed_dim = config.hidden_size\\n        num_heads = config.num_attention_heads\\n        head_dim = embed_dim // num_heads\\n        self.heads = nn.ModuleList(\\n            [AttentionHead(embed_dim, head_dim) for _ in range(num_heads)]\\n        )\\n        self.output_linear = nn.Linear(embed_dim, embed_dim)\\n68 | Chapter 3: Transformer Anatomy'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 92, 'page_label': '69'}, page_content='def forward(self, hidden_state):\\n        x = torch.cat([h(hidden_state) for h in self.heads], dim=-1)\\n        x = self.output_linear(x)\\n        return x\\nNotice that the concatenated output from the attention heads is also fed through a\\nfinal linear layer to produce an output tensor of shape [batch_size, seq_len,\\nhidden_dim] that is suitable for the feed-forward network downstream. To confirm,\\nlet’s see if the multi-head attention layer produces the expected shape of our inputs.\\nWe pass the configuration we loaded earlier from the pretrained BERT model when\\ninitializing the MultiHeadAttention module. This ensures that we use the same set‐\\ntings as BERT:\\nmultihead_attn = MultiHeadAttention(config)\\nattn_output = multihead_attn(inputs_embeds)\\nattn_output.size()\\ntorch.Size([1, 5, 768])\\nIt works! To wrap up this section on attention, let’s use BertViz again to visualize the\\nattention for two different uses of the word “flies” . Here we can use the head_view()\\nfunction from BertViz by computing the attentions of a pretrained checkpoint and\\nindicating where the sentence boundary lies:\\nfrom bertviz import head_view\\nfrom transformers import AutoModel\\nmodel = AutoModel.from_pretrained(model_ckpt, output_attentions=True)\\nsentence_a = \"time flies like an arrow\"\\nsentence_b = \"fruit flies like a banana\"\\nviz_inputs = tokenizer(sentence_a, sentence_b, return_tensors=\\'pt\\')\\nattention = model(**viz_inputs).attentions\\nsentence_b_start = (viz_inputs.token_type_ids == 0).sum(dim=1)\\ntokens = tokenizer.convert_ids_to_tokens(viz_inputs.input_ids[0])\\nhead_view(attention, tokens, sentence_b_start, heads=[8])\\nThe Encoder | 69'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 93, 'page_label': '70'}, page_content='This visualization shows the attention weights as lines connecting the token whose\\nembedding is getting updated (left) with every word that is being attended to (right).\\nThe intensity of the lines indicates the strength of the attention weights, with dark\\nlines representing values close to 1, and faint lines representing values close to 0.\\nIn this example, the input consists of two sentences and the [CLS] and [SEP] tokens\\nare the special tokens in BERT’s tokenizer that we encountered in Chapter 2 . One\\nthing we can see from the visualization is that the attention weights are strongest\\nbetween words that belong to the same sentence, which suggests BERT can tell that it\\nshould attend to words in the same sentence. However, for the word “flies” we can see\\nthat BERT has identified “arrow” as important in the first sentence and “fruit” and\\n“banana” in the second. These attention weights allow the model to distinguish the\\nuse of “flies” as a verb or noun, depending on the context in which it occurs!\\nNow that we’ve covered attention, let’s take a look at implementing the missing piece\\nof the encoder layer: position-wise feed-forward networks.\\nThe Feed-Forward Layer\\nThe feed-forward sublayer in the encoder and decoder is just a simple two-layer fully\\nconnected neural network, but with a twist: instead of processing the whole sequence\\nof embeddings as a single vector, it processes each embedding independently. For this\\nreason, this layer is often referred to as a position-wise feed-forward layer. Y ou may\\nalso see it referred to as a one-dimensional convolution with a kernel size of one, typ‐\\nically by people with a computer vision background (e.g., the OpenAI GPT codebase\\nuses this nomenclature). A rule of thumb from the literature is for the hidden size of\\nthe first layer to be four times the size of the embeddings, and a GELU activation\\nfunction is most commonly used. This is where most of the capacity and memoriza‐\\ntion is hypothesized to happen, and it’s the part that is most often scaled when scaling\\nup the models. We can implement this as a simple nn.Module as follows:\\nclass FeedForward(nn.Module):\\n    def __init__(self, config):\\n        super().__init__()\\n        self.linear_1 = nn.Linear(config.hidden_size, config.intermediate_size)\\n        self.linear_2 = nn.Linear(config.intermediate_size, config.hidden_size)\\n        self.gelu = nn.GELU()\\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\\n    def forward(self, x):\\n        x = self.linear_1(x)\\n        x = self.gelu(x)\\n        x = self.linear_2(x)\\n        x = self.dropout(x)\\n        return x\\n70 | Chapter 3: Transformer Anatomy'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 94, 'page_label': '71'}, page_content='Note that a feed-forward layer such as nn.Linear is usually applied to a tensor of\\nshape (batch_size, input_dim), where it acts on each element of the batch dimen‐\\nsion independently. This is actually true for any dimension except the last one, so\\nwhen we pass a tensor of shape (batch_size, seq_len, hidden_dim) the layer is\\napplied to all token embeddings of the batch and sequence independently, which is\\nexactly what we want. Let’s test this by passing the attention outputs:\\nfeed_forward = FeedForward(config)\\nff_outputs = feed_forward(attn_outputs)\\nff_outputs.size()\\ntorch.Size([1, 5, 768])\\nWe now have all the ingredients to create a fully fledged transformer encoder layer!\\nThe only decision left to make is where to place the skip connections and layer nor‐\\nmalization. Let’s take a look at how this affects the model architecture.\\nAdding Layer Normalization\\nAs mentioned earlier, the Transformer architecture makes use of layer normalization\\nand skip connections. The former normalizes each input in the batch to have zero\\nmean and unity variance. Skip connections pass a tensor to the next layer of the\\nmodel without processing and add it to the processed tensor. When it comes to plac‐\\ning the layer normalization in the encoder or decoder layers of a transformer, there\\nare two main choices adopted in the literature:\\nPost layer normalization\\nThis is the arrangement used in the Transformer paper; it places layer normaliza‐\\ntion in between the skip connections. This arrangement is tricky to train from\\nscratch as the gradients can diverge. For this reason, you will often see a concept\\nknown as learning rate warm-up, where the learning rate is gradually increased\\nfrom a small value to some maximum value during training.\\nPre layer normalization\\nThis is the most common arrangement found in the literature; it places layer nor‐\\nmalization within the span of the skip connections. This tends to be much more\\nstable during training, and it does not usually require any learning rate warm-up.\\nThe difference between the two arrangements is illustrated in Figure 3-6.\\nThe Encoder | 71'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 95, 'page_label': '72'}, page_content='Figure 3-6. Different arrangements of layer normalization in a transformer encoder\\nlayer\\nWe’ll use the second arrangement, so we can simply stick together our building\\nblocks as follows:\\nclass TransformerEncoderLayer(nn.Module):\\n    def __init__(self, config):\\n        super().__init__()\\n        self.layer_norm_1 = nn.LayerNorm(config.hidden_size)\\n        self.layer_norm_2 = nn.LayerNorm(config.hidden_size)\\n        self.attention = MultiHeadAttention(config)\\n        self.feed_forward = FeedForward(config)\\n    def forward(self, x):\\n        # Apply layer normalization and then copy input into query, key, value\\n        hidden_state = self.layer_norm_1(x)\\n        # Apply attention with a skip connection\\n        x = x + self.attention(hidden_state)\\n        # Apply feed-forward layer with a skip connection\\n        x = x + self.feed_forward(self.layer_norm_2(x))\\n        return x\\nLet’s now test this with our input embeddings:\\nencoder_layer = TransformerEncoderLayer(config)\\ninputs_embeds.shape, encoder_layer(inputs_embeds).size()\\n(torch.Size([1, 5, 768]), torch.Size([1, 5, 768]))\\nWe’ve now implemented our very first transformer encoder layer from scratch! How‐\\never, there is a caveat with the way we set up the encoder layers: they are totally\\n72 | Chapter 3: Transformer Anatomy'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 96, 'page_label': '73'}, page_content='4 In fancier terminology, the self-attention and feed-forward layers are said to be permutation equivariant—if\\nthe input is permuted then the corresponding output of the layer is permuted in exactly the same way.\\ninvariant to the position of the tokens. Since the multi-head attention layer is effec‐\\ntively a fancy weighted sum, the information on token position is lost.4\\nLuckily, there is an easy trick to incorporate positional information using positional\\nembeddings. Let’s take a look.\\nPositional Embeddings\\nPositional embeddings are based on a simple, yet very effective idea: augment the\\ntoken embeddings with a position-dependent pattern of values arranged in a vector.\\nIf the pattern is characteristic for each position, the attention heads and feed-forward\\nlayers in each stack can learn to incorporate positional information into their trans‐\\nformations.\\nThere are several ways to achieve this, and one of the most popular approaches is to\\nuse a learnable pattern, especially when the pretraining dataset is sufficiently large.\\nThis works exactly the same way as the token embeddings, but using the position\\nindex instead of the token ID as input. With that approach, an efficient way of encod‐\\ning the positions of tokens is learned during pretraining.\\nLet’s create a custom Embeddings module that combines a token embedding layer that\\nprojects the input_ids to a dense hidden state together with the positional embed‐\\nding that does the same for position_ids. The resulting embedding is simply the\\nsum of both embeddings:\\nclass Embeddings(nn.Module):\\n    def __init__(self, config):\\n        super().__init__()\\n        self.token_embeddings = nn.Embedding(config.vocab_size,\\n                                             config.hidden_size)\\n        self.position_embeddings = nn.Embedding(config.max_position_embeddings,\\n                                                config.hidden_size)\\n        self.layer_norm = nn.LayerNorm(config.hidden_size, eps=1e-12)\\n        self.dropout = nn.Dropout()\\n    def forward(self, input_ids):\\n        # Create position IDs for input sequence\\n        seq_length = input_ids.size(1)\\n        position_ids = torch.arange(seq_length, dtype=torch.long).unsqueeze(0)\\n        # Create token and position embeddings\\n        token_embeddings = self.token_embeddings(input_ids)\\n        position_embeddings = self.position_embeddings(position_ids)\\n        # Combine token and position embeddings\\n        embeddings = token_embeddings + position_embeddings\\n        embeddings = self.layer_norm(embeddings)\\nThe Encoder | 73'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 97, 'page_label': '74'}, page_content='5 By combining the idea of absolute and relative positional representations, rotary position embeddings achieve\\nexcellent results on many tasks. GPT-Neo is an example of a model with rotary position embeddings.\\n        embeddings = self.dropout(embeddings)\\n        return embeddings\\nembedding_layer = Embeddings(config)\\nembedding_layer(inputs.input_ids).size()\\ntorch.Size([1, 5, 768])\\nWe see that the embedding layer now creates a single, dense embedding for each\\ntoken.\\nWhile learnable position embeddings are easy to implement and widely used, there\\nare some alternatives:\\nAbsolute positional representations\\nTransformer models can use static patterns consisting of modulated sine and\\ncosine signals to encode the positions of the tokens. This works especially well\\nwhen there are not large volumes of data available.\\nRelative positional representations\\nAlthough absolute positions are important, one can argue that when computing\\nan embedding, the surrounding tokens are most important. Relative positional\\nrepresentations follow that intuition and encode the relative positions between\\ntokens. This cannot be set up by just introducing a new relative embedding layer\\nat the beginning, since the relative embedding changes for each token depending\\non where from the sequence we are attending to it. Instead, the attention mecha‐\\nnism itself is modified with additional terms that take the relative position\\nbetween tokens into account. Models such as DeBERTa use such representations.5\\nLet’s put all of this together now by building the full transformer encoder combining\\nthe embeddings with the encoder layers:\\nclass TransformerEncoder(nn.Module):\\n    def __init__(self, config):\\n        super().__init__()\\n        self.embeddings = Embeddings(config)\\n        self.layers = nn.ModuleList([TransformerEncoderLayer(config)\\n                                     for _ in range(config.num_hidden_layers)])\\n    def forward(self, x):\\n        x = self.embeddings(x)\\n        for layer in self.layers:\\n            x = layer(x)\\n        return x\\nLet’s check the output shapes of the encoder:\\n74 | Chapter 3: Transformer Anatomy'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 98, 'page_label': '75'}, page_content='encoder = TransformerEncoder(config)\\nencoder(inputs.input_ids).size()\\ntorch.Size([1, 5, 768])\\nWe can see that we get a hidden state for each token in the batch. This output format\\nmakes the architecture very flexible, and we can easily adapt it for various applica‐\\ntions such as predicting missing tokens in masked language modeling or predicting\\nthe start and end position of an answer in question answering. In the following sec‐\\ntion we’ll see how we can build a classifier like the one we used in Chapter 2.\\nAdding a Classification Head\\nTransformer models are usually divided into a task-independent body and a task-\\nspecific head. We’ll encounter this pattern again in Chapter 4 when we look at the\\ndesign pattern of \\n  Transformers. What we have built so far is the body, so if we wish\\nto build a text classifier, we will need to attach a classification head to that body. We\\nhave a hidden state for each token, but we only need to make one prediction. There\\nare several options to approach this. Traditionally, the first token in such models is\\nused for the prediction and we can attach a dropout and a linear layer to make a clas‐\\nsification prediction. The following class extends the existing encoder for sequence\\nclassification:\\nclass TransformerForSequenceClassification(nn.Module):\\n    def __init__(self, config):\\n        super().__init__()\\n        self.encoder = TransformerEncoder(config)\\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\\n        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\\n    def forward(self, x):\\n        x = self.encoder(x)[:, 0, :] # select hidden state of [CLS] token\\n        x = self.dropout(x)\\n        x = self.classifier(x)\\n        return x\\nBefore initializing the model we need to define how many classes we would like to\\npredict:\\nconfig.num_labels = 3\\nencoder_classifier = TransformerForSequenceClassification(config)\\nencoder_classifier(inputs.input_ids).size()\\ntorch.Size([1, 3])\\nThat is exactly what we have been looking for. For each example in the batch we get\\nthe unnormalized logits for each class in the output. This corresponds to the BERT\\nmodel that we used in Chapter 2 to detect emotions in tweets.\\nThe Encoder | 75'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 99, 'page_label': '76'}, page_content='6 Note that unlike the self-attention layer, the key and query vectors in encoder-decoder attention can have dif‐\\nferent lengths. This is because the encoder and decoder inputs will generally involve sequences of differing\\nlength. As a result, the matrix of attention scores in this layer is rectangular, not square.\\nThis concludes our analysis of the encoder and how we can combine it with a task-\\nspecific head. Let’s now cast our attention (pun intended!) to the decoder.\\nThe Decoder\\nAs illustrated in Figure 3-7, the main difference between the decoder and encoder is\\nthat the decoder has two attention sublayers:\\nMasked multi-head self-attention layer\\nEnsures that the tokens we generate at each timestep are only based on the past\\noutputs and the current token being predicted. Without this, the decoder could\\ncheat during training by simply copying the target translations; masking the\\ninputs ensures the task is not trivial.\\nEncoder-decoder attention layer\\nPerforms multi-head attention over the output key and value vectors of the\\nencoder stack, with the intermediate representations of the decoder acting as the\\nqueries.6 This way the encoder-decoder attention layer learns how to relate\\ntokens from two different sequences, such as two different languages. The\\ndecoder has access to the encoder keys and values in each block.\\nLet’s take a look at the modifications we need to make to include masking in our self-\\nattention layer, and leave the implementation of the encoder-decoder attention layer\\nas a homework problem. The trick with masked self-attention is to introduce a mask\\nmatrix with ones on the lower diagonal and zeros above:\\nseq_len = inputs.input_ids.size(-1)\\nmask = torch.tril(torch.ones(seq_len, seq_len)).unsqueeze(0)\\nmask[0]\\ntensor([[1., 0., 0., 0., 0.],\\n        [1., 1., 0., 0., 0.],\\n        [1., 1., 1., 0., 0.],\\n        [1., 1., 1., 1., 0.],\\n        [1., 1., 1., 1., 1.]])\\nHere we’ve used PyTorch’s tril() function to create the lower triangular matrix.\\nOnce we have this mask matrix, we can prevent each attention head from peeking at\\nfuture tokens by using Tensor.masked_fill() to replace all the zeros with negative\\ninfinity:\\nscores.masked_fill(mask == 0, -float(\"inf\"))\\n76 | Chapter 3: Transformer Anatomy'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 100, 'page_label': '77'}, page_content='tensor([[[26.8082,    -inf,    -inf,    -inf,    -inf],\\n         [-0.6981, 26.9043,    -inf,    -inf,    -inf],\\n         [-2.3190,  1.2928, 27.8710,    -inf,    -inf],\\n         [-0.5897,  0.3497, -0.3807, 27.5488,    -inf],\\n         [ 0.5275,  2.0493, -0.4869,  1.6100, 29.0893]]],\\n       grad_fn=<MaskedFillBackward0>)\\nFigure 3-7. Zooming into the transformer decoder layer\\nBy setting the upper values to negative infinity, we guarantee that the attention\\nweights are all zero once we take the softmax over the scores because e−∞= 0 (recall\\nthat softmax calculates the normalized exponential). We can easily include this mask‐\\ning behavior with a small change to our scaled dot-product attention function that we\\nimplemented earlier in this chapter:\\ndef scaled_dot_product_attention(query, key, value, mask=None):\\n    dim_k = query.size(-1)\\n    scores = torch.bmm(query, key.transpose(1, 2)) / sqrt(dim_k)\\n    if mask is not None:\\n        scores = scores.masked_fill(mask == 0, float(\"-inf\"))\\n    weights = F.softmax(scores, dim=-1)\\n    return weights.bmm(value)\\nFrom here it is a simple matter to build up the decoder layer; we point the reader to\\nthe excellent implementation of minGPT by Andrej Karpathy for details.\\nThe Decoder | 77'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 101, 'page_label': '78'}, page_content='We’ve given you a lot of technical information here, but now you should have a good\\nunderstanding of how every piece of the Transformer architecture works. Before we\\nmove on to building models for tasks more advanced than text classification, let’s\\nround out the chapter by stepping back a bit and looking at the landscape of different\\ntransformer models and how they relate to each other.\\nDemystifying Encoder-Decoder Attention\\nLet’s see if we can shed some light on the mysteries of encoder-decoder attention.\\nImagine you (the decoder) are in class taking an exam. Y our task is to predict the next\\nword based on the previous words (decoder inputs), which sounds simple but is\\nincredibly hard (try it yourself and predict the next words in a passage of this book).\\nFortunately, your neighbor (the encoder) has the full text. Unfortunately, they’re a\\nforeign exchange student and the text is in their mother tongue. Cunning students\\nthat you are, you figure out a way to cheat anyway. Y ou draw a little cartoon illustrat‐\\ning the text you already have (the query) and give it to your neighbor. They try to\\nfigure out which passage matches that description (the key), draw a cartoon describ‐\\ning the word following that passage (the value), and pass that back to you. With this\\nsystem in place, you ace the exam.\\nMeet the Transformers\\nAs you’ve seen in this chapter, there are three main architectures for transformer\\nmodels: encoders, decoders, and encoder-decoders. The initial success of the early\\ntransformer models triggered a Cambrian explosion in model development as\\nresearchers built models on various datasets of different size and nature, used new\\npretraining objectives, and tweaked the architecture to further improve performance.\\nAlthough the zoo of models is still growing fast, they can still be divided into these\\nthree categories.\\nIn this section we’ll provide a brief overview of the most important transformer mod‐\\nels in each class. Let’s start by taking a look at the transformer family tree.\\nThe Transformer Tree of Life\\nOver time, each of the three main architectures has undergone an evolution of its\\nown. This is illustrated in Figure 3-8, which shows a few of the most prominent mod‐\\nels and their descendants.\\n78 | Chapter 3: Transformer Anatomy'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 102, 'page_label': '79'}, page_content='7 A. Wang et al., “GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understand‐\\ning”, (2018).\\nFigure 3-8. An overview of some of the most prominent transformer architectures\\nWith over 50 different architectures included in \\n  Transformers, this family tree by\\nno means provides a complete overview of all the ones that exist: it simply highlights\\na few of the architectural milestones. We’ve covered the original Transformer archi‐\\ntecture in depth in this chapter, so let’s take a closer look at some of the key descend‐\\nants, starting with the encoder branch.\\nThe Encoder Branch\\nThe first encoder-only model based on the Transformer architecture was BERT. At\\nthe time it was published, it outperformed all the state-of-the-art models on the pop‐\\nular GLUE benchmark, 7 which measures natural language understanding (NLU)\\nacross several tasks of varying difficulty. Subsequently, the pretraining objective and\\nthe architecture of BERT have been adapted to further improve performance.\\nEncoder-only models still dominate research and industry on NLU tasks such as text\\nMeet the Transformers | 79'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 103, 'page_label': '80'}, page_content='8 J. Devlin et al., “BERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding”,\\n(2018).\\n9 V . Sanh et al., “DistilBERT, a Distilled Version of BERT: Smaller, Faster, Cheaper and Lighter”, (2019).\\n10 Y . Liu et al., “RoBERTa: A Robustly Optimized BERT Pretraining Approach”, (2019).\\n11 G. Lample, and A. Conneau, “Cross-Lingual Language Model Pretraining”, (2019).\\n12 A. Conneau et al., “Unsupervised Cross-Lingual Representation Learning at Scale”, (2019).\\nclassification, named entity recognition, and question answering. Let’s have a brief\\nlook at the BERT model and its variants:\\nBERT\\nBERT is pretrained with the two objectives of predicting masked tokens in texts\\nand determining if one text passage is likely to follow another.8 The former task is\\ncalled masked language modeling (MLM) and the latter next sentence prediction\\n(NSP).\\nDistilBERT\\nAlthough BERT delivers great results, it’s size can make it tricky to deploy in\\nenvironments where low latencies are required. By using a technique known as\\nknowledge distillation during pretraining, DistilBERT achieves 97% of BERT’s\\nperformance while using 40% less memory and being 60% faster. 9 Y ou can find\\nmore details on knowledge distillation in Chapter 8.\\nRoBERTa\\nA study following the release of BERT revealed that its performance can be fur‐\\nther improved by modifying the pretraining scheme. RoBERTa is trained longer,\\non larger batches with more training data, and it drops the NSP task. 10 Together,\\nthese changes significantly improve its performance compared to the original\\nBERT model.\\nXLM\\nSeveral pretraining objectives for building multilingual models were explored in\\nthe work on the cross-lingual language model (XLM), 11 including the autoregres‐\\nsive language modeling from GPT-like models and MLM from BERT. In addi‐\\ntion, the authors of the paper on XLM pretraining introduced translation\\nlanguage modeling (TLM), which is an extension of MLM to multiple language\\ninputs. Experimenting with these pretraining tasks, they achieved state-of-the-art\\nresults on several multilingual NLU benchmarks as well as on translation tasks.\\nXLM-RoBERTa\\nFollowing the work of XLM and RoBERTa, the XLM-RoBERTa or XLM-R model\\ntakes multilingual pretraining one step further by massively upscaling the\\ntraining data.12 Using the Common Crawl corpus, its developers created a dataset\\nwith 2.5 terabytes of text; they then trained an encoder with MLM on this\\n80 | Chapter 3: Transformer Anatomy'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 104, 'page_label': '81'}, page_content='13 Z. Lan et al., “ ALBERT: A Lite BERT for Self-Supervised Learning of Language Representations”, (2019).\\n14 K. Clark et al., “ELECTRA: Pre-Training Text Encoders as Discriminators Rather Than Generators”, (2020).\\n15 P . He et al., “DeBERTa: Decoding-Enhanced BERT with Disentangled Attention”, (2020).\\ndataset. Since the dataset only contains data without parallel texts (i.e., transla‐\\ntions), the TLM objective of XLM was dropped. This approach beats XLM and\\nmultilingual BERT variants by a large margin, especially on low-resource\\nlanguages.\\nALBERT\\nThe ALBERT model introduced three changes to make the encoder architecture\\nmore efficient.13 First, it decouples the token embedding dimension from the hid‐\\nden dimension, thus allowing the embedding dimension to be small and thereby\\nsaving parameters, especially when the vocabulary gets large. Second, all layers\\nshare the same parameters, which decreases the number of effective parameters\\neven further. Finally, the NSP objective is replaced with a sentence-ordering pre‐\\ndiction: the model needs to predict whether or not the order of two consecutive\\nsentences was swapped rather than predicting if they belong together at all. These\\nchanges make it possible to train even larger models with fewer parameters and\\nreach superior performance on NLU tasks.\\nELECTRA\\nOne limitation of the standard MLM pretraining objective is that at each training\\nstep only the representations of the masked tokens are updated, while the other\\ninput tokens are not. To address this issue, ELECTRA uses a two-model\\napproach:14 the first model (which is typically small) works like a standard\\nmasked language model and predicts masked tokens. The second model, called\\nthe discriminator, is then tasked to predict which of the tokens in the first model’s\\noutput were originally masked. Therefore, the discriminator needs to make a\\nbinary classification for every token, which makes training 30 times more effi‐\\ncient. For downstream tasks the discriminator is fine-tuned like a standard BERT\\nmodel.\\nDeBERTa\\nThe DeBERTa model introduces two architectural changes. 15 First, each token is\\nrepresented as two vectors: one for the content, the other for relative position. By\\ndisentangling the tokens’ content from their relative positions, the self-attention\\nlayers can better model the dependency of nearby token pairs. On the other\\nhand, the absolute position of a word is also important, especially for decoding.\\nFor this reason, an absolute position embedding is added just before the softmax\\nlayer of the token decoding head. DeBERTa is the first model (as an ensemble) to\\nMeet the Transformers | 81'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 105, 'page_label': '82'}, page_content='16 A. Wang et al., “SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems”,\\n(2019).\\n17 A. Radford et al., “Improving Language Understanding by Generative Pre-Training”, OpenAI (2018).\\n18 A. Radford et al., “Language Models Are Unsupervised Multitask Learners”, OpenAI (2019).\\n19 N.S. Keskar et al., “CTRL: A Conditional Transformer Language Model for Controllable Generation”, (2019).\\nbeat the human baseline on the SuperGLUE benchmark, 16 a more difficult ver‐\\nsion of GLUE consisting of several subtasks used to measure NLU performance.\\nNow that we’ve highlighted some of the major encoder-only architectures, let’s take a\\nlook at the decoder-only models.\\nThe Decoder Branch\\nThe progress on transformer decoder models has been spearheaded to a large extent\\nby OpenAI. These models are exceptionally good at predicting the next word in a\\nsequence and are thus mostly used for text generation tasks (see Chapter 5 for more\\ndetails). Their progress has been fueled by using larger datasets and scaling the lan‐\\nguage models to larger and larger sizes. Let’s have a look at the evolution of these fas‐\\ncinating generation models:\\nGPT\\nThe introduction of GPT combined two key ideas in NLP: 17 the novel and effi‐\\ncient transformer decoder architecture, and transfer learning. In that setup, the\\nmodel was pretrained by predicting the next word based on the previous ones.\\nThe model was trained on the BookCorpus and achieved great results on down‐\\nstream tasks such as classification.\\nGPT-2\\nInspired by the success of the simple and scalable pretraining approach, the origi‐\\nnal model and training set were upscaled to produce GPT-2. 18 This model is able\\nto produce long sequences of coherent text. Due to concerns about possible mis‐\\nuse, the model was released in a staged fashion, with smaller models being pub‐\\nlished first and the full model later.\\nCTRL\\nModels like GPT-2 can continue an input sequence (also called a prompt). How‐\\never, the user has little control over the style of the generated sequence. The\\nConditional Transformer Language (CTRL) model addresses this issue by adding\\n“control tokens” at the beginning of the sequence. 19 These allow the style of the\\ngenerated text to be controlled, which allows for diverse generation.\\n82 | Chapter 3: Transformer Anatomy'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 106, 'page_label': '83'}, page_content='20 J. Kaplan et al., “Scaling Laws for Neural Language Models”, (2020).\\n21 T. Brown et al., “Language Models Are Few-Shot Learners”, (2020).\\n22 S. Black et al., “GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-TensorFlow”, (2021); B.\\nWang and A. Komatsuzaki, “GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model”, (2021).\\n23 C. Raffel et al., “Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer”, (2019).\\nGPT-3\\nFollowing the success of scaling GPT up to GPT-2, a thorough analysis on the\\nbehavior of language models at different scales revealed that there are simple\\npower laws that govern the relation between compute, dataset size, model size,\\nand the performance of a language model. 20 Inspired by these insights, GPT-2\\nwas upscaled by a factor of 100 to yield GPT-3, 21 with 175 billion parameters.\\nBesides being able to generate impressively realistic text passages, the model also\\nexhibits few-shot learning capabilities: with a few examples of a novel task such\\nas translating text to code, the model is able to accomplish the task on new exam‐\\nples. OpenAI has not open-sourced this model, but provides an interface through\\nthe OpenAI API.\\nGPT-Neo/GPT-J-6B\\nGPT-Neo and GPT-J-6B are GPT-like models that were trained by EleutherAI, a\\ncollective of researchers who aim to re-create and release GPT-3 scale models. 22\\nThe current models are smaller variants of the full 175-billion-parameter model,\\nwith 1.3, 2.7, and 6 billion parameters, and are competitive with the smaller\\nGPT-3 models OpenAI offers.\\nThe final branch in the transformers tree of life is the encoder-decoder models. Let’s\\ntake a look.\\nThe Encoder-Decoder Branch\\nAlthough it has become common to build models using a single encoder or decoder\\nstack, there are several encoder-decoder variants of the Transformer architecture that\\nhave novel applications across both NLU and NLG domains:\\nT5\\nThe T5 model unifies all NLU and NLG tasks by converting them into text-to-\\ntext tasks.23 All tasks are framed as sequence-to-sequence tasks, where adopting\\nan encoder-decoder architecture is natural. For text classification problems, for\\nexample, this means that the text is used as the encoder input and the decoder\\nhas to generate the label as normal text instead of a class. We will look at this in\\nmore detail in Chapter 6 . The T5 architecture uses the original Transformer\\narchitecture. Using the large crawled C4 dataset, the model is pretrained with\\nmasked language modeling as well as the SuperGLUE tasks by translating all of\\nMeet the Transformers | 83'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 107, 'page_label': '84'}, page_content='24 M. Lewis et al., “BART: Denoising Sequence-to-Sequence Pre-Training for Natural Language Generation,\\nTranslation, and Comprehension”, (2019).\\n25 A. Fan et al., “Beyond English-Centric Multilingual Machine Translation”, (2020).\\n26 M. Zaheer et al., “Big Bird: Transformers for Longer Sequences”, (2020).\\nthem to text-to-text tasks. The largest model with 11 billion parameters yielded\\nstate-of-the-art results on several benchmarks.\\nBART\\nBART combines the pretraining procedures of BERT and GPT within the\\nencoder-decoder architecture.24 The input sequences undergo one of several pos‐\\nsible transformations, from simple masking to sentence permutation, token dele‐\\ntion, and document rotation. These modified inputs are passed through the\\nencoder, and the decoder has to reconstruct the original texts. This makes the\\nmodel more flexible as it is possible to use it for NLU as well as NLG tasks, and it\\nachieves state-of-the-art-performance on both.\\nM2M-100\\nConventionally a translation model is built for one language pair and translation\\ndirection. Naturally, this does not scale to many languages, and in addition there\\nmight be shared knowledge between language pairs that could be leveraged for\\ntranslation between rare languages. M2M-100 is the first translation model that\\ncan translate between any of 100 languages.25 This allows for high-quality transla‐\\ntions between rare and underrepresented languages. The model uses prefix\\ntokens (similar to the special [CLS] token) to indicate the source and target\\nlanguage.\\nBigBird\\nOne main limitation of transformer models is the maximum context size, due to\\nthe quadratic memory requirements of the attention mechanism. BigBird\\naddresses this issue by using a sparse form of attention that scales linearly. 26 This\\nallows for the drastic scaling of contexts from 512 tokens in most BERT models\\nto 4,096 in BigBird. This is especially useful in cases where long dependencies\\nneed to be conserved, such as in text summarization.\\nPretrained checkpoints of all models that we have seen in this section are available on\\nthe Hugging Face Hub and can be fine-tuned to your use case with \\n  Transformers,\\nas described in the previous chapter.\\nConclusion\\nIn this chapter we started at the heart of the Transformer architecture with a deep\\ndive into self-attention, and we subsequently added all the necessary parts to build a\\n84 | Chapter 3: Transformer Anatomy'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 108, 'page_label': '85'}, page_content='transformer encoder model. We added embedding layers for tokens and positional\\ninformation, we built in a feed-forward layer to complement the attention heads, and\\nfinally we added a classification head to the model body to make predictions. We also\\nhad a look at the decoder side of the Transformer architecture, and concluded the\\nchapter with an overview of the most important model architectures.\\nNow that you have a better understanding of the underlying principles, let’s go\\nbeyond simple classification and build a multilingual named entity recognition\\nmodel.\\nConclusion | 85'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 109, 'page_label': '86'}, page_content=''), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 110, 'page_label': '87'}, page_content='1 A. Conneau et al., “Unsupervised Cross-Lingual Representation Learning at Scale”, (2019).\\nCHAPTER 4\\nMultilingual Named Entity Recognition\\nSo far in this book we have applied transformers to solve NLP tasks on English cor‐\\npora—but what do you do when your documents are written in Greek, Swahili, or\\nKlingon? One approach is to search the Hugging Face Hub for a suitable pretrained\\nlanguage model and fine-tune it on the task at hand. However, these pretrained mod‐\\nels tend to exist only for “high-resource” languages like German, Russian, or Man‐\\ndarin, where plenty of webtext is available for pretraining. Another common\\nchallenge arises when your corpus is multilingual: maintaining multiple monolingual\\nmodels in production will not be any fun for you or your engineering team.\\nFortunately, there is a class of multilingual transformers that come to the rescue. Like\\nBERT, these models use masked language modeling as a pretraining objective, but\\nthey are trained jointly on texts in over one hundred languages. By pretraining on\\nhuge corpora across many languages, these multilingual transformers enable zero-\\nshot cross-lingual transfer. This means that a model that is fine-tuned on one language\\ncan be applied to others without any further training! This also makes these models\\nwell suited for “code-switching, ” where a speaker alternates between two or more lan‐\\nguages or dialects in the context of a single conversation.\\nIn this chapter we will explore how a single transformer model called XLM-RoBERTa\\n(introduced in Chapter 3 )1 can be fine-tuned to perform named entity recognition\\n(NER) across several languages. As we saw in Chapter 1, NER is a common NLP task\\nthat identifies entities like people, organizations, or locations in text. These entities\\ncan be used for various applications such as gaining insights from company docu‐\\nments, augmenting the quality of search engines, or simply building a structured\\ndatabase from a corpus.\\n87'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 111, 'page_label': '88'}, page_content='2 J. Hu et al., “XTREME: A Massively Multilingual Multi-Task Benchmark for Evaluating Cross-Lingual Gener‐\\nalization”, (2020); X. Pan et al., “Cross-Lingual Name Tagging and Linking for 282 Languages, ” Proceedings of\\nthe 55th Annual Meeting of the Association for Computational Linguistics 1 (July 2017): 1946–1958, http://\\ndx.doi.org/10.18653/v1/P17-1178.\\nFor this chapter let’s assume that we want to perform NER for a customer based in\\nSwitzerland, where there are four national languages (with English often serving as a\\nbridge between them). Let’s start by getting a suitable multilingual corpus for this\\nproblem.\\nZero-shot transfer or zero-shot learning usually refers to the task of\\ntraining a model on one set of labels and then evaluating it on a\\ndifferent set of labels. In the context of transformers, zero-shot\\nlearning may also refer to situations where a language model like\\nGPT-3 is evaluated on a downstream task that it wasn’t even fine-\\ntuned on.\\nThe Dataset\\nIn this chapter we will be using a subset of the Cross-lingual TRansfer Evaluation of\\nMultilingual Encoders (XTREME) benchmark called WikiANN or PAN-X. 2 This\\ndataset consists of Wikipedia articles in many languages, including the four most\\ncommonly spoken languages in Switzerland: German (62.9%), French (22.9%), Ital‐\\nian (8.4%), and English (5.9%). Each article is annotated with LOC (location), PER\\n(person), and ORG (organization) tags in the “inside-outside-beginning” (IOB2) for‐\\nmat. In this format, a B- prefix indicates the beginning of an entity, and consecutive\\ntokens belonging to the same entity are given an I- prefix. An O tag indicates that the\\ntoken does not belong to any entity. For example, the following sentence:\\nJeff Dean is a computer scientist at Google in California\\nwould be labeled in IOB2 format as shown in Table 4-1.\\nTable 4-1. An example of a sequence annotated with named entities\\nTokens Jeff Dean is a computer scientist at Google in California\\nTags B-PER I-PER O O O O O B-ORG O B-LOC\\nTo load one of the PAN-X subsets in XTREME, we’ll need to know which dataset\\nconfiguration to pass the load_dataset() function. Whenever you’re dealing with a\\ndataset that has multiple domains, you can use the get_dataset_config_names()\\nfunction to find out which subsets are available:\\n88 | Chapter 4: Multilingual Named Entity Recognition'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 112, 'page_label': '89'}, page_content='from datasets import get_dataset_config_names\\nxtreme_subsets = get_dataset_config_names(\"xtreme\")\\nprint(f\"XTREME has {len(xtreme_subsets)} configurations\")\\nXTREME has 183 configurations\\nWhoa, that’s a lot of configurations! Let’s narrow the search by just looking for the\\nconfigurations that start with “PAN”:\\npanx_subsets = [s for s in xtreme_subsets if s.startswith(\"PAN\")]\\npanx_subsets[:3]\\n[\\'PAN-X.af\\', \\'PAN-X.ar\\', \\'PAN-X.bg\\']\\nOK, it seems we’ve identified the syntax of the PAN-X subsets: each one has a two-\\nletter suffix that appears to be an ISO 639-1 language code . This means that to load\\nthe German corpus, we pass the de code to the name argument of load_dataset() as\\nfollows:\\nfrom datasets import load_dataset\\nload_dataset(\"xtreme\", name=\"PAN-X.de\")\\nTo make a realistic Swiss corpus, we’ll sample the German ( de), French ( fr), Italian\\n(it), and English ( en) corpora from PAN-X according to their spoken proportions.\\nThis will create a language imbalance that is very common in real-world datasets,\\nwhere acquiring labeled examples in a minority language can be expensive due to the\\nlack of domain experts who are fluent in that language. This imbalanced dataset will\\nsimulate a common situation when working on multilingual applications, and we’ll\\nsee how we can build a model that works on all languages.\\nTo keep track of each language, let’s create a Python defaultdict that stores the lan‐\\nguage code as the key and a PAN-X corpus of type DatasetDict as the value:\\nfrom collections import defaultdict\\nfrom datasets import DatasetDict\\nlangs = [\"de\", \"fr\", \"it\", \"en\"]\\nfracs = [0.629, 0.229, 0.084, 0.059]\\n# Return a DatasetDict if a key doesn\\'t exist\\npanx_ch = defaultdict(DatasetDict)\\nfor lang, frac in zip(langs, fracs):\\n    # Load monolingual corpus\\n    ds = load_dataset(\"xtreme\", name=f\"PAN-X.{lang}\")\\n    # Shuffle and downsample each split according to spoken proportion\\n    for split in ds:\\n        panx_ch[lang][split] = (\\n            ds[split]\\n            .shuffle(seed=0)\\n            .select(range(int(frac * ds[split].num_rows))))\\nThe Dataset | 89'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 113, 'page_label': '90'}, page_content='Here we’ve used the shuffle() method to make sure we don’t accidentally bias our\\ndataset splits, while select() allows us to downsample each corpus according to the\\nvalues in fracs. Let’s have a look at how many examples we have per language in the\\ntraining sets by accessing the Dataset.num_rows attribute:\\nimport pandas as pd\\npd.DataFrame({lang: [panx_ch[lang][\"train\"].num_rows] for lang in langs},\\n             index=[\"Number of training examples\"])\\nde fr it en\\nNumber of training examples 12580 4580 1680 1180\\nBy design, we have more examples in German than all other languages combined, so\\nwe’ll use it as a starting point from which to perform zero-shot cross-lingual transfer\\nto French, Italian, and English. Let’s inspect one of the examples in the German\\ncorpus:\\nelement = panx_ch[\"de\"][\"train\"][0]\\nfor key, value in element.items():\\n    print(f\"{key}: {value}\")\\nlangs: [\\'de\\', \\'de\\', \\'de\\', \\'de\\', \\'de\\', \\'de\\', \\'de\\', \\'de\\', \\'de\\', \\'de\\', \\'de\\', \\'de\\']\\nner_tags: [0, 0, 0, 0, 5, 6, 0, 0, 5, 5, 6, 0]\\ntokens: [\\'2.000\\', \\'Einwohnern\\', \\'an\\', \\'der\\', \\'Danziger\\', \\'Bucht\\', \\'in\\', \\'der\\',\\n\\'polnischen\\', \\'Woiwodschaft\\', \\'Pommern\\', \\'.\\']\\nAs with our previous encounters with Dataset objects, the keys of our example corre‐\\nspond to the column names of an Arrow table, while the values denote the entries in\\neach column. In particular, we see that the ner_tags column corresponds to the map‐\\nping of each entity to a class ID. This is a bit cryptic to the human eye, so let’s create a\\nnew column with the familiar LOC, PER, and ORG tags. To do this, the first thing to\\nnotice is that our Dataset object has a features attribute that specifies the underly‐\\ning data types associated with each column:\\nfor key, value in panx_ch[\"de\"][\"train\"].features.items():\\n    print(f\"{key}: {value}\")\\ntokens: Sequence(feature=Value(dtype=\\'string\\', id=None), length=-1, id=None)\\nner_tags: Sequence(feature=ClassLabel(num_classes=7, names=[\\'O\\', \\'B-PER\\',\\n\\'I-PER\\', \\'B-ORG\\', \\'I-ORG\\', \\'B-LOC\\', \\'I-LOC\\'], names_file=None, id=None),\\nlength=-1, id=None)\\nlangs: Sequence(feature=Value(dtype=\\'string\\', id=None), length=-1, id=None)\\nThe Sequence class specifies that the field contains a list of features, which in the case\\nof ner_tags corresponds to a list of ClassLabel features. Let’s pick out this feature\\nfrom the training set as follows:\\n90 | Chapter 4: Multilingual Named Entity Recognition'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 114, 'page_label': '91'}, page_content='tags = panx_ch[\"de\"][\"train\"].features[\"ner_tags\"].feature\\nprint(tags)\\nClassLabel(num_classes=7, names=[\\'O\\', \\'B-PER\\', \\'I-PER\\', \\'B-ORG\\', \\'I-ORG\\',\\n\\'B-LOC\\', \\'I-LOC\\'], names_file=None, id=None)\\nWe can use the ClassLabel.int2str() method that we encountered in Chapter 2 to\\ncreate a new column in our training set with class names for each tag. We’ll use the\\nmap() method to return a dict with the key corresponding to the new column name\\nand the value as a list of class names:\\ndef create_tag_names(batch):\\n    return {\"ner_tags_str\": [tags.int2str(idx) for idx in batch[\"ner_tags\"]]}\\npanx_de = panx_ch[\"de\"].map(create_tag_names)\\nNow that we have our tags in human-readable format, let’s see how the tokens and\\ntags align for the first example in the training set:\\nde_example = panx_de[\"train\"][0]\\npd.DataFrame([de_example[\"tokens\"], de_example[\"ner_tags_str\"]],\\n[\\'Tokens\\', \\'Tags\\'])\\n0 1 2 3 4 5 6 7 8 9 10 11\\nTokens 2.000 Einwohnern an der Danziger Bucht in der polnischen Woiwodschaft Pommern .\\nTags O O O O B-LOC I-LOC O O B-LOC B-LOC I-LOC O\\nThe presence of the LOC tags make sense since the sentence “2,000 Einwohnern an der\\nDanziger Bucht in der polnischen Woiwodschaft Pommern” means “2,000 inhabi‐\\ntants at the Gdansk Bay in the Polish voivodeship of Pomerania” in English, and\\nGdansk Bay is a bay in the Baltic sea, while “voivodeship” corresponds to a state in\\nPoland.\\nAs a quick check that we don’t have any unusual imbalance in the tags, let’s calculate\\nthe frequencies of each entity across each split:\\nfrom collections import Counter\\nsplit2freqs = defaultdict(Counter)\\nfor split, dataset in panx_de.items():\\n    for row in dataset[\"ner_tags_str\"]:\\n        for tag in row:\\n            if tag.startswith(\"B\"):\\n                tag_type = tag.split(\"-\")[1]\\n                split2freqs[split][tag_type] += 1\\npd.DataFrame.from_dict(split2freqs, orient=\"index\")\\nORG LOC PER\\nvalidation 2683 3172 2893\\nThe Dataset | 91'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 115, 'page_label': '92'}, page_content='ORG LOC PER\\ntest 2573 3180 3071\\ntrain 5366 6186 5810\\nThis looks good—the distributions of the PER, LOC, and ORG frequencies are roughly\\nthe same for each split, so the validation and test sets should provide a good measure\\nof our NER tagger’s ability to generalize. Next, let’s look at a few popular multilingual\\ntransformers and how they can be adapted to tackle our NER task.\\nMultilingual Transformers\\nMultilingual transformers involve similar architectures and training procedures as\\ntheir monolingual counterparts, except that the corpus used for pretraining consists\\nof documents in many languages. A remarkable feature of this approach is that\\ndespite receiving no explicit information to differentiate among the languages, the\\nresulting linguistic representations are able to generalize well across languages for a\\nvariety of downstream tasks. In some cases, this ability to perform cross-lingual\\ntransfer can produce results that are competitive with those of monolingual models,\\nwhich circumvents the need to train one model per language!\\nTo measure the progress of cross-lingual transfer for NER, the CoNLL-2002 and\\nCoNLL-2003 datasets are often used as a benchmark for English, Dutch, Spanish, and\\nGerman. This benchmark consists of news articles annotated with the same LOC, PER,\\nand ORG categories as PAN-X, but it contains an additional MISC label for miscellane‐\\nous entities that do not belong to the previous three groups. Multilingual transformer\\nmodels are usually evaluated in three different ways:\\nen\\nFine-tune on the English training data and then evaluate on each language’s test\\nset.\\neach\\nFine-tune and evaluate on monolingual test data to measure per-language\\nperformance.\\nall\\nFine-tune on all the training data to evaluate on all on each language’s test set.\\nWe will adopt a similar evaluation strategy for our NER task, but first we need to\\nselect a model to evaluate. One of the first multilingual transformers was mBERT,\\nwhich uses the same architecture and pretraining objective as BERT but adds Wikipe‐\\ndia articles from many languages to the pretraining corpus. Since then, mBERT has\\nbeen superseded by XLM-RoBERTa (or XLM-R for short), so that’s the model we’ll\\nconsider in this chapter.\\n92 | Chapter 4: Multilingual Named Entity Recognition'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 116, 'page_label': '93'}, page_content='3 Y . Liu et al., “RoBERTa: A Robustly Optimized BERT Pretraining Approach”, (2019).\\n4 T. Kudo and J. Richardson, “SentencePiece: A Simple and Language Independent Subword Tokenizer and\\nDetokenizer for Neural Text Processing”, (2018).\\nAs we saw in Chapter 3, XLM-R uses only MLM as a pretraining objective for 100\\nlanguages, but is distinguished by the huge size of its pretraining corpus compared to\\nits predecessors: Wikipedia dumps for each language and 2.5 terabytes of Common\\nCrawl data from the web. This corpus is several orders of magnitude larger than the\\nones used in earlier models and provides a significant boost in signal for low-resource\\nlanguages like Burmese and Swahili, where only a small number of Wikipedia articles\\nexist.\\nThe RoBERTa part of the model’s name refers to the fact that the pretraining\\napproach is the same as for the monolingual RoBERTa models. RoBERTa’s developers\\nimproved on several aspects of BERT, in particular by removing the next sentence\\nprediction task altogether.3 XLM-R also drops the language embeddings used in XLM\\nand uses SentencePiece to tokenize the raw texts directly. 4 Besides its multilingual\\nnature, a notable difference between XLM-R and RoBERTa is the size of the respec‐\\ntive vocabularies: 250,000 tokens versus 55,000!\\nXLM-R is a great choice for multilingual NLU tasks. In the next section, we’ll explore\\nhow it can efficiently tokenize across many languages.\\nA Closer Look at Tokenization\\nInstead of using a WordPiece tokenizer, XLM-R uses a tokenizer called SentencePiece\\nthat is trained on the raw text of all one hundred languages. To get a feel for how Sen‐\\ntencePiece compares to WordPiece, let’s load the BERT and XLM-R tokenizers in the\\nusual way with \\n  Transformers:\\nfrom transformers import AutoTokenizer\\nbert_model_name = \"bert-base-cased\"\\nxlmr_model_name = \"xlm-roberta-base\"\\nbert_tokenizer = AutoTokenizer.from_pretrained(bert_model_name)\\nxlmr_tokenizer = AutoTokenizer.from_pretrained(xlmr_model_name)\\nBy encoding a small sequence of text we can also retrieve the special tokens that each\\nmodel used during pretraining:\\ntext = \"Jack Sparrow loves New York!\"\\nbert_tokens = bert_tokenizer(text).tokens()\\nxlmr_tokens = xlmr_tokenizer(text).tokens()\\nBERT [CLS] Jack Spa ##rrow loves New York ! [SEP] None\\nA Closer Look at Tokenization | 93'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 117, 'page_label': '94'}, page_content='XLM-R <s> ▁Jack ▁Spar row ▁love s ▁New ▁York ! </s>\\nHere we see that instead of the [CLS] and [SEP] tokens that BERT uses for sentence\\nclassification tasks, XLM-R uses <s> and <\\\\s> to denote the start and end of a\\nsequence. These tokens are added in the final stage of tokenization, as we’ll see next.\\nThe Tokenizer Pipeline\\nSo far we have treated tokenization as a single operation that transforms strings to\\nintegers we can pass through the model. This is not entirely accurate, and if we take a\\ncloser look we can see that it is actually a full processing pipeline that usually consists\\nof four steps, as shown in Figure 4-1.\\nFigure 4-1. The steps in the tokenization pipeline\\nLet’s take a closer look at each processing step and illustrate their effect with the\\nexample sentence “Jack Sparrow loves New Y ork!”:\\nNormalization\\nThis step corresponds to the set of operations you apply to a raw string to make it\\n“cleaner. ” Common operations include stripping whitespace and removing accen‐\\nted characters. Unicode normalization is another common normalization opera‐\\ntion applied by many tokenizers to deal with the fact that there often exist various\\nways to write the same character. This can make two versions of the “same” string\\n(i.e., with the same sequence of abstract characters) appear different; Unicode\\nnormalization schemes like NFC, NFD, NFKC, and NFKD replace the various\\nways to write the same character with standard forms. Another example of nor‐\\nmalization is lowercasing. If the model is expected to only accept and use lower‐\\ncase characters, this technique can be used to reduce the size of the vocabulary it\\nrequires. After normalization, our example string would look like “jack sparrow\\nloves new york!” .\\nPretokenization\\nThis step splits a text into smaller objects that give an upper bound to what your\\ntokens will be at the end of training. A good way to think of this is that the preto‐\\nkenizer will split your text into “words, ” and your final tokens will be parts of\\nthose words. For the languages that allow this (English, German, and many Indo-\\nEuropean languages), strings can typically be split into words on whitespace and\\npunctuation. For example, this step might transform our [\"jack\", \"sparrow\",\\n\"loves\", \"new\", \"york\", \"!\"] . These words are then simpler to split into\\n94 | Chapter 4: Multilingual Named Entity Recognition'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 118, 'page_label': '95'}, page_content='subwords with Byte-Pair Encoding (BPE) or Unigram algorithms in the next step\\nof the pipeline. However, splitting into “words” is not always a trivial and deter‐\\nministic operation, or even an operation that makes sense. For instance, in lan‐\\nguages like Chinese, Japanese, or Korean, grouping symbols in semantic units\\nlike Indo-European words can be a nondeterministic operation with several\\nequally valid groups. In this case, it might be best to not pretokenize the text and\\ninstead use a language-specific library for pretokenization.\\nTokenizer model\\nOnce the input texts are normalized and pretokenized, the tokenizer applies a\\nsubword splitting model on the words. This is the part of the pipeline that needs\\nto be trained on your corpus (or that has been trained if you are using a pre‐\\ntrained tokenizer). The role of the model is to split the words into subwords to\\nreduce the size of the vocabulary and try to reduce the number of out-of-\\nvocabulary tokens. Several subword tokenization algorithms exist, including\\nBPE, Unigram, and WordPiece. For instance, our running example might look\\nlike [jack, spa, rrow, loves, new, york, !]  after the tokenizer model is\\napplied. Note that at this point we no longer have a list of strings but a list of inte‐\\ngers (input IDs); to keep the example illustrative, we’ve kept the words but drop‐\\nped the quotes to indicate the transformation.\\nPostprocessing\\nThis is the last step of the tokenization pipeline, in which some additional trans‐\\nformations can be applied on the list of tokens—for instance, adding special\\ntokens at the beginning or end of the input sequence of token indices. For exam‐\\nple, a BERT-style tokenizer would add classifications and separator tokens: [CLS,\\njack, spa, rrow, loves, new, york, !, SEP]. This sequence (recall that this\\nwill be a sequence of integers, not the tokens you see here) can then be fed to the\\nmodel.\\nGoing back to our comparison of XLM-R and BERT, we now understand that Senten‐\\ncePiece adds <s> and <\\\\s> instead of [CLS] and [SEP] in the postprocessing step (as a\\nconvention, we’ll continue to use [CLS] and [SEP] in the graphical illustrations). Let’s\\ngo back to the SentencePiece tokenizer to see what makes it special.\\nThe SentencePiece Tokenizer\\nThe SentencePiece tokenizer is based on a type of subword segmentation called\\nUnigram and encodes each input text as a sequence of Unicode characters. This last\\nfeature is especially useful for multilingual corpora since it allows SentencePiece to be\\nagnostic about accents, punctuation, and the fact that many languages, like Japanese,\\ndo not have whitespace characters. Another special feature of SentencePiece is that\\nwhitespace is assigned the Unicode symbol U+2581, or the ▁ character, also called\\nthe lower one quarter block character. This enables SentencePiece to detokenize a\\nA Closer Look at Tokenization | 95'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 119, 'page_label': '96'}, page_content='sequence without ambiguities and without relying on language-specific pretokeniz‐\\ners. In our example from the previous section, for instance, we can see that Word‐\\nPiece has lost the information that there is no whitespace between “Y ork” and “!” . By\\ncontrast, SentencePiece preserves the whitespace in the tokenized text so we can con‐\\nvert back to the raw text without ambiguity:\\n\"\".join(xlmr_tokens).replace(u\"\\\\u2581\", \" \")\\n\\'<s> Jack Sparrow loves New York!</s>\\'\\nNow that we understand how SentencePiece works, let’s see how we can encode our\\nsimple example in a form suitable for NER. The first thing to do is load the pretrained\\nmodel with a token classification head. But instead of loading this head directly from\\n Transformers, we will build it ourselves! By diving deeper into the \\n  Transformers\\nAPI, we can do this with just a few steps.\\nTransformers for Named Entity Recognition\\nIn Chapter 2, we saw that for text classification BERT uses the special [CLS] token to\\nrepresent an entire sequence of text. This representation is then fed through a fully\\nconnected or dense layer to output the distribution of all the discrete label values, as\\nshown in Figure 4-2.\\nFigure 4-2. Fine-tuning an encoder-based transformer for sequence classification\\n96 | Chapter 4: Multilingual Named Entity Recognition'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 120, 'page_label': '97'}, page_content='5 J. Devlin et al., “BERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding”,\\n(2018).\\nBERT and other encoder-only transformers take a similar approach for NER, except\\nthat the representation of each individual input token is fed into the same fully\\nconnected layer to output the entity of the token. For this reason, NER is often\\nframed as a token classification task. The process looks something like the diagram in\\nFigure 4-3.\\nFigure 4-3. Fine-tuning an encoder-based transformer for named entity recognition\\nSo far, so good, but how should we handle subwords in a token classification task?\\nFor example, the first name “Christa” in Figure 4-3  is tokenized into the subwords\\n“Chr” and “##ista” , so which one(s) should be assigned the B-PER label?\\nIn the BERT paper,5 the authors assigned this label to the first subword (“Chr” in our\\nexample) and ignored the following subword (“##ista”). This is the convention we’ll\\nadopt here, and we’ll indicate the ignored subwords with IGN. We can later easily\\npropagate the predicted label of the first subword to the subsequent subwords in the\\npostprocessing step. We could also have chosen to include the representation of the\\n“##ista” subword by assigning it a copy of the B-LOC label, but this violates the IOB2\\nformat.\\nFortunately, all the architecture aspects we’ve seen in BERT carry over to XLM-R\\nsince its architecture is based on RoBERTa, which is identical to BERT! Next we’ll see\\nhow \\n  Transformers supports many other tasks with minor modifications.\\nTransformers for Named Entity Recognition | 97'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 121, 'page_label': '98'}, page_content='The Anatomy of the Transformers Model Class\\n Transformers is organized around dedicated classes for each architecture and task.\\nThe model classes associated with different tasks are named according to a <Model\\nName>For<Task> convention, or AutoModelFor<Task> when using the AutoModel\\nclasses.\\nHowever, this approach has its limitations, and to motivate going deeper into the \\n Transformers API, consider the following scenario. Suppose you have a great idea\\nto solve an NLP problem that has been on your mind for a long time with a trans‐\\nformer model. So you set up a meeting with your boss and, with an artfully crafted\\nPowerPoint presentation, you pitch that you could increase the revenue of your\\ndepartment if you can finally solve the problem. Impressed with your colorful presen‐\\ntation and talk of profits, your boss generously agrees to give you one week to build a\\nproof-of-concept. Happy with the outcome, you start working straight away. Y ou fire\\nup your GPU and open a notebook. Y ou execute from transformers import Bert\\nForTaskXY (note that TaskXY is the imaginary task you would like to solve) and color\\nescapes your face as the dreaded red color fills your screen: ImportError: cannot\\nimport name BertForTaskXY. Oh no, there is no BERT model for your use case! How\\ncan you complete the project in one week if you have to implement the whole model\\nyourself?! Where should you even start?\\nDon’t panic! \\n  Transformers is designed to enable you to easily extend existing mod‐\\nels for your specific use case. Y ou can load the weights from pretrained models, and\\nyou have access to task-specific helper functions. This lets you build custom models\\nfor specific objectives with very little overhead. In this section, we’ll see how we can\\nimplement our own custom model.\\nBodies and Heads\\nThe main concept that makes \\n  Transformers so versatile is the split of the architec‐\\nture into a body and head (as we saw in Chapter 1). We have already seen that when\\nwe switch from the pretraining task to the downstream task, we need to replace the\\nlast layer of the model with one that is suitable for the task. This last layer is called the\\nmodel head; it’s the part that is task-specific. The rest of the model is called the body;\\nit includes the token embeddings and transformer layers that are task-agnostic. This\\nstructure is reflected in the \\n  Transformers code as well: the body of a model is\\nimplemented in a class such as BertModel or GPT2Model that returns the hidden states\\nof the last layer. Task-specific models such as BertForMaskedLM or BertForSequence\\nClassification use the base model and add the necessary head on top of the hidden\\nstates, as shown in Figure 4-4.\\n98 | Chapter 4: Multilingual Named Entity Recognition'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 122, 'page_label': '99'}, page_content='Figure 4-4. The BertModel class only contains the body of the model, while the Bert\\nFor<Task> classes combine the body with a dedicated head for a given task\\nAs we’ll see next, this separation of bodies and heads allows us to build a custom head\\nfor any task and just mount it on top of a pretrained model.\\nCreating a Custom Model for Token Classification\\nLet’s go through the exercise of building a custom token classification head for XLM-\\nR. Since XLM-R uses the same model architecture as RoBERTa, we will use RoBERTa\\nas the base model, but augmented with settings specific to XLM-R. Note that this is\\nan educational exercise to show you how to build a custom model for your own task.\\nFor token classification, an XLMRobertaForTokenClassification class already exists\\nthat you can import from \\n  Transformers. If you want, you can skip to the next sec‐\\ntion and simply use that one.\\nTo get started, we need a data structure that will represent our XLM-R NER tagger. As\\na first guess, we’ll need a configuration object to initialize the model and a forward()\\nfunction to generate the outputs. Let’s go ahead and build our XLM-R class for token\\nclassification:\\nimport torch.nn as nn\\nfrom transformers import XLMRobertaConfig\\nfrom transformers.modeling_outputs import TokenClassifierOutput\\nfrom transformers.models.roberta.modeling_roberta import RobertaModel\\nfrom transformers.models.roberta.modeling_roberta import RobertaPreTrainedModel\\nclass XLMRobertaForTokenClassification(RobertaPreTrainedModel):\\n    config_class = XLMRobertaConfig\\n    def __init__(self, config):\\n        super().__init__(config)\\n        self.num_labels = config.num_labels\\n        # Load model body\\n        self.roberta = RobertaModel(config, add_pooling_layer=False)\\n        # Set up token classification head\\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\\n        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\\n        # Load and initialize weights\\n        self.init_weights()\\n    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None,\\nThe Anatomy of the Transformers Model Class | 99'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 123, 'page_label': '100'}, page_content='labels=None, **kwargs):\\n        # Use model body to get encoder representations\\n        outputs = self.roberta(input_ids, attention_mask=attention_mask,\\n                               token_type_ids=token_type_ids, **kwargs)\\n        # Apply classifier to encoder representation\\n        sequence_output = self.dropout(outputs[0])\\n        logits = self.classifier(sequence_output)\\n        # Calculate losses\\n        loss = None\\n        if labels is not None:\\n            loss_fct = nn.CrossEntropyLoss()\\n            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\\n        # Return model output object\\n        return TokenClassifierOutput(loss=loss, logits=logits,\\n                                     hidden_states=outputs.hidden_states,\\n                                     attentions=outputs.attentions)\\nThe config_class ensures that the standard XLM-R settings are used when we initi‐\\nalize a new model. If you want to change the default parameters, you can do this by\\noverwriting the default settings in the configuration. With the super() method we\\ncall the initialization function of the RobertaPreTrainedModel class. This abstract\\nclass handles the initialization or loading of pretrained weights. Then we load our\\nmodel body, which is RobertaModel, and extend it with our own classification head\\nconsisting of a dropout and a standard feed-forward layer. Note that we set add_\\npooling_layer=False to ensure all hidden states are returned and not only the one\\nassociated with the [CLS] token. Finally, we initialize all the weights by calling the\\ninit_weights() method we inherit from RobertaPreTrainedModel, which will load\\nthe pretrained weights for the model body and randomly initialize the weights of our\\ntoken classification head.\\nThe only thing left to do is to define what the model should do in a forward pass with\\na forward() method. During the forward pass, the data is first fed through the model\\nbody. There are a number of input variables, but the only ones we need for now are\\ninput_ids and attention_mask. The hidden state, which is part of the model body\\noutput, is then fed through the dropout and classification layers. If we also provide\\nlabels in the forward pass, we can directly calculate the loss. If there is an attention\\nmask we need to do a little bit more work to make sure we only calculate the loss of\\nthe unmasked tokens. Finally, we wrap all the outputs in a TokenClassifierOutput\\nobject that allows us to access elements in a the familiar named tuple from previous\\nchapters.\\nBy just implementing two functions of a simple class, we can build our own custom\\ntransformer model. And since we inherit from a PreTrainedModel, we instantly get\\naccess to all the useful \\n  Transformer utilities, such as from_pretrained()! Let’s\\nhave a look how we can load pretrained weights into our custom model.\\n100 | Chapter 4: Multilingual Named Entity Recognition'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 124, 'page_label': '101'}, page_content='Loading a Custom Model\\nNow we are ready to load our token classification model. We’ll need to provide some\\nadditional information beyond the model name, including the tags that we will use to\\nlabel each entity and the mapping of each tag to an ID and vice versa. All of this\\ninformation can be derived from our tags variable, which as a ClassLabel object has\\na names attribute that we can use to derive the mapping:\\nindex2tag = {idx: tag for idx, tag in enumerate(tags.names)}\\ntag2index = {tag: idx for idx, tag in enumerate(tags.names)}\\nWe’ll store these mappings and the tags.num_classes attribute in the AutoConfig\\nobject that we encountered in Chapter 3. Passing keyword arguments to the from_pre\\ntrained() method overrides the default values:\\nfrom transformers import AutoConfig\\nxlmr_config = AutoConfig.from_pretrained(xlmr_model_name,\\n                                         num_labels=tags.num_classes,\\n                                         id2label=index2tag, label2id=tag2index)\\nThe AutoConfig class contains the blueprint of a model’s architecture. When we load\\na model with AutoModel.from_pretrained(model_ckpt), the configuration file asso‐\\nciated with that model is downloaded automatically. However, if we want to modify\\nsomething like the number of classes or label names, then we can load the configura‐\\ntion first with the parameters we would like to customize.\\nNow, we can load the model weights as usual with the from_pretrained() function\\nwith the additional config argument. Note that we did not implement loading pre‐\\ntrained weights in our custom model class; we get this for free by inheriting from\\nRobertaPreTrainedModel:\\nimport torch\\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\\nxlmr_model = (XLMRobertaForTokenClassification\\n              .from_pretrained(xlmr_model_name, config=xlmr_config)\\n              .to(device))\\nAs a quick check that we have initialized the tokenizer and model correctly, let’s test\\nthe predictions on our small sequence of known entities:\\ninput_ids = xlmr_tokenizer.encode(text, return_tensors=\"pt\")\\npd.DataFrame([xlmr_tokens, input_ids[0].numpy()], index=[\"Tokens\", \"Input IDs\"])\\n0 1 2 3 4 5 6 7 8 9\\nTokens <s> ▁Jack ▁Spar row ▁love s ▁New ▁York ! </s>\\nInput IDs 0 21763 37456 15555 5161 7 2356 5753 38 2\\nThe Anatomy of the Transformers Model Class | 101'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 125, 'page_label': '102'}, page_content='As you can see here, the start <s> and end </s> tokens are given the IDs 0 and 2,\\nrespectively.\\nFinally, we need to pass the inputs to the model and extract the predictions by taking\\nthe argmax to get the most likely class per token:\\noutputs = xlmr_model(input_ids.to(device)).logits\\npredictions = torch.argmax(outputs, dim=-1)\\nprint(f\"Number of tokens in sequence: {len(xlmr_tokens)}\")\\nprint(f\"Shape of outputs: {outputs.shape}\")\\nNumber of tokens in sequence: 10\\nShape of outputs: torch.Size([1, 10, 7])\\nHere we see that the logits have the shape [batch_size, num_tokens, num_tags],\\nwith each token given a logit among the seven possible NER tags. By enumerating\\nover the sequence, we can quickly see what the pretrained model predicts:\\npreds = [tags.names[p] for p in predictions[0].cpu().numpy()]\\npd.DataFrame([xlmr_tokens, preds], index=[\"Tokens\", \"Tags\"])\\n0 1 2 3 4 5 6 7 8 9\\nTokens <s> ▁Jack ▁Spar row ▁love s ▁New ▁York ! </s>\\nTags O I-LOC B-LOC B-LOC O I-LOC O O I-LOC B-LOC\\nUnsurprisingly, our token classification layer with random weights leaves a lot to be\\ndesired; let’s fine-tune on some labeled data to make it better! Before doing so, let’s\\nwrap the preceding steps into a helper function for later use:\\ndef tag_text(text, tags, model, tokenizer):\\n    # Get tokens with special characters\\n    tokens = tokenizer(text).tokens()\\n    # Encode the sequence into IDs\\n    input_ids = xlmr_tokenizer(text, return_tensors=\"pt\").input_ids.to(device)\\n    # Get predictions as distribution over 7 possible classes\\n    outputs = model(input_ids)[0]\\n    # Take argmax to get most likely class per token\\n    predictions = torch.argmax(outputs, dim=2)\\n    # Convert to DataFrame\\n    preds = [tags.names[p] for p in predictions[0].cpu().numpy()]\\n    return pd.DataFrame([tokens, preds], index=[\"Tokens\", \"Tags\"])\\nBefore we can train the model, we also need to tokenize the inputs and prepare the\\nlabels. We’ll do that next.\\n102 | Chapter 4: Multilingual Named Entity Recognition'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 126, 'page_label': '103'}, page_content='Tokenizing Texts for NER\\nNow that we’ve established that the tokenizer and model can encode a single example,\\nour next step is to tokenize the whole dataset so that we can pass it to the XLM-R\\nmodel for fine-tuning. As we saw in Chapter 2 , \\n  Datasets provides a fast way to\\ntokenize a Dataset object with the map() operation. To achieve this, recall that we\\nfirst need to define a function with the minimal signature:\\nfunction(examples: Dict[str, List]) -> Dict[str, List]\\nwhere examples is equivalent to a slice of a Dataset, e.g., panx_de[\\'train\\'][:10].\\nSince the XLM-R tokenizer returns the input IDs for the model’s inputs, we just need\\nto augment this information with the attention mask and the label IDs that encode\\nthe information about which token is associated with each NER tag.\\nFollowing the approach taken in the \\n  Transformers documentation, let’s look at\\nhow this works with our single German example by first collecting the words and tags\\nas ordinary lists:\\nwords, labels = de_example[\"tokens\"], de_example[\"ner_tags\"]\\nNext, we tokenize each word and use the is_split_into_words argument to tell the\\ntokenizer that our input sequence has already been split into words:\\ntokenized_input = xlmr_tokenizer(de_example[\"tokens\"], is_split_into_words=True)\\ntokens = xlmr_tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\\npd.DataFrame([tokens], index=[\"Tokens\"])\\n0 1 2 3 4 5 6 ... 18 19 20 21 22 23 24\\nTokens <s> ▁2.000 ▁Einwohner n ▁an ▁der ▁Dan ... schaft ▁Po mmer n ▁ . </s>\\nIn this example we can see that the tokenizer has split “Einwohnern” into two sub‐\\nwords, “ ▁Einwohner” and “n” . Since we’re following the convention that only\\n“▁Einwohner” should be associated with the B-LOC label, we need a way to mask the\\nsubword representations after the first subword. Fortunately, tokenized_input is a\\nclass that contains a word_ids() function that can help us achieve this:\\nword_ids = tokenized_input.word_ids()\\npd.DataFrame([tokens, word_ids], index=[\"Tokens\", \"Word IDs\"])\\n0 1 2 3 4 5 6 ... 18 19 20 21 22 23 24\\nTokens <s> ▁2.000 ▁Einwohner n ▁an ▁der ▁Dan ... schaft ▁Po mmer n ▁ . </s>\\nWord\\nIDs\\nNone 0 1 1 2 3 4 ... 9 10 10 10 11 11 None\\nTokenizing Texts for NER | 103'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 127, 'page_label': '104'}, page_content='Here we can see that word_ids has mapped each subword to the corresponding index\\nin the words sequence, so the first subword, “ ▁2.000” , is assigned the index 0, while\\n“▁Einwohner” and “n” are assigned the index 1 (since “Einwohnern” is the second\\nword in words). We can also see that special tokens like <s> and <\\\\s> are mapped to\\nNone. Let’s set –100 as the label for these special tokens and the subwords we wish to\\nmask during training:\\nprevious_word_idx = None\\nlabel_ids = []\\nfor word_idx in word_ids:\\n    if word_idx is None or word_idx == previous_word_idx:\\n        label_ids.append(-100)\\n    elif word_idx != previous_word_idx:\\n        label_ids.append(labels[word_idx])\\n    previous_word_idx = word_idx\\nlabels = [index2tag[l] if l != -100 else \"IGN\" for l in label_ids]\\nindex = [\"Tokens\", \"Word IDs\", \"Label IDs\", \"Labels\"]\\npd.DataFrame([tokens, word_ids, label_ids, labels], index=index)\\n0 1 2 3 4 5 ... 19 20 21 22 23 24\\nTokens <s> ▁2.000 ▁Einwohner n ▁an ▁der ... ▁Po mmer n ▁ . </s>\\nWord IDs None 0 1 1 2 3 ... 10 10 10 11 11 None\\nLabel IDs -100 0 0 -100 0 0 ... 6 -100 -100 0 -100 -100\\nLabels IGN O O IGN O O ... I-LOC IGN IGN O IGN IGN\\nWhy did we choose –100 as the ID to mask subword representa‐\\ntions? The reason is that in PyTorch the cross-entropy loss class\\ntorch.nn.CrossEntropyLoss has an attribute called ignore_index\\nwhose value is –100. This index is ignored during training, so we\\ncan use it to ignore the tokens associated with consecutive\\nsubwords.\\nAnd that’s it! We can clearly see how the label IDs align with the tokens, so let’s scale\\nthis out to the whole dataset by defining a single function that wraps all the logic:\\ndef tokenize_and_align_labels(examples):\\n    tokenized_inputs = xlmr_tokenizer(examples[\"tokens\"], truncation=True,\\n                                      is_split_into_words=True)\\n    labels = []\\n    for idx, label in enumerate(examples[\"ner_tags\"]):\\n        word_ids = tokenized_inputs.word_ids(batch_index=idx)\\n        previous_word_idx = None\\n        label_ids = []\\n        for word_idx in word_ids:\\n104 | Chapter 4: Multilingual Named Entity Recognition'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 128, 'page_label': '105'}, page_content='if word_idx is None or word_idx == previous_word_idx:\\n                label_ids.append(-100)\\n            else:\\n                label_ids.append(label[word_idx])\\n            previous_word_idx = word_idx\\n        labels.append(label_ids)\\n    tokenized_inputs[\"labels\"] = labels\\n    return tokenized_inputs\\nWe now have all the ingredients we need to encode each split, so let’s write a function\\nwe can iterate over:\\ndef encode_panx_dataset(corpus):\\n    return corpus.map(tokenize_and_align_labels, batched=True,\\n                      remove_columns=[\\'langs\\', \\'ner_tags\\', \\'tokens\\'])\\nBy applying this function to a DatasetDict object, we get an encoded Dataset object\\nper split. Let’s use this to encode our German corpus:\\npanx_de_encoded = encode_panx_dataset(panx_ch[\"de\"])\\nNow that we have a model and a dataset, we need to define a performance metric.\\nPerformance Measures\\nEvaluating a NER model is similar to evaluating a text classification model, and it is\\ncommon to report results for precision, recall, and F1-score. The only subtlety is that\\nall words of an entity need to be predicted correctly in order for a prediction to be\\ncounted as correct. Fortunately, there is a nifty library called seqeval that is designed\\nfor these kinds of tasks. For example, given some placeholder NER tags and model\\npredictions, we can compute the metrics via seqeval’s classification_report()\\nfunction:\\nfrom seqeval.metrics import classification_report\\ny_true = [[\"O\", \"O\", \"O\", \"B-MISC\", \"I-MISC\", \"I-MISC\", \"O\"],\\n          [\"B-PER\", \"I-PER\", \"O\"]]\\ny_pred = [[\"O\", \"O\", \"B-MISC\", \"I-MISC\", \"I-MISC\", \"I-MISC\", \"O\"],\\n          [\"B-PER\", \"I-PER\", \"O\"]]\\nprint(classification_report(y_true, y_pred))\\n              precision    recall  f1-score   support\\n        MISC       0.00      0.00      0.00         1\\n         PER       1.00      1.00      1.00         1\\n   micro avg       0.50      0.50      0.50         2\\n   macro avg       0.50      0.50      0.50         2\\nweighted avg       0.50      0.50      0.50         2\\nAs we can see, seqeval expects the predictions and labels as lists of lists, with each list\\ncorresponding to a single example in our validation or test sets. To integrate these\\nPerformance Measures | 105'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 129, 'page_label': '106'}, page_content='metrics during training, we need a function that can take the outputs of the model\\nand convert them into the lists that seqeval expects. The following does the trick by\\nensuring we ignore the label IDs associated with subsequent subwords:\\nimport numpy as np\\ndef align_predictions(predictions, label_ids):\\n    preds = np.argmax(predictions, axis=2)\\n    batch_size, seq_len = preds.shape\\n    labels_list, preds_list = [], []\\n    for batch_idx in range(batch_size):\\n        example_labels, example_preds = [], []\\n        for seq_idx in range(seq_len):\\n            # Ignore label IDs = -100\\n            if label_ids[batch_idx, seq_idx] != -100:\\n                example_labels.append(index2tag[label_ids[batch_idx][seq_idx]])\\n                example_preds.append(index2tag[preds[batch_idx][seq_idx]])\\n        labels_list.append(example_labels)\\n        preds_list.append(example_preds)\\n    return preds_list, labels_list\\nEquipped with a performance metric, we can move on to actually training the model.\\nFine-Tuning XLM-RoBERTa\\nWe now have all the ingredients to fine-tune our model! Our first strategy will be to\\nfine-tune our base model on the German subset of PAN-X and then evaluate its zero-\\nshot cross-lingual performance on French, Italian, and English. As usual, we’ll use the\\n Transformers Trainer to handle our training loop, so first we need to define the\\ntraining attributes using the TrainingArguments class:\\nfrom transformers import TrainingArguments\\nnum_epochs = 3\\nbatch_size = 24\\nlogging_steps = len(panx_de_encoded[\"train\"]) // batch_size\\nmodel_name = f\"{xlmr_model_name}-finetuned-panx-de\"\\ntraining_args = TrainingArguments(\\n    output_dir=model_name, log_level=\"error\", num_train_epochs=num_epochs,\\n    per_device_train_batch_size=batch_size,\\n    per_device_eval_batch_size=batch_size, evaluation_strategy=\"epoch\",\\n    save_steps=1e6, weight_decay=0.01, disable_tqdm=False,\\n    logging_steps=logging_steps, push_to_hub=True)\\nHere we evaluate the model’s predictions on the validation set at the end of every\\nepoch, tweak the weight decay, and set save_steps to a large number to disable\\ncheckpointing and thus speed up training.\\n106 | Chapter 4: Multilingual Named Entity Recognition'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 130, 'page_label': '107'}, page_content='This is also a good point to make sure we are logged in to the Hugging Face Hub (if\\nyou’re working in a terminal, you can execute the command huggingface-cli login\\ninstead):\\nfrom huggingface_hub import notebook_login\\nnotebook_login()\\nWe also need to tell the Trainer how to compute metrics on the validation set, so\\nhere we can use the align_predictions() function that we defined earlier to extract\\nthe predictions and labels in the format needed by seqeval to calculate the F1-score:\\nfrom seqeval.metrics import f1_score\\ndef compute_metrics(eval_pred):\\n    y_pred, y_true = align_predictions(eval_pred.predictions,\\n                                       eval_pred.label_ids)\\n    return {\"f1\": f1_score(y_true, y_pred)}\\nThe final step is to define a data collator so we can pad each input sequence to the\\nlargest sequence length in a batch. \\n  Transformers provides a dedicated data collator\\nfor token classification that will pad the labels along with the inputs:\\nfrom transformers import DataCollatorForTokenClassification\\ndata_collator = DataCollatorForTokenClassification(xlmr_tokenizer)\\nPadding the labels is necessary because, unlike in a text classification task, the labels\\nare also sequences. One important detail here is that the label sequences are padded\\nwith the value –100, which, as we’ve seen, is ignored by PyTorch loss functions.\\nWe will train several models in the course of this chapter, so we’ll avoid initializing a\\nnew model for every Trainer by creating a model_init() method. This method loads\\nan untrained model and is called at the beginning of the train() call:\\ndef model_init():\\n    return (XLMRobertaForTokenClassification\\n            .from_pretrained(xlmr_model_name, config=xlmr_config)\\n            .to(device))\\nWe can now pass all this information together with the encoded datasets to the\\nTrainer:\\nfrom transformers import Trainer\\ntrainer = Trainer(model_init=model_init, args=training_args,\\n                  data_collator=data_collator, compute_metrics=compute_metrics,\\n                  train_dataset=panx_de_encoded[\"train\"],\\n                  eval_dataset=panx_de_encoded[\"validation\"],\\n                  tokenizer=xlmr_tokenizer)\\nand then run the training loop as follows and push the final model to the Hub:\\nFine-Tuning XLM-RoBERTa | 107'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 131, 'page_label': '108'}, page_content='trainer.train() trainer.push_to_hub(commit_message=\"Training completed!\")\\nEpoch Training Loss Validation Loss F1\\n1 0.2652 0.160244 0.822974\\n2 0.1314 0.137195 0.852747\\n3 0.0806 0.138774 0.864591\\nThese F1 scores are quite good for a NER model. To confirm that our model works as\\nexpected, let’s test it on the German translation of our simple example:\\ntext_de = \"Jeff Dean ist ein Informatiker bei Google in Kalifornien\"\\ntag_text(text_de, tags, trainer.model, xlmr_tokenizer)\\n0 1 2 3 4 5 ... 8 9 10 11 12 13\\nTokens <s> ▁Jeff ▁De an ▁ist ▁ein ... ▁bei ▁Google ▁in ▁Kaliforni en </s>\\nTags O B-PER I-PER I-PER O O ... O B-ORG O B-LOC I-LOC O\\nIt works! But we should never get too confident about performance based on a single\\nexample. Instead, we should conduct a proper and thorough investigation of the\\nmodel’s errors. In the next section we explore how to do this for the NER task.\\nError Analysis\\nBefore we dive deeper into the multilingual aspects of XLM-R, let’s take a minute to\\ninvestigate the errors of our model. As we saw in Chapter 2, a thorough error analysis\\nof your model is one of the most important aspects when training and debugging\\ntransformers (and machine learning models in general). There are several failure\\nmodes where it might look like the model is performing well, while in practice it has\\nsome serious flaws. Examples where training can fail include:\\n• We might accidentally mask too many tokens and also mask some of our labels to\\nget a really promising loss drop.\\n• The compute_metrics() function might have a bug that overestimates the true\\nperformance.\\n• We might include the zero class or O entity in NER as a normal class, which will\\nheavily skew the accuracy and F1-score since it is the majority class by a large\\nmargin.\\nWhen the model performs much worse than expected, looking at the errors can yield\\nuseful insights and reveal bugs that would be hard to spot by just looking at the code.\\nAnd even if the model performs well and there are no bugs in the code, error analysis\\nis still a useful tool to understand the model’s strengths and weaknesses. These are\\n108 | Chapter 4: Multilingual Named Entity Recognition'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 132, 'page_label': '109'}, page_content='aspects we always need to keep in mind when we deploy a model in a production\\nenvironment.\\nFor our analysis we will again use one of the most powerful tools at our disposal,\\nwhich is to look at the validation examples with the highest loss. We can reuse much\\nof the function we built to analyze the sequence classification model in Chapter 2, but\\nwe’ll now calculate a loss per token in the sample sequence.\\nLet’s define a method that we can apply to the validation set:\\nfrom torch.nn.functional import cross_entropy\\ndef forward_pass_with_label(batch):\\n    # Convert dict of lists to list of dicts suitable for data collator\\n    features = [dict(zip(batch, t)) for t in zip(*batch.values())]\\n    # Pad inputs and labels and put all tensors on device\\n    batch = data_collator(features)\\n    input_ids = batch[\"input_ids\"].to(device)\\n    attention_mask = batch[\"attention_mask\"].to(device)\\n    labels = batch[\"labels\"].to(device)\\n    with torch.no_grad():\\n        # Pass data through model\\n        output = trainer.model(input_ids, attention_mask)\\n        # logit.size: [batch_size, sequence_length, classes]\\n        # Predict class with largest logit value on classes axis\\n        predicted_label = torch.argmax(output.logits, axis=-1).cpu().numpy()\\n    # Calculate loss per token after flattening batch dimension with view\\n    loss = cross_entropy(output.logits.view(-1, 7),\\n                         labels.view(-1), reduction=\"none\")\\n    # Unflatten batch dimension and convert to numpy array\\n    loss = loss.view(len(input_ids), -1).cpu().numpy()\\n    return {\"loss\":loss, \"predicted_label\": predicted_label}\\nWe can now apply this function to the whole validation set using map() and load all\\nthe data into a DataFrame for further analysis:\\nvalid_set = panx_de_encoded[\"validation\"]\\nvalid_set = valid_set.map(forward_pass_with_label, batched=True, batch_size=32)\\ndf = valid_set.to_pandas()\\nThe tokens and the labels are still encoded with their IDs, so let’s map the tokens and\\nlabels back to strings to make it easier to read the results. For the padding tokens with\\nlabel –100 we assign a special label, IGN, so we can filter them later. We also get rid of\\nall the padding in the loss and predicted_label fields by truncating them to the\\nlength of the inputs:\\nindex2tag[-100] = \"IGN\"\\ndf[\"input_tokens\"] = df[\"input_ids\"].apply(\\n    lambda x: xlmr_tokenizer.convert_ids_to_tokens(x))\\ndf[\"predicted_label\"] = df[\"predicted_label\"].apply(\\n    lambda x: [index2tag[i] for i in x])\\nError Analysis | 109'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 133, 'page_label': '110'}, page_content='df[\"labels\"] = df[\"labels\"].apply(\\n    lambda x: [index2tag[i] for i in x])\\ndf[\\'loss\\'] = df.apply(\\n    lambda x: x[\\'loss\\'][:len(x[\\'input_ids\\'])], axis=1)\\ndf[\\'predicted_label\\'] = df.apply(\\n    lambda x: x[\\'predicted_label\\'][:len(x[\\'input_ids\\'])], axis=1)\\ndf.head(1)\\nattention_mask input_ids labels loss predicted_label input_tokens\\n0 [1, 1, 1, 1, 1, 1, 1] [0, 10699, 11,\\n15, 16104,\\n1388, 2]\\n[IGN, B-\\nORG, IGN, I-\\nORG, I-ORG,\\nI-ORG, IGN]\\n[0.0,\\n0.014679872, 0.0,\\n0.009469474,\\n0.010393422,\\n0.01293836, 0.0]\\n[I-ORG, B-ORG, I-ORG,\\nI-ORG, I-ORG, I-ORG, I-\\nORG]\\n[<s>, ▁Ham, a, ▁(,\\n▁Unternehmen, ▁),\\n</s>]\\nEach column contains a list of tokens, labels, predicted labels, and so on for each\\nsample. Let’s have a look at the tokens individually by unpacking these lists. The\\npandas.Series.explode() function allows us to do exactly that in one line by creat‐\\ning a row for each element in the original rows list. Since all the lists in one row have\\nthe same length, we can do this in parallel for all columns. We also drop the padding\\ntokens we named IGN, since their loss is zero anyway. Finally, we cast the losses, which\\nare still numpy.Array objects, to standard floats:\\ndf_tokens = df.apply(pd.Series.explode)\\ndf_tokens = df_tokens.query(\"labels != \\'IGN\\'\")\\ndf_tokens[\"loss\"] = df_tokens[\"loss\"].astype(float).round(2)\\ndf_tokens.head(7)\\nattention_mask input_ids labels loss predicted_label input_tokens\\n1 10699 B-ORG 0.01 B-ORG ▁Ham\\n1 15 I-ORG 0.01 I-ORG ▁(\\n1 16104 I-ORG 0.01 I-ORG ▁Unternehmen\\n1 1388 I-ORG 0.01 I-ORG ▁)\\n1 56530 O 0.00 O ▁WE\\n1 83982 B-ORG 0.34 B-ORG ▁Luz\\n1 10 I-ORG 0.45 I-ORG ▁a\\nWith the data in this shape, we can now group it by the input tokens and aggregate\\nthe losses for each token with the count, mean, and sum. Finally, we sort the\\naggregated data by the sum of the losses and see which tokens have accumulated the\\nmost loss in the validation set:\\n(\\n    df_tokens.groupby(\"input_tokens\")[[\"loss\"]]\\n    .agg([\"count\", \"mean\", \"sum\"])\\n    .droplevel(level=0, axis=1)  # Get rid of multi-level columns\\n110 | Chapter 4: Multilingual Named Entity Recognition'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 134, 'page_label': '111'}, page_content='.sort_values(by=\"sum\", ascending=False)\\n    .reset_index()\\n    .round(2)\\n    .head(10)\\n    .T\\n)\\n0 1 2 3 4 5 6 7 8 9\\ninput_tokens ▁ ▁der ▁in ▁von ▁/ ▁und ▁( ▁) ▁\\'\\' ▁A\\ncount 6066 1388 989 808 163 1171 246 246 2898 125\\nmean 0.03 0.1 0.14 0.14 0.64 0.08 0.3 0.29 0.02 0.44\\nsum 200.71 138.05 137.33 114.92 104.28 99.15 74.49 72.35 59.31 54.48\\nWe can observe several patterns in this list:\\n• The whitespace token has the highest total loss, which is not surprising since it is\\nalso the most common token in the list. However, its mean loss is much lower\\nthan the other tokens in the list. This means that the model doesn’t struggle to\\nclassify it.\\n• Words like “in” , “von” , “der” , and “und” appear relatively frequently. They often\\nappear together with named entities and are sometimes part of them, which\\nexplains why the model might mix them up.\\n• Parentheses, slashes, and capital letters at the beginning of words are rarer but\\nhave a relatively high average loss. We will investigate them further.\\nWe can also group the label IDs and look at the losses for each class:\\n(\\n    df_tokens.groupby(\"labels\")[[\"loss\"]]\\n    .agg([\"count\", \"mean\", \"sum\"])\\n    .droplevel(level=0, axis=1)\\n    .sort_values(by=\"mean\", ascending=False)\\n    .reset_index()\\n    .round(2)\\n    .T\\n)\\n0 1 2 3 4 5 6\\nlabels B-ORG I-LOC I-ORG B-LOC B-PER I-PER O\\ncount 2683 1462 3820 3172 2893 4139 43648\\nmean 0.66 0.64 0.48 0.35 0.26 0.18 0.03\\nsum 1769.47 930.94 1850.39 1111.03 760.56 750.91 1354.46\\nError Analysis | 111'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 135, 'page_label': '112'}, page_content='We see that B-ORG has the highest average loss, which means that determining the\\nbeginning of an organization poses a challenge to our model.\\nWe can break this down further by plotting the confusion matrix of the token classifi‐\\ncation, where we see that the beginning of an organization is often confused with the\\nsubsequent I-ORG token:\\nfrom sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\\ndef plot_confusion_matrix(y_preds, y_true, labels):\\n    cm = confusion_matrix(y_true, y_preds, normalize=\"true\")\\n    fig, ax = plt.subplots(figsize=(6, 6))\\n    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\\n    disp.plot(cmap=\"Blues\", values_format=\".2f\", ax=ax, colorbar=False)\\n    plt.title(\"Normalized confusion matrix\")\\n    plt.show()\\nplot_confusion_matrix(df_tokens[\"labels\"], df_tokens[\"predicted_label\"],\\n                      tags.names)\\nFrom the plot, we can see that our model tends to confuse the B-ORG and I-ORG enti‐\\nties the most. Otherwise, it is quite good at classifying the remaining entities, which is\\nclear by the near diagonal nature of the confusion matrix.\\n112 | Chapter 4: Multilingual Named Entity Recognition'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 136, 'page_label': '113'}, page_content='Now that we’ve examined the errors at the token level, let’s move on and look at\\nsequences with high losses. For this calculation, we’ll revisit our “unexploded” Data\\nFrame and calculate the total loss by summing over the loss per token. To do this, let’s\\nfirst write a function that helps us display the token sequences with the labels and the\\nlosses:\\ndef get_samples(df):\\n    for _, row in df.iterrows():\\n        labels, preds, tokens, losses = [], [], [], []\\n        for i, mask in enumerate(row[\"attention_mask\"]):\\n            if i not in {0, len(row[\"attention_mask\"])}:\\n                labels.append(row[\"labels\"][i])\\n                preds.append(row[\"predicted_label\"][i])\\n                tokens.append(row[\"input_tokens\"][i])\\n                losses.append(f\"{row[\\'loss\\'][i]:.2f}\")\\n        df_tmp = pd.DataFrame({\"tokens\": tokens, \"labels\": labels,\\n                               \"preds\": preds, \"losses\": losses}).T\\n        yield df_tmp\\ndf[\"total_loss\"] = df[\"loss\"].apply(sum)\\ndf_tmp = df.sort_values(by=\"total_loss\", ascending=False).head(3)\\nfor sample in get_samples(df_tmp):\\n    display(sample)\\n0 1 2 3 4 ... 13 14 15 16 17\\ntokens ▁\\'\\' 8 . ▁Juli ▁\\'\\' ... n ischen ▁Gar de </s>\\nlabels B-ORG IGN IGN I-ORG I-ORG ... IGN IGN I-ORG IGN IGN\\npreds O O O O O ... I-ORG I-ORG I-ORG I-ORG O\\nlosses 7.89 0.00 0.00 6.88 8.05 ... 0.00 0.00 0.01 0.00 0.00\\n0 1 2 3 4 ... 14 15 16 17 18\\ntokens ▁\\' ▁\\'\\' ▁Τ Κ ▁\\'\\' ... k ▁\\'\\' ▁\\' ala </s>\\nlabels O O O IGN O ... IGN I-LOC I-LOC IGN IGN\\npreds O O B-ORG O O ... O O O O O\\nlosses 0.00 0.00 3.59 0.00 0.00 ... 0.00 7.66 7.78 0.00 0.00\\n0 1 2 3 4 ... 10 11 12 13 14\\ntokens ▁United ▁Nations ▁Multi dimensional ▁Integra ... ▁the ▁Central ▁African ▁Republic </s>\\nlabels B-PER I-PER I-PER IGN I-PER ... I-PER I-PER I-PER I-PER IGN\\npreds B-ORG I-ORG I-ORG I-ORG I-ORG ... I-\\nORG\\nI-ORG I-ORG I-ORG I-\\nORG\\nlosses 6.46 5.59 5.51 0.00 5.11 ... 4.77 5.32 5.10 4.87 0.00\\nError Analysis | 113'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 137, 'page_label': '114'}, page_content='It is apparent that something is wrong with the labels of these samples; for example,\\nthe United Nations and the Central African Republic are each labeled as a person! At\\nthe same time, “8. Juli” in the first example is labeled as an organization. It turns out\\nthe annotations for the PAN-X dataset were generated through an automated process.\\nSuch annotations are often referred to as “silver standard” (in contrast to the “gold\\nstandard” of human-generated annotations), and it is no surprise that there are cases\\nwhere the automated approach failed to produce sensible labels. In fact, such failure\\nmodes are not unique to automatic approaches; even when humans carefully anno‐\\ntate data, mistakes can occur when the concentration of the annotators fades or they\\nsimply misunderstand the sentence.\\nAnother thing we noticed earlier was that parentheses and slashes had a relatively\\nhigh loss. Let’s look at a few examples of sequences with an opening parenthesis:\\ndf_tmp = df.loc[df[\"input_tokens\"].apply(lambda x: u\"\\\\u2581(\" in x)].head(2)\\nfor sample in get_samples(df_tmp):\\n    display(sample)\\n0 1 2 3 4 5\\ntokens ▁Ham a ▁( ▁Unternehmen ▁) </s>\\nlabels B-ORG IGN I-ORG I-ORG I-ORG IGN\\npreds B-ORG I-ORG I-ORG I-ORG I-ORG I-ORG\\nlosses 0.01 0.00 0.01 0.01 0.01 0.00\\n0 1 2 3 4 5 6 7\\ntokens ▁Kesk kül a ▁( ▁Mart na ▁) </s>\\nlabels B-LOC IGN IGN I-LOC I-LOC IGN I-LOC IGN\\npreds B-LOC I-LOC I-LOC I-LOC I-LOC I-LOC I-LOC I-LOC\\nlosses 0.02 0.00 0.00 0.01 0.01 0.00 0.01 0.00\\nIn general we would not include the parentheses and their contents as part of the\\nnamed entity, but this seems to be the way the automatic extraction annotated the\\ndocuments. In the other examples, the parentheses contain a geographic specification.\\nWhile this is indeed a location as well, we might want disconnect it from the original\\nlocation in the annotations. This dataset consists of Wikipedia articles in different\\nlanguages, and the article titles often contain some sort of explanation in parentheses.\\nFor instance, in the first example the text in parentheses indicates that Hama is an\\n“Unternehmen, ” or company in English. These are important details to know when\\nwe roll out the model, as they might have implications on the downstream perfor‐\\nmance of the whole pipeline the model is part of.\\nWith a relatively simple analysis, we’ve identified some weaknesses in both our model\\nand the dataset. In a real use case we would iterate on this step, cleaning up the\\n114 | Chapter 4: Multilingual Named Entity Recognition'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 138, 'page_label': '115'}, page_content='dataset, retraining the model, and analyzing the new errors until we were satisfied\\nwith the performance.\\nHere we analyzed the errors on a single language, but we are also interested in the\\nperformance across languages. In the next section we’ll perform some experiments to\\nsee how well the cross-lingual transfer in XLM-R works.\\nCross-Lingual Transfer\\nNow that we have fine-tuned XLM-R on German, we can evaluate its ability to trans‐\\nfer to other languages via the predict() method of the Trainer. Since we plan to\\nevaluate multiple languages, let’s create a simple function that does this for us:\\ndef get_f1_score(trainer, dataset):\\n    return trainer.predict(dataset).metrics[\"test_f1\"]\\nWe can use this function to examine the performance on the test set and keep track of\\nour scores in a dict:\\nf1_scores = defaultdict(dict)\\nf1_scores[\"de\"][\"de\"] = get_f1_score(trainer, panx_de_encoded[\"test\"])\\nprint(f\"F1-score of [de] model on [de] dataset: {f1_scores[\\'de\\'][\\'de\\']:.3f}\")\\nF1-score of [de] model on [de] dataset: 0.868\\nThese are pretty good results for a NER task. Our metrics are in the ballpark of 85%,\\nand we can see that the model seems to struggle the most on the ORG entities, proba‐\\nbly because these are the least common in the training data and many organization\\nnames are rare in XLM-R’s vocabulary. How about the other languages? To warm up,\\nlet’s see how our model fine-tuned on German fares on French:\\ntext_fr = \"Jeff Dean est informaticien chez Google en Californie\"\\ntag_text(text_fr, tags, trainer.model, xlmr_tokenizer)\\n0 1 2 3 4 5 6 7 8 9 10 11 12 13\\nTokens <s> ▁Jeff ▁De an ▁est ▁informatic ien ▁chez ▁Google ▁en ▁Cali for nie </s>\\nTags O B-PER I-\\nPER\\nI-\\nPER\\nO O O O B-ORG O B-LOC I-\\nLOC\\nI-\\nLOC\\nO\\nNot bad! Although the name and organization are the same in both languages, the\\nmodel did manage to correctly label the French translation of “Kalifornien” . Next, let’s\\nquantify how well our German model fares on the whole French test set by writing a\\nsimple function that encodes a dataset and generates the classification report on it:\\ndef evaluate_lang_performance(lang, trainer):\\n    panx_ds = encode_panx_dataset(panx_ch[lang])\\n    return get_f1_score(trainer, panx_ds[\"test\"])\\nCross-Lingual Transfer | 115'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 139, 'page_label': '116'}, page_content='f1_scores[\"de\"][\"fr\"] = evaluate_lang_performance(\"fr\", trainer)\\nprint(f\"F1-score of [de] model on [fr] dataset: {f1_scores[\\'de\\'][\\'fr\\']:.3f}\")\\nF1-score of [de] model on [fr] dataset: 0.714\\nAlthough we see a drop of about 15 points in the micro-averaged metrics, remember\\nthat our model has not seen a single labeled French example! In general, the size of\\nthe performance drop is related to how “far away” the languages are from each other.\\nAlthough German and French are grouped as Indo-European languages, they techni‐\\ncally belong to different language families: Germanic and Romance, respectively.\\nNext, let’s evaluate the performance on Italian. Since Italian is also a Romance lan‐\\nguage, we expect to get a similar result as we found on French:\\nf1_scores[\"de\"][\"it\"] = evaluate_lang_performance(\"it\", trainer)\\nprint(f\"F1-score of [de] model on [it] dataset: {f1_scores[\\'de\\'][\\'it\\']:.3f}\")\\nF1-score of [de] model on [it] dataset: 0.692\\nIndeed, our expectations are borne out by the F1-scores. Finally, let’s examine the per‐\\nformance on English, which belongs to the Germanic language family:\\nf1_scores[\"de\"][\"en\"] = evaluate_lang_performance(\"en\", trainer)\\nprint(f\"F1-score of [de] model on [en] dataset: {f1_scores[\\'de\\'][\\'en\\']:.3f}\")\\nF1-score of [de] model on [en] dataset: 0.589\\nSurprisingly, our model fares worst on English, even though we might intuitively\\nexpect German to be more similar to English than French. Having fine-tuned on Ger‐\\nman and performed zero-shot transfer to French and English, let’s next examine\\nwhen it makes sense to fine-tune directly on the target language.\\nWhen Does Zero-Shot Transfer Make Sense?\\nSo far we’ve seen that fine-tuning XLM-R on the German corpus yields an F1-score of\\naround 85%, and without any additional training the model is able to achieve modest\\nperformance on the other languages in our corpus. The question is, how good are\\nthese results and how do they compare against an XLM-R model fine-tuned on a\\nmonolingual corpus?\\nIn this section we will explore this question for the French corpus by fine-tuning\\nXLM-R on training sets of increasing size. By tracking the performance this way, we\\ncan determine at which point zero-shot cross-lingual transfer is superior, which in\\npractice can be useful for guiding decisions about whether to collect more labeled\\ndata.\\nFor simplicity, we’ll keep the same hyperparameters from the fine-tuning run on the\\nGerman corpus, except that we’ll tweak the logging_steps argument of Training\\nArguments to account for the changing training set sizes. We can wrap this all\\ntogether in a simple function that takes a DatasetDict object corresponding to a\\n116 | Chapter 4: Multilingual Named Entity Recognition'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 140, 'page_label': '117'}, page_content='monolingual corpus, downsamples it by num_samples, and fine-tunes XLM-R on that\\nsample to return the metrics from the best epoch:\\ndef train_on_subset(dataset, num_samples):\\n    train_ds = dataset[\"train\"].shuffle(seed=42).select(range(num_samples))\\n    valid_ds = dataset[\"validation\"]\\n    test_ds = dataset[\"test\"]\\n    training_args.logging_steps = len(train_ds) // batch_size\\n    trainer = Trainer(model_init=model_init, args=training_args,\\n        data_collator=data_collator, compute_metrics=compute_metrics,\\n        train_dataset=train_ds, eval_dataset=valid_ds, tokenizer=xlmr_tokenizer)\\n    trainer.train()\\n    if training_args.push_to_hub:\\n        trainer.push_to_hub(commit_message=\"Training completed!\")\\n    f1_score = get_f1_score(trainer, test_ds)\\n    return pd.DataFrame.from_dict(\\n        {\"num_samples\": [len(train_ds)], \"f1_score\": [f1_score]})\\nAs we did with fine-tuning on the German corpus, we also need to encode the French\\ncorpus into input IDs, attention masks, and label IDs:\\npanx_fr_encoded = encode_panx_dataset(panx_ch[\"fr\"])\\nNext let’s check that our function works by running it on a small training set of 250\\nexamples:\\ntraining_args.push_to_hub = False\\nmetrics_df = train_on_subset(panx_fr_encoded, 250)\\nmetrics_df\\nnum_samples f1_score\\n0 250 0.137329\\nWe can see that with only 250 examples, fine-tuning on French underperforms the\\nzero-shot transfer from German by a large margin. Let’s now increase our training set\\nsizes to 500, 1,000, 2,000, and 4,000 examples to get an idea of how the performance\\nincreases:\\nfor num_samples in [500, 1000, 2000, 4000]:\\n    metrics_df = metrics_df.append(\\n        train_on_subset(panx_fr_encoded, num_samples), ignore_index=True)\\nWe can compare how fine-tuning on French samples compares to zero-shot cross-\\nlingual transfer from German by plotting the F1-scores on the test set as a function of\\nincreasing training set size:\\nfig, ax = plt.subplots()\\nax.axhline(f1_scores[\"de\"][\"fr\"], ls=\"--\", color=\"r\")\\nmetrics_df.set_index(\"num_samples\").plot(ax=ax)\\nCross-Lingual Transfer | 117'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 141, 'page_label': '118'}, page_content='plt.legend([\"Zero-shot from de\", \"Fine-tuned on fr\"], loc=\"lower right\")\\nplt.ylim((0, 1))\\nplt.xlabel(\"Number of Training Samples\")\\nplt.ylabel(\"F1 Score\")\\nplt.show()\\nFrom the plot we can see that zero-shot transfer remains competitive until about 750\\ntraining examples, after which fine-tuning on French reaches a similar level of perfor‐\\nmance to what we obtained when fine-tuning on German. Nevertheless, this result is\\nnot to be sniffed at! In our experience, getting domain experts to label even hundreds\\nof documents can be costly, especially for NER, where the labeling process is fine-\\ngrained and time-consuming.\\nThere is one final technique we can try to evaluate multilingual learning: fine-tuning\\non multiple languages at once! Let’s see how we can do this.\\nFine-Tuning on Multiple Languages at Once\\nSo far we’ve seen that zero-shot cross-lingual transfer from German to French or Ital‐\\nian produces a drop of around 15 points in performance. One way to mitigate this is\\nby fine-tuning on multiple languages at the same time. To see what type of gains we\\ncan get, let’s first use the concatenate_datasets() function from \\n  Datasets to con‐\\ncatenate the German and French corpora together:\\nfrom datasets import concatenate_datasets\\ndef concatenate_splits(corpora):\\n    multi_corpus = DatasetDict()\\n    for split in corpora[0].keys():\\n        multi_corpus[split] = concatenate_datasets(\\n118 | Chapter 4: Multilingual Named Entity Recognition'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 142, 'page_label': '119'}, page_content='[corpus[split] for corpus in corpora]).shuffle(seed=42)\\n    return multi_corpus\\npanx_de_fr_encoded = concatenate_splits([panx_de_encoded, panx_fr_encoded])\\nFor training, we’ll again use the same hyperparameters from the previous sections, so\\nwe can simply update the logging steps, model, and datasets in the trainer:\\ntraining_args.logging_steps = len(panx_de_fr_encoded[\"train\"]) // batch_size\\ntraining_args.push_to_hub = True\\ntraining_args.output_dir = \"xlm-roberta-base-finetuned-panx-de-fr\"\\ntrainer = Trainer(model_init=model_init, args=training_args,\\n    data_collator=data_collator, compute_metrics=compute_metrics,\\n    tokenizer=xlmr_tokenizer, train_dataset=panx_de_fr_encoded[\"train\"],\\n    eval_dataset=panx_de_fr_encoded[\"validation\"])\\ntrainer.train()\\ntrainer.push_to_hub(commit_message=\"Training completed!\")\\nLet’s have a look at how the model performs on the test set of each language:\\nfor lang in langs:\\n    f1 = evaluate_lang_performance(lang, trainer)\\n    print(f\"F1-score of [de-fr] model on [{lang}] dataset: {f1:.3f}\")\\nF1-score of [de-fr] model on [de] dataset: 0.866\\nF1-score of [de-fr] model on [fr] dataset: 0.868\\nF1-score of [de-fr] model on [it] dataset: 0.815\\nF1-score of [de-fr] model on [en] dataset: 0.677\\nIt performs much better on the French split than before, matching the performance\\non the German test set. Interestingly, its performance on the Italian and English splits\\nalso improves by roughly 10 points! So, even adding training data in another lan‐\\nguage improves the performance of the model on unseen languages.\\nLet’s round out our analysis by comparing the performance of fine-tuning on each\\nlanguage separately against multilingual learning on all the corpora. Since we have\\nalready fine-tuned on the German corpus, we can fine-tune on the remaining lan‐\\nguages with our train_on_subset() function, with num_samples equal to the num‐\\nber of examples in the training set:\\ncorpora = [panx_de_encoded]\\n# Exclude German from iteration\\nfor lang in langs[1:]:\\n    training_args.output_dir = f\"xlm-roberta-base-finetuned-panx-{lang}\"\\n    # Fine-tune on monolingual corpus\\n    ds_encoded = encode_panx_dataset(panx_ch[lang])\\n    metrics = train_on_subset(ds_encoded, ds_encoded[\"train\"].num_rows)\\n    # Collect F1-scores in common dict\\n    f1_scores[lang][lang] = metrics[\"f1_score\"][0]\\nCross-Lingual Transfer | 119'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 143, 'page_label': '120'}, page_content='# Add monolingual corpus to list of corpora to concatenate\\n    corpora.append(ds_encoded)\\nNow that we’ve fine-tuned on each language’s corpus, the next step is to concatenate\\nall the splits together to create a multilingual corpus of all four languages. As with the\\nprevious German and French analysis, we can use the concatenate_splits() func‐\\ntion to do this step for us on the list of corpora we generated in the previous step:\\ncorpora_encoded = concatenate_splits(corpora)\\nNow that we have our multilingual corpus, we run the familiar steps with the trainer:\\ntraining_args.logging_steps = len(corpora_encoded[\"train\"]) // batch_size\\ntraining_args.output_dir = \"xlm-roberta-base-finetuned-panx-all\"\\ntrainer = Trainer(model_init=model_init, args=training_args,\\n    data_collator=data_collator, compute_metrics=compute_metrics,\\n    tokenizer=xlmr_tokenizer, train_dataset=corpora_encoded[\"train\"],\\n    eval_dataset=corpora_encoded[\"validation\"])\\ntrainer.train()\\ntrainer.push_to_hub(commit_message=\"Training completed!\")\\nThe final step is to generate the predictions from the trainer on each language’s test\\nset. This will give us an insight into how well multilingual learning is really working.\\nWe’ll collect the F1-scores in our f1_scores dictionary and then create a DataFrame\\nthat summarizes the main results from our multilingual experiments:\\nfor idx, lang in enumerate(langs):\\n    f1_scores[\"all\"][lang] = get_f1_score(trainer, corpora[idx][\"test\"])\\nscores_data = {\"de\": f1_scores[\"de\"],\\n               \"each\": {lang: f1_scores[lang][lang] for lang in langs},\\n               \"all\": f1_scores[\"all\"]}\\nf1_scores_df = pd.DataFrame(scores_data).T.round(4)\\nf1_scores_df.rename_axis(index=\"Fine-tune on\", columns=\"Evaluated on\",\\n                         inplace=True)\\nf1_scores_df\\nEvaluated on de fr it en\\nFine-tune on\\nde 0.8677 0.7141 0.6923 0.5890\\neach 0.8677 0.8505 0.8192 0.7068\\nall 0.8682 0.8647 0.8575 0.7870\\nFrom these results we can draw a few general conclusions:\\n• Multilingual learning can provide significant gains in performance, especially if\\nthe low-resource languages for cross-lingual transfer belong to similar language\\n120 | Chapter 4: Multilingual Named Entity Recognition'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 144, 'page_label': '121'}, page_content='families. In our experiments we can see that German, French, and Italian achieve\\nsimilar performance in the all category, suggesting that these languages are\\nmore similar to each other than to English.\\n• As a general strategy, it is a good idea to focus attention on cross-lingual transfer\\nwithin language families, especially when dealing with different scripts like\\nJapanese.\\nInteracting with Model Widgets\\nIn this chapter, we’ve pushed quite a few fine-tuned models to the Hub. Although we\\ncould use the pipeline() function to interact with them on our local machine, the\\nHub provides widgets that are great for this kind of workflow. An example is shown in\\nFigure 4-5  for our transformersbook/xlm-roberta-base-finetuned-panx-all\\ncheckpoint, which as you can see has done a good job at identifying all the entities of\\na German text.\\nFigure 4-5. Example of a widget on the Hugging Face Hub\\nConclusion\\nIn this chapter we saw how to tackle an NLP task on a multilingual corpus using a\\nsingle transformer pretrained on 100 languages: XLM-R. Although we were able to\\nshow that cross-lingual transfer from German to French is competitive when only a\\nsmall number of labeled examples are available for fine-tuning, this good perfor‐\\nmance generally does not occur if the target language is significantly different from\\nthe one the base model was fine-tuned on or was not one of the 100 languages used\\nduring pretraining. Recent proposals like MAD-X are designed precisely for these\\nInteracting with Model Widgets | 121'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 145, 'page_label': '122'}, page_content='6 J. Pfeiffer et al., “MAD-X: An Adapter-Based Framework for Multi-Task Cross-Lingual Transfer”, (2020).\\nlow-resource scenarios, and since MAD-X is built on top of \\n  Transformers you can\\neasily adapt the code in this chapter to work with it!6\\nSo far we have looked at two tasks: sequence classification and token classification.\\nThese both fall into the domain of natural language understanding, where text is syn‐\\nthesized into predictions. In the next chapter we have our first look at text generation,\\nwhere not only the input but also the output of the model is text.\\n122 | Chapter 4: Multilingual Named Entity Recognition'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 146, 'page_label': '123'}, page_content='1 This example comes from OpenAI’s blog post on GPT-2.\\nCHAPTER 5\\nText Generation\\nOne of the most uncanny features of transformer-based language models is their abil‐\\nity to generate text that is almost indistinguishable from text written by humans. A\\nfamous example is OpenAI’s GPT-2, which when given the prompt:1\\nIn a shocking finding, scientist discovered a herd of unicorns living in a remote, previ‐\\nously unexplored valley, in the Andes Mountains. Even more surprising to the\\nresearchers was the fact that the unicorns spoke perfect English.\\nwas able to generate a compelling news article about talking unicorns:\\nThe scientist named the population, after their distinctive horn, Ovid’s Unicorn. These\\nfour-horned, silver-white unicorns were previously unknown to science. Now, after\\nalmost two centuries, the mystery of what sparked this odd phenomenon is finally\\nsolved. Dr. Jorge Pérez, an evolutionary biologist from the University of La Paz, and\\nseveral companions, were exploring the Andes Mountains when they found a small\\nvalley, with no other animals or humans. Pérez noticed that the valley had what\\nappeared to be a natural fountain, surrounded by two peaks of rock and silver snow.\\nPérez and the others then ventured further into the valley. “By the time we reached the\\ntop of one peak, the water looked blue, with some crystals on top, ” said Pérez. Pérez\\nand his friends were astonished to see the unicorn herd. These creatures could be seen\\nfrom the air without having to move too much to see them—they were so close they\\ncould touch their horns. While examining these bizarre creatures the scientists discov‐\\nered that the creatures also spoke some fairly regular English …\\n123'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 147, 'page_label': '124'}, page_content='2 However, as Delip Rao points out, whether Meena intends to tell corny jokes is a subtle question.\\nWhat makes this example so remarkable is that it was generated without any explicit\\nsupervision! By simply learning to predict the next word in the text of millions of web\\npages, GPT-2 and its more powerful descendants like GPT-3 are able to acquire a\\nbroad set of skills and pattern recognition abilities that can be activated with different\\nkinds of input prompts. Figure 5-1  shows how language models are sometimes\\nexposed during pretraining to sequences of tasks where they need to predict the fol‐\\nlowing tokens based on the context alone, like addition, unscrambling words, and\\ntranslation. This allows them to transfer this knowledge effectively during fine-tuning\\nor (if the model is large enough) at inference time. These tasks are not chosen ahead\\nof time, but occur naturally in the huge corpora used to train billion-parameter lan‐\\nguage models.\\nFigure 5-1. During pretraining, language models are exposed to sequences of tasks that\\ncan be adapted during inference (courtesy of Tom B. Brown)\\nThe ability of transformers to generate realistic text has led to a diverse range of\\napplications, like InferKit, Write With Transformer, AI Dungeon, and conversational\\nagents like Google’s Meena that can even tell corny jokes, as shown in Figure 5-2!2\\n124 | Chapter 5: Text Generation'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 148, 'page_label': '125'}, page_content='Figure 5-2. Meena on the left telling a corny joke to a human on the right (courtesy of\\nDaniel Adiwardana and Thang Luong)\\nIn this chapter we’ll use GPT-2 to illustrate how text generation works for language\\nmodels and explore how different decoding strategies impact the generated texts.\\nThe Challenge with Generating Coherent Text\\nSo far in this book, we have focused on tackling NLP tasks via a combination of pre‐\\ntraining and supervised fine-tuning. As we’ve seen, for task-specific heads like\\nsequence or token classification, generating predictions is fairly straightforward; the\\nmodel produces some logits and we either take the maximum value to get the predic‐\\nted class, or apply a softmax function to obtain the predicted probabilities per class.\\nBy contrast, converting the model’s probabilistic output to text requires a decoding\\nmethod, which introduces a few challenges that are unique to text generation:\\n• The decoding is done iteratively and thus involves significantly more compute\\nthan simply passing inputs once through the forward pass of a model.\\n• The quality and diversity of the generated text depend on the choice of decoding\\nmethod and associated hyperparameters.\\nTo understand how this decoding process works, let’s start by examining how GPT-2\\nis pretrained and subsequently applied to generate text.\\nThe Challenge with Generating Coherent Text | 125'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 149, 'page_label': '126'}, page_content='Like other autoregressive or causal language models, GPT-2 is pretrained to estimate\\nthe probability P\\ud835\\ud835 of a sequence of tokens \\ud835 = y1, y2, ...yt occurring in the text,\\ngiven some initial prompt or context sequence \\ud835 = x1, x2, ...xk. Since it is impractical\\nto acquire enough training data to estimate P\\ud835\\ud835 directly, it is common to use the\\nchain rule of probability to factorize it as a product of conditional probabilities:\\nP y1, ..., yt \\ud835 = ∏\\nt = 1\\nN\\nP yt y < t, \\ud835\\nwhere y<t is a shorthand notation for the sequence y1, ..., yt − 1. It is from these con‐\\nditional probabilities that we pick up the intuition that autoregressive language mod‐\\neling amounts to predicting each word given the preceding words in a sentence; this\\nis exactly what the probability on the righthand side of the preceding equation\\ndescribes. Notice that this pretraining objective is quite different from BERT’s, which\\nutilizes both past and future contexts to predict a masked token.\\nBy now you may have guessed how we can adapt this next token prediction task to\\ngenerate text sequences of arbitrary length. As shown in Figure 5-3, we start with a\\nprompt like “Transformers are the” and use the model to predict the next token.\\nOnce we have determined the next token, we append it to the prompt and then use\\nthe new input sequence to generate another token. We do this until we have reached a\\nspecial end-of-sequence token or a predefined maximum length.\\nFigure 5-3. Generating text from an input sequence by adding a new word to the input\\nat each step\\nSince the output sequence is conditioned on the choice of input\\nprompt, this type of text generation is often called conditional text\\ngeneration.\\n126 | Chapter 5: Text Generation'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 150, 'page_label': '127'}, page_content='3 If you run out of memory on your machine, you can load a smaller GPT-2 version by replacing model_name =\\n\"gpt-xl\" with model_name = \"gpt\".\\nAt the heart of this process lies a decoding method that determines which token is\\nselected at each timestep. Since the language model head produces a logit zt,i per\\ntoken in the vocabulary at each step, we can get the probability distribution over the\\nnext possible token wi by taking the softmax:\\nP yt = wi y < t, \\ud835 = softmax zt, i\\nThe goal of most decoding methods is to search for the most likely overall sequence\\nby picking a \\ud835 such that:\\n\\ud835 = argmax\\n\\ud835\\nP \\ud835 \\ud835\\nFinding \\ud835 directly would involve evaluating every possible sequence with the lan‐\\nguage model. Since there does not exist an algorithm that can do this in a reasonable\\namount of time, we rely on approximations instead. In this chapter we’ll explore a few\\nof these approximations and gradually build up toward smarter and more complex\\nalgorithms that can be used to generate high-quality texts.\\nGreedy Search Decoding\\nThe simplest decoding method to get discrete tokens from a model’s continuous out‐\\nput is to greedily select the token with the highest probability at each timestep:\\nyt = argmax\\nyt\\nP yt y < t, \\ud835\\nTo see how greedy search works, let’s start by loading the 1.5-billion-parameter ver‐\\nsion of GPT-2 with a language modeling head:3\\nimport torch\\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\\nmodel_name = \"gpt2-xl\"\\ntokenizer = AutoTokenizer.from_pretrained(model_name)\\nmodel = AutoModelForCausalLM.from_pretrained(model_name).to(device)\\nNow let’s generate some text! Although \\n  Transformers provides a generate() func‐\\ntion for autoregressive models like GPT-2, we’ll implement this decoding method\\nGreedy Search Decoding | 127'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 151, 'page_label': '128'}, page_content='ourselves to see what goes on under the hood. To warm up, we’ll take the same itera‐\\ntive approach shown in Figure 5-3 : we’ll use “Transformers are the” as the input\\nprompt and run the decoding for eight timesteps. At each timestep, we pick out the\\nmodel’s logits for the last token in the prompt and wrap them with a softmax to get a\\nprobability distribution. We then pick the next token with the highest probability, add\\nit to the input sequence, and run the process again. The following code does the job,\\nand also stores the five most probable tokens at each timestep so we can visualize the\\nalternatives:\\nimport pandas as pd\\ninput_txt = \"Transformers are the\"\\ninput_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].to(device)\\niterations = []\\nn_steps = 8\\nchoices_per_step = 5\\nwith torch.no_grad():\\n    for _ in range(n_steps):\\n        iteration = dict()\\n        iteration[\"Input\"] = tokenizer.decode(input_ids[0])\\n        output = model(input_ids=input_ids)\\n        # Select logits of the first batch and the last token and apply softmax\\n        next_token_logits = output.logits[0, -1, :]\\n        next_token_probs = torch.softmax(next_token_logits, dim=-1)\\n        sorted_ids = torch.argsort(next_token_probs, dim=-1, descending=True)\\n        # Store tokens with highest probabilities\\n        for choice_idx in range(choices_per_step):\\n            token_id = sorted_ids[choice_idx]\\n            token_prob = next_token_probs[token_id].cpu().numpy()\\n            token_choice = (\\n                f\"{tokenizer.decode(token_id)} ({100 * token_prob:.2f}%)\"\\n            )\\n            iteration[f\"Choice {choice_idx+1}\"] = token_choice\\n        # Append predicted next token to input\\n        input_ids = torch.cat([input_ids, sorted_ids[None, 0, None]], dim=-1)\\n        iterations.append(iteration)\\npd.DataFrame(iterations)\\nInput Choice 1 Choice 2 Choice 3 Choice 4 Choice 5\\n0 Transformers are the most (8.53%) only (4.96%) best (4.65%) Transformers\\n(4.37%)\\nultimate\\n(2.16%)\\n1 Transformers are the most popular\\n(16.78%)\\npowerful\\n(5.37%)\\ncommon (4.96%) famous (3.72%) successful\\n(3.20%)\\n2 Transformers are the most\\npopular\\ntoy (10.63%) toys (7.23%) Transformers\\n(6.60%)\\nof (5.46%) and (3.76%)\\n3 Transformers are the most\\npopular toy\\nline (34.38%) in (18.20%) of (11.71%) brand (6.10%) line (2.69%)\\n128 | Chapter 5: Text Generation'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 152, 'page_label': '129'}, page_content='Input Choice 1 Choice 2 Choice 3 Choice 4 Choice 5\\n4 Transformers are the most\\npopular toy line\\nin (46.28%) of (15.09%) , (4.94%) on (4.40%) ever (2.72%)\\n5 Transformers are the most\\npopular toy line in\\nthe (65.99%) history\\n(12.42%)\\nAmerica (6.91%) Japan (2.44%) North (1.40%)\\n6 Transformers are the most\\npopular toy line in the\\nworld\\n(69.26%)\\nUnited\\n(4.55%)\\nhistory (4.29%) US (4.23%) U (2.30%)\\n7 Transformers are the most\\npopular toy line in the world\\n, (39.73%) . (30.64%) and (9.87%) with (2.32%) today (1.74%)\\nWith this simple method we were able to generate the sentence “Transformers are the\\nmost popular toy line in the world” . Interestingly, this indicates that GPT-2 has inter‐\\nnalized some knowledge about the Transformers media franchise, which was created\\nby two toy companies (Hasbro and Takara Tomy). We can also see the other possible\\ncontinuations at each step, which shows the iterative nature of text generation. Unlike\\nother tasks such as sequence classification where a single forward pass suffices to gen‐\\nerate the predictions, with text generation we need to decode the output tokens one at\\na time.\\nImplementing greedy search wasn’t too hard, but we’ll want to use the built-in\\ngenerate() function from \\n  Transformers to explore more sophisticated decoding\\nmethods. To reproduce our simple example, let’s make sure sampling is switched off\\n(it’s off by default, unless the specific configuration of the model you are loading the\\ncheckpoint from states otherwise) and specify the max_new_tokens for the number of\\nnewly generated tokens:\\ninput_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].to(device)\\noutput = model.generate(input_ids, max_new_tokens=n_steps, do_sample=False)\\nprint(tokenizer.decode(output[0]))\\nTransformers are the most popular toy line in the world,\\nNow let’s try something a bit more interesting: can we reproduce the unicorn story\\nfrom OpenAI? As we did previously, we’ll encode the prompt with the tokenizer, and\\nwe’ll specify a larger value for max_length to generate a longer sequence of text:\\nmax_length = 128\\ninput_txt = \"\"\"In a shocking finding, scientist discovered \\\\\\na herd of unicorns living in a remote, previously unexplored \\\\\\nvalley, in the Andes Mountains. Even more surprising to the \\\\\\nresearchers was the fact that the unicorns spoke perfect English.\\\\n\\\\n\\n\"\"\"\\ninput_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].to(device)\\noutput_greedy = model.generate(input_ids, max_length=max_length,\\n                               do_sample=False)\\nprint(tokenizer.decode(output_greedy[0]))\\nGreedy Search Decoding | 129'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 153, 'page_label': '130'}, page_content='4 N.S. Keskar et al., “CTRL: A Conditional Transformer Language Model for Controllable Generation”, (2019).\\nIn a shocking finding, scientist discovered a herd of unicorns living in a\\nremote, previously unexplored valley, in the Andes Mountains. Even more\\nsurprising to the researchers was the fact that the unicorns spoke perfect\\nEnglish.\\nThe researchers, from the University of California, Davis, and the University of\\nColorado, Boulder, were conducting a study on the Andean cloud forest, which is\\nhome to the rare species of cloud forest trees.\\nThe researchers were surprised to find that the unicorns were able to\\ncommunicate with each other, and even with humans.\\nThe researchers were surprised to find that the unicorns were able\\nWell, the first few sentences are quite different from the OpenAI example and amus‐\\ningly involve different universities being credited with the discovery! We can also see\\none of the main drawbacks with greedy search decoding: it tends to produce repeti‐\\ntive output sequences, which is certainly undesirable in a news article. This is a com‐\\nmon problem with greedy search algorithms, which can fail to give you the optimal\\nsolution; in the context of decoding, they can miss word sequences whose overall\\nprobability is higher just because high-probability words happen to be preceded by\\nlow-probability ones.\\nFortunately, we can do better—let’s examine a popular method known as beam search\\ndecoding.\\nAlthough greedy search decoding is rarely used for text generation\\ntasks that require diversity, it can be useful for producing short\\nsequences like arithmetic where a deterministic and factually cor‐\\nrect output is preferred. 4 For these tasks, you can condition GPT-2\\nby providing a few line-separated examples in the format \"5 + 8\\n=> 13 \\\\n 7 + 2 => 9 \\\\n 1 + 0 =>\" as the input prompt.\\nBeam Search Decoding\\nInstead of decoding the token with the highest probability at each step, beam search\\nkeeps track of the top-b most probable next tokens, where b is referred to as the num‐\\nber of beams or partial hypotheses. The next set of beams are chosen by considering\\nall possible next-token extensions of the existing set and selecting the b most likely\\nextensions. The process is repeated until we reach the maximum length or an EOS\\n130 | Chapter 5: Text Generation'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 154, 'page_label': '131'}, page_content='token, and the most likely sequence is selected by ranking the b beams according to\\ntheir log probabilities. An example of beam search is shown in Figure 5-4.\\nFigure 5-4. Beam search with two beams\\nWhy do we score the sequences using log probabilities instead of the probabilities\\nthemselves? That calculating the overall probability of a sequence P y1, y2, ..., yt \\ud835\\ninvolves calculating a product of conditional probabilities P yt y < t, \\ud835  is one reason.\\nSince each conditional probability is typically a small number in the range [ 0, 1],\\ntaking their product can lead to an overall probability that can easily underflow. This\\nmeans that the computer can no longer precisely represent the result of the calcula‐\\ntion. For example, suppose we have a sequence of t = 1024 tokens and generously\\nassume that the probability for each token is 0.5. The overall probability for this\\nsequence is an extremely small number:\\n0.5 ** 1024\\n5.562684646268003e-309\\nwhich leads to numerical instability as we run into underflow. We can avoid this by\\ncalculating a related term, the log probability. If we apply the logarithm to the joint\\nand conditional probabilities, then with the help of the product rule for logarithms\\nwe get:\\nlog P y1, ...yt \\ud835 = ∑\\nt = 1\\nN\\nlog P yt y < t, \\ud835\\nIn other words, the product of probabilities we saw earlier becomes a sum of log\\nprobabilities, which is much less likely to run into numerical instabilities. For exam‐\\nple, calculating the log probability of the same example as before gives:\\nBeam Search Decoding | 131'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 155, 'page_label': '132'}, page_content='import numpy as np\\nsum([np.log(0.5)] * 1024)\\n-709.7827128933695\\nThis is a number we can easily deal with, and this approach still works for much\\nsmaller numbers. Since we only want to compare relative probabilities, we can do this\\ndirectly with log probabilities.\\nLet’s calculate and compare the log probabilities of the texts generated by greedy and\\nbeam search to see if beam search can improve the overall probability. Since \\n  Trans‐\\nformers models return the unnormalized logits for the next token given the input\\ntokens, we first need to normalize the logits to create a probability distribution over\\nthe whole vocabulary for each token in the sequence. We then need to select only the\\ntoken probabilities that were present in the sequence. The following function imple‐\\nments these steps:\\nimport torch.nn.functional as F\\ndef log_probs_from_logits(logits, labels):\\n    logp = F.log_softmax(logits, dim=-1)\\n    logp_label = torch.gather(logp, 2, labels.unsqueeze(2)).squeeze(-1)\\n    return logp_label\\nThis gives us the log probability for a single token, so to get the total log probability\\nof a sequence we just need to sum the log probabilities for each token:\\ndef sequence_logprob(model, labels, input_len=0):\\n    with torch.no_grad():\\n        output = model(labels)\\n        log_probs = log_probs_from_logits(\\n            output.logits[:, :-1, :], labels[:, 1:])\\n        seq_log_prob = torch.sum(log_probs[:, input_len:])\\n    return seq_log_prob.cpu().numpy()\\nNote that we ignore the log probabilities of the input sequence because they are not\\ngenerated by the model. We can also see that it is important to align the logits and the\\nlabels; since the model predicts the next token, we do not get a logit for the first label,\\nand we don’t need the last logit because we don’t have a ground truth token for it.\\nLet’s use these functions to first calculate the sequence log probability of the greedy\\ndecoder on the OpenAI prompt:\\nlogp = sequence_logprob(model, output_greedy, input_len=len(input_ids[0]))\\nprint(tokenizer.decode(output_greedy[0]))\\nprint(f\"\\\\nlog-prob: {logp:.2f}\")\\nIn a shocking finding, scientist discovered a herd of unicorns living in a\\nremote, previously unexplored valley, in the Andes Mountains. Even more\\nsurprising to the researchers was the fact that the unicorns spoke perfect\\nEnglish.\\n132 | Chapter 5: Text Generation'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 156, 'page_label': '133'}, page_content='The researchers, from the University of California, Davis, and the University of\\nColorado, Boulder, were conducting a study on the Andean cloud forest, which is\\nhome to the rare species of cloud forest trees.\\nThe researchers were surprised to find that the unicorns were able to\\ncommunicate with each other, and even with humans.\\nThe researchers were surprised to find that the unicorns were able\\nlog-prob: -87.43\\nNow let’s compare this to a sequence that is generated with beam search. To activate\\nbeam search with the generate() function we just need to specify the number of\\nbeams with the num_beams parameter. The more beams we choose, the better the\\nresult potentially gets; however, the generation process becomes much slower since\\nwe generate parallel sequences for each beam:\\noutput_beam = model.generate(input_ids, max_length=max_length, num_beams=5,\\n                             do_sample=False)\\nlogp = sequence_logprob(model, output_beam, input_len=len(input_ids[0]))\\nprint(tokenizer.decode(output_beam[0]))\\nprint(f\"\\\\nlog-prob: {logp:.2f}\")\\nIn a shocking finding, scientist discovered a herd of unicorns living in a\\nremote, previously unexplored valley, in the Andes Mountains. Even more\\nsurprising to the researchers was the fact that the unicorns spoke perfect\\nEnglish.\\nThe discovery of the unicorns was made by a team of scientists from the\\nUniversity of California, Santa Cruz, and the National Geographic Society.\\nThe scientists were conducting a study of the Andes Mountains when they\\ndiscovered a herd of unicorns living in a remote, previously unexplored valley,\\nin the Andes Mountains. Even more surprising to the researchers was the fact\\nthat the unicorns spoke perfect English\\nlog-prob: -55.23\\nWe can see that we get a better log probability (higher is better) with beam search\\nthan we did with simple greedy decoding. However, we can see that beam search also\\nsuffers from repetitive text. One way to address this is to impose an n-gram penalty\\nwith the no_repeat_ngram_size parameter that tracks which n-grams have been seen\\nand sets the next token probability to zero if it would produce a previously seen\\nn-gram:\\nBeam Search Decoding | 133'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 157, 'page_label': '134'}, page_content='output_beam = model.generate(input_ids, max_length=max_length, num_beams=5,\\n                             do_sample=False, no_repeat_ngram_size=2)\\nlogp = sequence_logprob(model, output_beam, input_len=len(input_ids[0]))\\nprint(tokenizer.decode(output_beam[0]))\\nprint(f\"\\\\nlog-prob: {logp:.2f}\")\\nIn a shocking finding, scientist discovered a herd of unicorns living in a\\nremote, previously unexplored valley, in the Andes Mountains. Even more\\nsurprising to the researchers was the fact that the unicorns spoke perfect\\nEnglish.\\nThe discovery was made by a team of scientists from the University of\\nCalifornia, Santa Cruz, and the National Geographic Society.\\nAccording to a press release, the scientists were conducting a survey of the\\narea when they came across the herd. They were surprised to find that they were\\nable to converse with the animals in English, even though they had never seen a\\nunicorn in person before. The researchers were\\nlog-prob: -93.12\\nThis isn’t too bad! We’ve managed to stop the repetitions, and we can see that despite\\nproducing a lower score, the text remains coherent. Beam search with n-gram penalty\\nis a good way to find a trade-off between focusing on high-probability tokens (with\\nbeam search) while reducing repetitions (with n-gram penalty), and it’s commonly\\nused in applications such as summarization or machine translation where factual cor‐\\nrectness is important. When factual correctness is less important than the diversity of\\ngenerated output, for instance in open-domain chitchat or story generation, another\\nalternative to reduce repetitions while improving diversity is to use sampling. Let’s\\nround out our exploration of text generation by examining a few of the most com‐\\nmon sampling methods.\\nSampling Methods\\nThe simplest sampling method is to randomly sample from the probability distribu‐\\ntion of the model’s outputs over the full vocabulary at each timestep:\\nP yt = wi y < t, \\ud835 = softmax zt, i =\\nexp zt, i\\n∑j = 1\\nV exp zt, j\\nwhere V denotes the cardinality of the vocabulary. We can easily control the diver‐\\nsity of the output by adding a temperature parameter T that rescales the logits before\\ntaking the softmax:\\n134 | Chapter 5: Text Generation'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 158, 'page_label': '135'}, page_content='5 If you know some physics, you may recognize a striking resemblance to the Boltzmann distribution.\\nP yt = wi y < t, \\ud835 =\\nexp zt, i/T\\n∑j = 1\\nV exp zt, j/T\\nBy tuning T we can control the shape of the probability distribution. 5 When T≪1,\\nthe distribution becomes peaked around the origin and the rare tokens are sup‐\\npressed. On the other hand, when T≫1, the distribution flattens out and each token\\nbecomes equally likely. The effect of temperature on token probabilities is shown in\\nFigure 5-5.\\nFigure 5-5. Distribution of randomly generated token probabilities for three selected\\ntemperatures\\nTo see how we can use temperature to influence the generated text, let’s sample with\\nT= 2 by setting the temperature parameter in the generate() function (we’ll\\nexplain the meaning of the top_k parameter in the next section):\\noutput_temp = model.generate(input_ids, max_length=max_length, do_sample=True,\\n                             temperature=2.0, top_k=0)\\nprint(tokenizer.decode(output_temp[0]))\\nIn a shocking finding, scientist discovered a herd of unicorns living in a\\nremote, previously unexplored valley, in the Andes Mountains. Even more\\nsurprising to the researchers was the fact that the unicorns spoke perfect\\nEnglish.\\nWhile the station aren protagonist receive Pengala nostalgiates tidbitRegarding\\nSampling Methods | 135'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 159, 'page_label': '136'}, page_content=\"Jenny loclonju AgreementCON irrational �rite Continent seaf A jer Turner\\nDorbecue WILL Pumpkin mere Thatvernuildagain YoAniamond disse *\\nRunewitingkusstemprop});b zo coachinginventorymodules deflation press\\nVaticanpres Wrestling chargesThingsctureddong Ty physician PET KimBi66 graz Oz\\nat aff da temporou MD6 radi iter\\nWe can clearly see that a high temperature has produced mostly gibberish; by accen‐\\ntuating the rare tokens, we’ve caused the model to create strange grammar and quite a\\nfew made-up words! Let’s see what happens if we cool down the temperature:\\noutput_temp = model.generate(input_ids, max_length=max_length, do_sample=True,\\n                             temperature=0.5, top_k=0)\\nprint(tokenizer.decode(output_temp[0]))\\nIn a shocking finding, scientist discovered a herd of unicorns living in a\\nremote, previously unexplored valley, in the Andes Mountains. Even more\\nsurprising to the researchers was the fact that the unicorns spoke perfect\\nEnglish.\\nThe scientists were searching for the source of the mysterious sound, which was\\nmaking the animals laugh and cry.\\nThe unicorns were living in a remote valley in the Andes mountains\\n'When we first heard the noise of the animals, we thought it was a lion or a\\ntiger,' said Luis Guzman, a researcher from the University of Buenos Aires,\\nArgentina.\\n'But when\\nThis is significantly more coherent, and even includes a quote from yet another uni‐\\nversity being credited with the discovery! The main lesson we can draw from temper‐\\nature is that it allows us to control the quality of the samples, but there’s always a\\ntrade-off between coherence (low temperature) and diversity (high temperature) that\\none has to tune to the use case at hand.\\nAnother way to adjust the trade-off between coherence and diversity is to truncate\\nthe distribution of the vocabulary. This allows us to adjust the diversity freely with\\nthe temperature, but in a more limited range that excludes words that would be too\\nstrange in the context (i.e., low-probability words). There are two main ways to do\\nthis: top-k and nucleus (or top-p) sampling. Let’s take a look.\\nTop-k and Nucleus Sampling\\nTop-k and nucleus (top- p) sampling are two popular alternatives or extensions to\\nusing temperature. In both cases, the basic idea is to restrict the number of possible\\ntokens we can sample from at each timestep. To see how this works, let’s first visualize\\n136 | Chapter 5: Text Generation\"), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 160, 'page_label': '137'}, page_content='the cumulative probability distribution of the model’s outputs at T= 1 as seen in\\nFigure 5-6.\\nLet’s tease apart these plots, since they contain a lot of information. In the upper plot\\nwe can see a histogram of the token probabilities. It has a peak around 10−8 and a\\nsecond, smaller peak around 10−4, followed by a sharp drop with just a handful of\\ntokens occurring with probability between 10−2 and 10−1. Looking at this diagram,\\nwe can see that the probability of picking the token with the highest probability (the\\nisolated bar at 10−1) is 1 in 10.\\nFigure 5-6. Probability distribution of next token prediction (upper) and cumulative dis‐\\ntribution of descending token probabilities (lower)\\nIn the lower plot, we’ve ordered the tokens by descending probability and calculated\\nthe cumulative sum of the first 10,000 tokens (in total, there are 50,257 tokens in\\nGPT-2’s vocabulary). The curved line represents the probability of picking any of the\\nTop-k and Nucleus Sampling | 137'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 161, 'page_label': '138'}, page_content=\"preceding tokens. For example, there is roughly a 96% chance of picking any of the\\n1,000 tokens with the highest probability. We see that the probability rises quickly\\nabove 90% but saturates to close to 100% only after several thousand tokens. The plot\\nshows that there is a 1 in 100 chance of not picking any of the tokens that are not\\neven in the top 2,000.\\nAlthough these numbers might appear small at first sight, they become important\\nbecause we sample once per token when generating text. So even if there is only a 1 in\\n100 or 1,000 chance, if we sample hundreds of times there is a significant chance of\\npicking an unlikely token at some point—and picking such tokens when sampling\\ncan badly influence the quality of the generated text. For this reason, we generally\\nwant to avoid these very unlikely tokens. This is where top- k and top- p sampling\\ncome into play.\\nThe idea behind top- k sampling is to avoid the low-probability choices by only sam‐\\npling from the k tokens with the highest probability. This puts a fixed cut on the long\\ntail of the distribution and ensures that we only sample from likely choices. Going\\nback to Figure 5-6, top-k sampling is equivalent to defining a vertical line and sam‐\\npling from the tokens on the left. Again, the generate() function provides an easy\\nmethod to achieve this with the top_k argument:\\noutput_topk = model.generate(input_ids, max_length=max_length, do_sample=True,\\n                             top_k=50)\\nprint(tokenizer.decode(output_topk[0]))\\nIn a shocking finding, scientist discovered a herd of unicorns living in a\\nremote, previously unexplored valley, in the Andes Mountains. Even more\\nsurprising to the researchers was the fact that the unicorns spoke perfect\\nEnglish.\\nThe wild unicorns roam the Andes Mountains in the region of Cajamarca, on the\\nborder with Argentina (Picture: Alamy/Ecole Nationale Supérieure d'Histoire\\nNaturelle)\\nThe researchers came across about 50 of the animals in the valley. They had\\nlived in such a remote and isolated area at that location for nearly a thousand\\nyears that\\nThis is arguably the most human-looking text we’ve generated so far. But how do we\\nchoose k? The value of k is chosen manually and is the same for each choice in the\\nsequence, independent of the actual output distribution. We can find a good value for\\nk by looking at some text quality metrics, which we will explore in the next chapter—\\nbut that fixed cutoff might not be very satisfactory.\\nAn alternative is to use a dynamic cutoff. With nucleus or top- p sampling, instead of\\nchoosing a fixed cutoff value, we set a condition of when to cut off. This condition is\\nwhen a certain probability mass in the selection is reached. Let’s say we set that value\\n138 | Chapter 5: Text Generation\"), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 162, 'page_label': '139'}, page_content='to 95%. We then order all tokens in descending order by probability and add one\\ntoken after another from the top of the list until the sum of the probabilities of the\\nselected tokens is 95%. Returning to Figure 5-6, the value for p defines a horizontal\\nline on the cumulative sum of probabilities plot, and we sample only from tokens\\nbelow the line. Depending on the output distribution, this could be just one (very\\nlikely) token or a hundred (more equally likely) tokens. At this point, you are proba‐\\nbly not surprised that the generate() function also provides an argument to activate\\ntop-p sampling. Let’s try it out:\\noutput_topp = model.generate(input_ids, max_length=max_length, do_sample=True,\\n                             top_p=0.90)\\nprint(tokenizer.decode(output_topp[0]))\\nIn a shocking finding, scientist discovered a herd of unicorns living in a\\nremote, previously unexplored valley, in the Andes Mountains. Even more\\nsurprising to the researchers was the fact that the unicorns spoke perfect\\nEnglish.\\nThe scientists studied the DNA of the animals and came to the conclusion that\\nthe herd are descendants of a prehistoric herd that lived in Argentina about\\n50,000 years ago.\\nAccording to the scientific analysis, the first humans who migrated to South\\nAmerica migrated into the Andes Mountains from South Africa and Australia, after\\nthe last ice age had ended.\\nSince their migration, the animals have been adapting to\\nTop-p sampling has also produced a coherent story, and this time with a new twist\\nabout migrations from Australia to South America. Y ou can even combine the two\\nsampling approaches to get the best of both worlds. Setting top_k=50 and top_p=0.9\\ncorresponds to the rule of choosing tokens with a probability mass of 90%, from a\\npool of at most 50 tokens.\\nWe can also apply beam search when we use sampling. Instead of\\nselecting the next batch of candidate tokens greedily, we can sample\\nthem and build up the beams in the same way.\\nTop-k and Nucleus Sampling | 139'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 163, 'page_label': '140'}, page_content='Which Decoding Method Is Best?\\nUnfortunately, there is no universally “best” decoding method. Which approach is\\nbest will depend on the nature of the task you are generating text for. If you want\\nyour model to perform a precise task like arithmetic or providing an answer to a spe‐\\ncific question, then you should lower the temperature or use deterministic methods\\nlike greedy search in combination with beam search to guarantee getting the most\\nlikely answer. If you want the model to generate longer texts and even be a bit crea‐\\ntive, then you should switch to sampling methods and increase the temperature or\\nuse a mix of top-k and nucleus sampling.\\nConclusion\\nIn this chapter we looked at text generation, which is a very different task from the\\nNLU tasks we encountered previously. Generating text requires at least one forward\\npass per generated token, and even more if we use beam search. This makes text gen‐\\neration computationally demanding, and one needs the right infrastructure to run a\\ntext generation model at scale. In addition, a good decoding strategy that transforms\\nthe model’s output probabilities into discrete tokens can improve the text quality.\\nFinding the best decoding strategy requires some experimentation and a subjective\\nevaluation of the generated texts.\\nIn practice, however, we don’t want to make these decisions based on gut feeling\\nalone! Like with other NLP tasks, we should choose a model performance metric that\\nreflects the problem we want to solve. Unsurprisingly, there are a wide range of\\nchoices, and we will encounter the most common ones in the next chapter, where we\\nhave a look at how to train and evaluate a model for text summarization. Or, if you\\ncan’t wait to learn how to train a GPT-type model from scratch, you can skip right to\\nChapter 10, where we collect a large dataset of code and then train an autoregressive\\nlanguage model on it.\\n140 | Chapter 5: Text Generation'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 164, 'page_label': '141'}, page_content='CHAPTER 6\\nSummarization\\nAt one point or another, you’ve probably needed to summarize a document, be it a\\nresearch article, a financial earnings report, or a thread of emails. If you think about\\nit, this requires a range of abilities, such as understanding long passages, reasoning\\nabout the contents, and producing fluent text that incorporates the main topics from\\nthe original document. Moreover, accurately summarizing a news article is very dif‐\\nferent from summarizing a legal contract, so being able to do so requires a sophistica‐\\nted degree of domain generalization. For these reasons, text summarization is a\\ndifficult task for neural language models, including transformers. Despite these chal‐\\nlenges, text summarization offers the prospect for domain experts to significantly\\nspeed up their workflows and is used by enterprises to condense internal knowledge,\\nsummarize contracts, automatically generate content for social media releases,\\nand more.\\nTo help you understand the challenges involved, this chapter will explore how we can\\nleverage pretrained transformers to summarize documents. Summarization is a clas‐\\nsic sequence-to-sequence (seq2seq) task with an input text and a target text. As we\\nsaw in Chapter 1, this is where encoder-decoder transformers excel.\\nIn this chapter we will build our own encoder-decoder model to condense dialogues\\nbetween several people into a crisp summary. But before we get to that, let’s begin by\\ntaking a look at one of the canonical datasets for summarization: the CNN/DailyMail\\ncorpus.\\nThe CNN/DailyMail Dataset\\nThe CNN/DailyMail dataset consists of around 300,000 pairs of news articles and\\ntheir corresponding summaries, composed from the bullet points that CNN and the\\nDailyMail attach to their articles. An important aspect of the dataset is that the\\n141'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 165, 'page_label': '142'}, page_content='summaries are abstractive and not extractive, which means that they consist of new\\nsentences instead of simple excerpts. The dataset is available on the Hub; we’ll use\\nversion 3.0.0, which is a nonanonymized version set up for summarization. We can\\nselect versions in a similar manner as splits, we saw in Chapter 4 , with a version\\nkeyword. So let’s dive in and have a look at it:\\nfrom datasets import load_dataset\\ndataset = load_dataset(\"cnn_dailymail\", version=\"3.0.0\")\\nprint(f\"Features: {dataset[\\'train\\'].column_names}\")\\nFeatures: [\\'article\\', \\'highlights\\', \\'id\\']\\nThe dataset has three columns: article, which contains the news articles, high\\nlights with the summaries, and id to uniquely identify each article. Let’s look at an\\nexcerpt from an article:\\nsample = dataset[\"train\"][1]\\nprint(f\"\"\"\\nArticle (excerpt of 500 characters, total length: {len(sample[\"article\"])}):\\n\"\"\")\\nprint(sample[\"article\"][:500])\\nprint(f\\'\\\\nSummary (length: {len(sample[\"highlights\"])}):\\')\\nprint(sample[\"highlights\"])\\nArticle (excerpt of 500 characters, total length: 3192):\\n(CNN) -- Usain Bolt rounded off the world championships Sunday by claiming his\\nthird gold in Moscow as he anchored Jamaica to victory in the men\\'s 4x100m\\nrelay. The fastest man in the world charged clear of United States rival Justin\\nGatlin as the Jamaican quartet of Nesta Carter, Kemar Bailey-Cole, Nickel\\nAshmeade and Bolt won in 37.36 seconds. The U.S finished second in 37.56 seconds\\nwith Canada taking the bronze after Britain were disqualified for a faulty\\nhandover. The 26-year-old Bolt has n\\nSummary (length: 180):\\nUsain Bolt wins third gold of world championship .\\nAnchors Jamaica to 4x100m relay victory .\\nEighth gold at the championships for Bolt .\\nJamaica double up in women\\'s 4x100m relay .\\nWe see that the articles can be very long compared to the target summary; in this par‐\\nticular case the difference is 17-fold. Long articles pose a challenge to most trans‐\\nformer models since the context size is usually limited to 1,000 tokens or so, which is\\nequivalent to a few paragraphs of text. The standard, yet crude way to deal with this\\nfor summarization is to simply truncate the texts beyond the model’s context size.\\nObviously there could be important information for the summary toward the end of\\nthe text, but for now we need to live with this limitation of the model architectures.\\n142 | Chapter 6: Summarization'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 166, 'page_label': '143'}, page_content='Text Summarization Pipelines\\nLet’s see how a few of the most popular transformer models for summarization per‐\\nform by first looking qualitatively at the outputs for the preceding example. Although\\nthe model architectures we will be exploring have varying maximum input sizes, let’s\\nrestrict the input text to 2,000 characters to have the same input for all models and\\nthus make the outputs more comparable:\\nsample_text = dataset[\"train\"][1][\"article\"][:2000]\\n# We\\'ll collect the generated summaries of each model in a dictionary\\nsummaries = {}\\nA convention in summarization is to separate the summary sentences by a newline.\\nWe could add a newline token after each full stop, but this simple heuristic would fail\\nfor strings like “U.S. ” or “U.N. ” The Natural Language Toolkit (NLTK) package\\nincludes a more sophisticated algorithm that can differentiate the end of a sentence\\nfrom punctuation that occurs in abbreviations:\\nimport nltk\\nfrom nltk.tokenize import sent_tokenize\\nnltk.download(\"punkt\")\\nstring = \"The U.S. are a country. The U.N. is an organization.\"\\nsent_tokenize(string)\\n[\\'The U.S. are a country.\\', \\'The U.N. is an organization.\\']\\nIn the following sections we will load several large models. If you\\nrun out of memory, you can either replace the large models with\\nsmaller checkpoints (e.g., “gpt” , “t5-small”) or skip this section and\\njump to “Evaluating PEGASUS on the CNN/DailyMail Dataset” on\\npage 154.\\nSummarization Baseline\\nA common baseline for summarizing news articles is to simply take the first three\\nsentences of the article. With NLTK’s sentence tokenizer, we can easily implement\\nsuch a baseline:\\ndef three_sentence_summary(text):\\n    return \"\\\\n\".join(sent_tokenize(text)[:3])\\nsummaries[\"baseline\"] = three_sentence_summary(sample_text)\\nText Summarization Pipelines | 143'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 167, 'page_label': '144'}, page_content='1 A. Radford et al., “Language Models Are Unsupervised Multitask Learners”, OpenAI (2019).\\nGPT-2\\nWe’ve already seen in Chapter 5 how GPT-2 can generate text given some prompt.\\nOne of the model’s surprising features is that we can also use it to generate summaries\\nby simply appending “TL;DR” at the end of the input text. The expression “TL;DR”\\n(too long; didn’t read) is often used on platforms like Reddit to indicate a short ver‐\\nsion of a long post. We will start our summarization experiment by re-creating the\\nprocedure of the original paper with the pipeline() function from \\n  Transformers.1\\nWe create a text generation pipeline and load the large GPT-2 model:\\nfrom transformers import pipeline, set_seed\\nset_seed(42)\\npipe = pipeline(\"text-generation\", model=\"gpt2-xl\")\\ngpt2_query = sample_text + \"\\\\nTL;DR:\\\\n\"\\npipe_out = pipe(gpt2_query, max_length=512, clean_up_tokenization_spaces=True)\\nsummaries[\"gpt2\"] = \"\\\\n\".join(\\n    sent_tokenize(pipe_out[0][\"generated_text\"][len(gpt2_query) :]))\\nHere we just store the summaries of the generated text by slicing off the input query\\nand keep the result in a Python dictionary for later comparison.\\nT5\\nNext let’s try the T5 transformer. As we saw in Chapter 3 , the developers of this\\nmodel performed a comprehensive study of transfer learning in NLP and found they\\ncould create a universal transformer architecture by formulating all tasks as text-to-\\ntext tasks. The T5 checkpoints are trained on a mixture of unsupervised data (to\\nreconstruct masked words) and supervised data for several tasks, including summari‐\\nzation. These checkpoints can thus be directly used to perform summarization\\nwithout fine-tuning by using the same prompts used during pretraining. In this\\nframework, the input format for the model to summarize a document is \"summarize:\\n<ARTICLE>\", and for translation it looks like \"translate English to German:\\n<TEXT>\". As shown in Figure 6-1, this makes T5 extremely versatile and allows you to\\nsolve many tasks with a single model.\\nWe can directly load T5 for summarization with the pipeline() function, which also\\ntakes care of formatting the inputs in the text-to-text format so we don’t need to pre‐\\npend them with \"summarize\":\\npipe = pipeline(\"summarization\", model=\"t5-large\")\\npipe_out = pipe(sample_text)\\nsummaries[\"t5\"] = \"\\\\n\".join(sent_tokenize(pipe_out[0][\"summary_text\"]))\\n144 | Chapter 6: Summarization'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 168, 'page_label': '145'}, page_content='2 M. Lewis et al., “BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation,\\nTranslation, and Comprehension”, (2019).\\n3 J. Zhang et al., “PEGASUS: Pre-Training with Extracted Gap-Sentences for Abstractive Summarization”,\\n(2019).\\nFigure 6-1. Diagram of T5’s text-to-text framework (courtesy of Colin Raffel); besides\\ntranslation and summarization, the CoLA (linguistic acceptability) and STSB (semantic\\nsimilarity) tasks are shown\\nBART\\nBART also uses an encoder-decoder architecture and is trained to reconstruct cor‐\\nrupted inputs. It combines the pretraining schemes of BERT and GPT-2. 2 We’ll use\\nthe facebook/bart-large-ccn checkpoint, which has been specifically fine-tuned on\\nthe CNN/DailyMail dataset:\\npipe = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\\npipe_out = pipe(sample_text)\\nsummaries[\"bart\"] = \"\\\\n\".join(sent_tokenize(pipe_out[0][\"summary_text\"]))\\nPEGASUS\\nLike BART, PEGASUS is an encoder-decoder transformer. 3 As shown in Figure 6-2,\\nits pretraining objective is to predict masked sentences in multisentence texts. The\\nauthors argue that the closer the pretraining objective is to the downstream task, the\\nmore effective it is. With the aim of finding a pretraining objective that is closer to\\nsummarization than general language modeling, they automatically identified, in a\\nvery large corpus, sentences containing most of the content of their surrounding\\nparagraphs (using summarization evaluation metrics as a heuristic for content\\noverlap) and pretrained the PEGASUS model to reconstruct these sentences, thereby\\nobtaining a state-of-the-art model for text summarization.\\nText Summarization Pipelines | 145'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 169, 'page_label': '146'}, page_content='Figure 6-2. Diagram of PEGASUS architecture (courtesy of Jingqing Zhang et al.)\\nThis model has a special token for newlines, which is why we don’t need the\\nsent_tokenize() function:\\npipe = pipeline(\"summarization\", model=\"google/pegasus-cnn_dailymail\")\\npipe_out = pipe(sample_text)\\nsummaries[\"pegasus\"] = pipe_out[0][\"summary_text\"].replace(\" .<n>\", \".\\\\n\")\\nComparing Different Summaries\\nNow that we have generated summaries with four different models, let’s compare the\\nresults. Keep in mind that one model has not been trained on the dataset at all\\n(GPT-2), one model has been fine-tuned on this task among others (T5), and two\\nmodels have exclusively been fine-tuned on this task (BART and PEGASUS). Let’s\\nhave a look at the summaries these models have generated:\\nprint(\"GROUND TRUTH\")\\nprint(dataset[\"train\"][1][\"highlights\"])\\nprint(\"\")\\nfor model_name in summaries:\\n    print(model_name.upper())\\n    print(summaries[model_name])\\n    print(\"\")\\nGROUND TRUTH\\nUsain Bolt wins third gold of world championship .\\nAnchors Jamaica to 4x100m relay victory .\\nEighth gold at the championships for Bolt .\\nJamaica double up in women\\'s 4x100m relay .\\nBASELINE\\n146 | Chapter 6: Summarization'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 170, 'page_label': '147'}, page_content=\"(CNN) -- Usain Bolt rounded off the world championships Sunday by claiming his\\nthird gold in Moscow as he anchored Jamaica to victory in the men's 4x100m\\nrelay.\\nThe fastest man in the world charged clear of United States rival Justin Gatlin\\nas the Jamaican quartet of Nesta Carter, Kemar Bailey-Cole, Nickel Ashmeade and\\nBolt won in 37.36 seconds.\\nThe U.S finished second in 37.56 seconds with Canada taking the bronze after\\nBritain were disqualified for a faulty handover.\\nGPT2\\nNesta, the fastest man in the world.\\nGatlin, the most successful Olympian ever.\\nKemar, a Jamaican legend.\\nShelly-Ann, the fastest woman ever.\\nBolt, the world's greatest athlete.\\nThe team sport of pole vaulting\\nT5\\nusain bolt wins his third gold medal of the world championships in the men's\\n4x100m relay .\\nthe 26-year-old anchored Jamaica to victory in the event in the Russian capital\\n.\\nhe has now collected eight gold medals at the championships, equaling the record\\n.\\nBART\\nUsain Bolt wins his third gold of the world championships in Moscow.\\nBolt anchors Jamaica to victory in the men's 4x100m relay.\\nThe 26-year-old has now won eight gold medals at world championships.\\nJamaica's women also win gold in the relay, beating France in the process.\\nPEGASUS\\nUsain Bolt wins third gold of world championships.\\nAnchors Jamaica to victory in men's 4x100m relay.\\nEighth gold at the championships for Bolt.\\nJamaica also win women's 4x100m relay .\\nThe first thing we notice by looking at the model outputs is that the summary gener‐\\nated by GPT-2 is quite different from the others. Instead of giving a summary of the\\ntext, it summarizes the characters. Often the GPT-2 model “hallucinates” or invents\\nfacts, since it was not explicitly trained to generate truthful summaries. For example,\\nat the time of writing, Nesta is not the fastest man in the world, but sits in ninth place.\\nComparing the other three model summaries against the ground truth, we see that\\nthere is remarkable overlap, with PEGASUS’s output bearing the most striking\\nresemblance.\\nNow that we have inspected a few models, let’s try to decide which one we would use\\nin a production setting. All four models seem to provide qualitatively reasonable\\nresults, and we could generate a few more examples to help us decide. However, this\\nis not a systematic way of determining the best model! Ideally, we would define a\\nComparing Different Summaries | 147\"), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 171, 'page_label': '148'}, page_content='4 K. Papineni et al., “BLEU: A Method for Automatic Evaluation of Machine Translation, ” Proceedings of the\\n40th Annual Meeting of the Association for Computational Linguistics (July 2002): 311–318, http://dx.doi.org/\\n10.3115/1073083.1073135.\\nmetric, measure it for all models on some benchmark dataset, and choose the one\\nwith the best performance. But how do you define a metric for text generation? The\\nstandard metrics that we’ve seen, like accuracy, recall, and precision, are not easy to\\napply to this task. For each “gold standard” summary written by a human, dozens of\\nother summaries with synonyms, paraphrases, or a slightly different way of formulat‐\\ning the facts could be just as acceptable.\\nIn the next section we will look at some common metrics that have been developed\\nfor measuring the quality of generated text.\\nMeasuring the Quality of Generated Text\\nGood evaluation metrics are important, since we use them to measure the perfor‐\\nmance of models not only when we train them but also later, in production. If we\\nhave bad metrics we might be blind to model degradation, and if they are misaligned\\nwith the business goals we might not create any value.\\nMeasuring performance on a text generation task is not as easy as with standard clas‐\\nsification tasks such as sentiment analysis or named entity recognition. Take the\\nexample of translation; given a sentence like “I love dogs!” in English and translating\\nit to Spanish there can be multiple valid possibilities, like “¡Me encantan los perros!”\\nor “¡Me gustan los perros!” Simply checking for an exact match to a reference transla‐\\ntion is not optimal; even humans would fare badly on such a metric because we all\\nwrite text slightly differently from each other (and even from ourselves, depending on\\nthe time of the day or year!). Fortunately, there are alternatives.\\nTwo of the most common metrics used to evaluate generated text are BLEU and\\nROUGE. Let’s take a look at how they’re defined.\\nBLEU\\nThe idea of BLEU is simple:4 instead of looking at how many of the tokens in the gen‐\\nerated texts are perfectly aligned with the reference text tokens, we look at words or\\nn-grams. BLEU is a precision-based metric, which means that when we compare the\\ntwo texts we count the number of words in the generation that occur in the reference\\nand divide it by the length of the generation.\\nHowever, there is an issue with this vanilla precision. Assume the generated text just\\nrepeats the same word over and over again, and this word also appears in the refer‐\\nence. If it is repeated as many times as the length of the reference text, then we get\\n148 | Chapter 6: Summarization'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 172, 'page_label': '149'}, page_content='perfect precision! For this reason, the authors of the BLEU paper introduced a slight\\nmodification: a word is only counted as many times as it occurs in the reference. To\\nillustrate this point, suppose we have the reference text “the cat is on the mat” and the\\ngenerated text “the the the the the the” .\\nFrom this simple example, we can calculate the precision values as follows:\\npvanilla = 6\\n6\\npmod = 2\\n6\\nand we can see that the simple correction has produced a much more reasonable\\nvalue. Now let’s extend this by not only looking at single words, but n-grams as well.\\nLet’s assume we have one generated sentence, snt, that we want to compare against a\\nreference sentence, snt′. We extract all possible n-grams of degree n and do the\\naccounting to get the precision pn:\\npn =\\n∑n‐gram ∈ snt′ Countclip n‐gram\\n∑n‐gram ∈ snt Count n‐gram\\nIn order to avoid rewarding repetitive generations, the count in the numerator is clip‐\\nped. What this means is that the occurrence count of an n-gram is capped at how\\nmany times it appears in the reference sentence. Also note that the definition of a sen‐\\ntence is not very strict in this equation, and if you had a generated text spanning mul‐\\ntiple sentences you would treat it as one sentence.\\nIn general we have more than one sample in the test set we want to evaluate, so we\\nneed to slightly extend the equation by summing over all samples in the corpus C:\\npn =\\n∑snt ∈ C ∑n‐gram ∈ snt′ Countclip n‐gram\\n∑snt′ ∈ C ∑n‐gram ∈ snt Count n‐gram\\nWe’re almost there. Since we are not looking at recall, all generated sequences that are\\nshort but precise have a benefit compared to sentences that are longer. Therefore, the\\nprecision score favors short generations. To compensate for that the authors of BLEU\\nintroduced an additional term, the brevity penalty:\\nBR = min 1, e\\n1 − ℓre f /ℓgen\\nMeasuring the Quality of Generated Text | 149'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 173, 'page_label': '150'}, page_content='By taking the minimum, we ensure that this penalty never exceeds 1 and the expo‐\\nnential term becomes exponentially small when the length of the generated text lgen is\\nsmaller than the reference text lre f. At this point you might ask, why don’t we just use\\nsomething like an F1-score to account for recall as well? The answer is that often in\\ntranslation datasets there are multiple reference sentences instead of just one, so if we\\nalso measured recall we would incentivize translations that used all the words from all\\nthe references. Therefore, it’s preferable to look for high precision in the translation\\nand make sure the translation and reference have a similar length.\\nFinally, we can put everything together and get the equation for the BLEU score:\\nBLEU‐N = BR × ∏\\nn = 1\\nN\\npn\\n1/N\\nThe last term is the geometric mean of the modified precision up to n-gram N. In\\npractice, the BLEU-4 score is often reported. However, you can probably already see\\nthat this metric has many limitations; for instance, it doesn’t take synonyms into\\naccount, and many steps in the derivation seem like ad hoc and rather fragile heuris‐\\ntics. Y ou can find a wonderful exposition of BLEU’s flaws in Rachel Tatman’s blog\\npost “Evaluating Text Output in NLP: BLEU at Y our Own Risk”.\\nIn general, the field of text generation is still looking for better evaluation metrics,\\nand finding ways to overcome the limits of metrics like BLEU is an active area of\\nresearch. Another weakness of the BLEU metric is that it expects the text to already\\nbe tokenized. This can lead to varying results if the exact same method for text toke‐\\nnization is not used. The SacreBLEU metric addresses this issue by internalizing the\\ntokenization step; for this reason, it is the preferred metric for benchmarking.\\nWe’ve now worked through some theory, but what we really want to do is calculate\\nthe score for some generated text. Does that mean we need to implement all this logic\\nin Python? Fear not, \\n  Datasets also provides metrics! Loading a metric works just\\nlike loading a dataset:\\nfrom datasets import load_metric\\nbleu_metric = load_metric(\"sacrebleu\")\\nThe bleu_metric object is an instance of the Metric class, and works like an aggrega‐\\ntor: you can add single instances with add() or whole batches via add_batch(). Once\\nyou have added all the samples you need to evaluate, you then call compute() and the\\nmetric is calculated. This returns a dictionary with several values, such as the preci‐\\nsion for each n-gram, the length penalty, as well as the final BLEU score. Let’s look at\\nthe example from before:\\n150 | Chapter 6: Summarization'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 174, 'page_label': '151'}, page_content='import pandas as pd\\nimport numpy as np\\nbleu_metric.add(\\n    prediction=\"the the the the the the\", reference=[\"the cat is on the mat\"])\\nresults = bleu_metric.compute(smooth_method=\"floor\", smooth_value=0)\\nresults[\"precisions\"] = [np.round(p, 2) for p in results[\"precisions\"]]\\npd.DataFrame.from_dict(results, orient=\"index\", columns=[\"Value\"])\\nValue\\nscore 0.0\\ncounts [2, 0, 0, 0]\\ntotals [6, 5, 4, 3]\\nprecisions [33.33, 0.0, 0.0, 0.0]\\nbp 1.0\\nsys_len 6\\nref_len 6\\nThe BLEU score also works if there are multiple reference transla‐\\ntions. This is why reference is passed as a list. To make the metric\\nsmoother for zero counts in the n-grams, BLEU integrates methods\\nto modify the precision calculation. One method is to add a con‐\\nstant to the numerator. That way, a missing n-gram does not cause\\nthe score to automatically go to zero. For the purpose of explaining\\nthe values, we turn it off by setting smooth_value=0.\\nWe can see the precision of the 1-gram is indeed 2/6, whereas the precisions for the\\n2/3/4-grams are all 0. (For more information about the individual metrics, like counts\\nand bp, see the SacreBLEU repository.) This means the geometric mean is zero, and\\nthus also the BLEU score. Let’s look at another example where the prediction is\\nalmost correct:\\nbleu_metric.add(\\n    prediction=\"the cat is on mat\", reference=[\"the cat is on the mat\"])\\nresults = bleu_metric.compute(smooth_method=\"floor\", smooth_value=0)\\nresults[\"precisions\"] = [np.round(p, 2) for p in results[\"precisions\"]]\\npd.DataFrame.from_dict(results, orient=\"index\", columns=[\"Value\"])\\nValue\\nscore 57.893007\\ncounts [5, 3, 2, 1]\\ntotals [5, 4, 3, 2]\\nprecisions [100.0, 75.0, 66.67, 50.0]\\nMeasuring the Quality of Generated Text | 151'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 175, 'page_label': '152'}, page_content='5 C-Y . Lin, “ROUGE: A Package for Automatic Evaluation of Summaries, ” Text Summarization Branches Out\\n(July 2004), https://aclanthology.org/W04-1013.pdf.\\nValue\\nbp 0.818731\\nsys_len 5\\nref_len 6\\nWe observe that the precision scores are much better. The 1-grams in the prediction\\nall match, and only in the precision scores do we see that something is off. For the 4-\\ngram there are only two candidates, [\"the\", \"cat\", \"is\", \"on\"]  and [\"cat\",\\n\"is\", \"on\", \"mat\"], where the last one does not match, hence the precision of 0.5.\\nThe BLEU score is widely used for evaluating text, especially in machine translation,\\nsince precise translations are usually favored over translations that include all possible\\nand appropriate words.\\nThere are other applications, such as summarization, where the situation is different.\\nThere, we want all the important information in the generated text, so we favor high\\nrecall. This is where the ROUGE score is usually used.\\nROUGE\\nThe ROUGE score was specifically developed for applications like summarization\\nwhere high recall is more important than just precision.5 The approach is very similar\\nto the BLEU score in that we look at different n-grams and compare their occurrences\\nin the generated text and the reference texts. The difference is that with ROUGE we\\ncheck how many n-grams in the reference text also occur in the generated text. For\\nBLEU we looked at how many n-grams in the generated text appear in the reference,\\nso we can reuse the precision formula with the minor modification that we count\\nthe (unclipped) occurrence of reference n-grams in the generated text in the\\ndenominator:\\nROUGE‐N =\\n∑snt’ ∈ C ∑n‐gram ∈ snt′ Countmatch n‐gram\\n∑snt’ ∈ C ∑n‐gram ∈ snt′ Count n‐gram\\nThis was the original proposal for ROUGE. Subsequently, researchers have found that\\nfully removing precision can have strong negative effects. Going back to the BLEU\\nformula without the clipped counting, we can measure precision as well, and we can\\nthen combine both precision and recall ROUGE scores in the harmonic mean to get\\nan F1-score. This score is the metric that is nowadays commonly reported for\\nROUGE.\\n152 | Chapter 6: Summarization'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 176, 'page_label': '153'}, page_content='There is a separate score in ROUGE to measure the longest common substring (LCS),\\ncalled ROUGE-L. The LCS can be calculated for any pair of strings. For example, the\\nLCS for “abab” and “abc” would be “ab” , and its the length would be 2. If we want to\\ncompare this value between two samples we need to somehow normalize it because\\notherwise a longer text would be at an advantage. To achieve this, the inventor of\\nROUGE came up with an F-score-like scheme where the LCS is normalized with the\\nlength of the reference and generated text, then the two normalized scores are mixed\\ntogether:\\nRLCS = LCS X, Y\\nm\\nPLCS = LCS X, Y\\nn\\nFLCS =\\n1 + β2 RLCSPLCS\\nRLCS + βPLCS\\n, where β = PLCS/RLCS\\nThat way the LCS score is properly normalized and can be compared across samples.\\nIn the \\n  Datasets implementation, two variations of ROUGE are calculated: one cal‐\\nculates the score per sentence and averages it for the summaries (ROUGE-L), and the\\nother calculates it directly over the whole summary (ROUGE-Lsum).\\nWe can load the metric as follows:\\nrouge_metric = load_metric(\"rouge\")\\nWe already generated a set of summaries with GPT-2 and the other models, and now\\nwe have a metric to compare the summaries systematically. Let’s apply the ROUGE\\nscore to all the summaries generated by the models:\\nreference = dataset[\"train\"][1][\"highlights\"]\\nrecords = []\\nrouge_names = [\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"]\\nfor model_name in summaries:\\n    rouge_metric.add(prediction=summaries[model_name], reference=reference)\\n    score = rouge_metric.compute()\\n    rouge_dict = dict((rn, score[rn].mid.fmeasure) for rn in rouge_names)\\n    records.append(rouge_dict)\\npd.DataFrame.from_records(records, index=summaries.keys())\\nrouge1 rouge2 rougeL rougeLsum\\nbaseline 0.303571 0.090909 0.214286 0.232143\\ngpt2 0.187500 0.000000 0.125000 0.187500\\nMeasuring the Quality of Generated Text | 153'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 177, 'page_label': '154'}, page_content='rouge1 rouge2 rougeL rougeLsum\\nt5 0.486486 0.222222 0.378378 0.486486\\nbart 0.582278 0.207792 0.455696 0.506329\\npegasus 0.866667 0.655172 0.800000 0.833333\\nThe ROUGE metric in the \\n  Datasets library also calculates confi‐\\ndence intervals (by default, the 5th and 95th percentiles). The aver‐\\nage value is stored in the attribute mid and the interval can be\\nretrieved with low and high.\\nThese results are obviously not very reliable as we only looked at a single sample, but\\nwe can compare the quality of the summary for that one example. The table confirms\\nour observation that of the models we considered, GPT-2 performs worst. This is not\\nsurprising since it is the only model of the group that was not explicitly trained to\\nsummarize. It is striking, however, that the simple first-three-sentence baseline\\ndoesn’t fare too poorly compared to the transformer models that have on the order of\\na billion parameters! PEGASUS and BART are the best models overall (higher\\nROUGE scores are better), but T5 is slightly better on ROUGE-1 and the LCS scores.\\nThese results place T5 and PEGASUS as the best models, but again these results\\nshould be treated with caution as we only evaluated the models on a single example.\\nLooking at the results in the PEGASUS paper, we would expect the PEGASUS to out‐\\nperform T5 on the CNN/DailyMail dataset.\\nLet’s see if we can reproduce those results with PEGASUS.\\nEvaluating PEGASUS on the CNN/DailyMail Dataset\\nWe now have all the pieces in place to evaluate the model properly: we have a dataset\\nwith a test set from CNN/DailyMail, we have a metric with ROUGE, and we have a\\nsummarization model. We just need to put the pieces together. Let’s first evaluate the\\nperformance of the three-sentence baseline:\\ndef evaluate_summaries_baseline(dataset, metric,\\n                                column_text=\"article\",\\n                                column_summary=\"highlights\"):\\n    summaries = [three_sentence_summary(text) for text in dataset[column_text]]\\n    metric.add_batch(predictions=summaries,\\n                     references=dataset[column_summary])\\n    score = metric.compute()\\n    return score\\nNow we’ll apply the function to a subset of the data. Since the test fraction of the\\nCNN/DailyMail dataset consists of roughly 10,000 samples, generating summaries for\\nall these articles takes a lot of time. Recall from Chapter 5 that every generated token\\n154 | Chapter 6: Summarization'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 178, 'page_label': '155'}, page_content='requires a forward pass through the model; generating just 100 tokens for each sam‐\\nple will thus require 1 million forward passes, and if we use beam search this number\\nis multiplied by the number of beams. For the purpose of keeping the calculations rel‐\\natively fast, we’ll subsample the test set and run the evaluation on 1,000 samples\\ninstead. This should give us a much more stable score estimation while completing in\\nless than one hour on a single GPU for the PEGASUS model:\\ntest_sampled = dataset[\"test\"].shuffle(seed=42).select(range(1000))\\nscore = evaluate_summaries_baseline(test_sampled, rouge_metric)\\nrouge_dict = dict((rn, score[rn].mid.fmeasure) for rn in rouge_names)\\npd.DataFrame.from_dict(rouge_dict, orient=\"index\", columns=[\"baseline\"]).T\\nrouge1 rouge2 rougeL rougeLsum\\nbaseline 0.396061 0.173995 0.245815 0.361158\\nThe scores are mostly worse than on the previous example, but still better than those\\nachieved by GPT-2! Now let’s implement the same evaluation function for evaluating\\nthe PEGASUS model:\\nfrom tqdm import tqdm\\nimport torch\\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\\ndef chunks(list_of_elements, batch_size):\\n    \"\"\"Yield successive batch-sized chunks from list_of_elements.\"\"\"\\n    for i in range(0, len(list_of_elements), batch_size):\\n        yield list_of_elements[i : i + batch_size]\\ndef evaluate_summaries_pegasus(dataset, metric, model, tokenizer,\\n                               batch_size=16, device=device,\\n                               column_text=\"article\",\\n                               column_summary=\"highlights\"):\\n    article_batches = list(chunks(dataset[column_text], batch_size))\\n    target_batches = list(chunks(dataset[column_summary], batch_size))\\n    for article_batch, target_batch in tqdm(\\n        zip(article_batches, target_batches), total=len(article_batches)):\\n        inputs = tokenizer(article_batch, max_length=1024,  truncation=True,\\n                        padding=\"max_length\", return_tensors=\"pt\")\\n        summaries = model.generate(input_ids=inputs[\"input_ids\"].to(device),\\n                         attention_mask=inputs[\"attention_mask\"].to(device),\\n                         length_penalty=0.8, num_beams=8, max_length=128)\\n        decoded_summaries = [tokenizer.decode(s, skip_special_tokens=True,\\n                                clean_up_tokenization_spaces=True)\\n               for s in summaries]\\nEvaluating PEGASUS on the CNN/DailyMail Dataset | 155'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 179, 'page_label': '156'}, page_content='decoded_summaries = [d.replace(\"<n>\", \" \") for d in decoded_summaries]\\n        metric.add_batch(predictions=decoded_summaries, references=target_batch)\\n    score = metric.compute()\\n    return score\\nLet’s unpack this evaluation code a bit. First we split the dataset into smaller batches\\nthat we can process simultaneously. Then for each batch we tokenize the input arti‐\\ncles and feed them to the generate() function to produce the summaries using beam\\nsearch. We use the same generation parameters as proposed in the paper. The new\\nparameter for length penalty ensures that the model does not generate sequences that\\nare too long. Finally, we decode the generated texts, replace the <n> token, and add\\nthe decoded texts with the references to the metric. At the end, we compute and\\nreturn the ROUGE scores. Let’s now load the model again with the AutoModelFor\\nSeq2SeqLM class, used for seq2seq generation tasks, and evaluate it:\\nfrom transformers import AutoModelForSeq2SeqLM, AutoTokenizer\\nmodel_ckpt = \"google/pegasus-cnn_dailymail\"\\ntokenizer = AutoTokenizer.from_pretrained(model_ckpt)\\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_ckpt).to(device)\\nscore = evaluate_summaries_pegasus(test_sampled, rouge_metric,\\n                                   model, tokenizer, batch_size=8)\\nrouge_dict = dict((rn, score[rn].mid.fmeasure) for rn in rouge_names)\\npd.DataFrame(rouge_dict, index=[\"pegasus\"])\\nrouge1 rouge2 rougeL rougeLsum\\npegasus 0.434381 0.210883 0.307195 0.373231\\nThese numbers are very close to the published results. One thing to note here is that\\nthe loss and per-token accuracy are decoupled to some degree from the ROUGE\\nscores. The loss is independent of the decoding strategy, whereas the ROUGE score is\\nstrongly coupled.\\nSince ROUGE and BLEU correlate better with human judgment than loss or accu‐\\nracy, we should focus on them and carefully explore and choose the decoding strategy\\nwhen building text generation models. These metrics are far from perfect, however,\\nand one should always consider human judgments as well.\\nNow that we’re equipped with an evaluation function, it’s time to train our own\\nmodel for summarization.\\n156 | Chapter 6: Summarization'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 180, 'page_label': '157'}, page_content='Training a Summarization Model\\nWe’ve worked through a lot of details on text summarization and evaluation, so let’s\\nput this to use to train a custom text summarization model! For our application, we’ll\\nuse the SAMSum dataset developed by Samsung, which consists of a collection of dia‐\\nlogues along with brief summaries. In an enterprise setting, these dialogues might\\nrepresent the interactions between a customer and the support center, so generating\\naccurate summaries can help improve customer service and detect common patterns\\namong customer requests. Let’s load it and look at an example:\\ndataset_samsum = load_dataset(\"samsum\")\\nsplit_lengths = [len(dataset_samsum[split])for split in dataset_samsum]\\nprint(f\"Split lengths: {split_lengths}\")\\nprint(f\"Features: {dataset_samsum[\\'train\\'].column_names}\")\\nprint(\"\\\\nDialogue:\")\\nprint(dataset_samsum[\"test\"][0][\"dialogue\"])\\nprint(\"\\\\nSummary:\")\\nprint(dataset_samsum[\"test\"][0][\"summary\"])\\nSplit lengths: [14732, 819, 818]\\nFeatures: [\\'id\\', \\'dialogue\\', \\'summary\\']\\nDialogue:\\nHannah: Hey, do you have Betty\\'s number?\\nAmanda: Lemme check\\nHannah: <file_gif>\\nAmanda: Sorry, can\\'t find it.\\nAmanda: Ask Larry\\nAmanda: He called her last time we were at the park together\\nHannah: I don\\'t know him well\\nHannah: <file_gif>\\nAmanda: Don\\'t be shy, he\\'s very nice\\nHannah: If you say so..\\nHannah: I\\'d rather you texted him\\nAmanda: Just text him \\nHannah: Urgh.. Alright\\nHannah: Bye\\nAmanda: Bye bye\\nSummary:\\nHannah needs Betty\\'s number but Amanda doesn\\'t have it. She needs to contact\\nLarry.\\nThe dialogues look like what you would expect from a chat via SMS or WhatsApp,\\nincluding emojis and placeholders for GIFs. The dialogue field contains the full text\\nand the summary the summarized dialogue. Could a model that was fine-tuned on the\\nCNN/DailyMail dataset deal with that? Let’s find out!\\nTraining a Summarization Model | 157'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 181, 'page_label': '158'}, page_content='Evaluating PEGASUS on SAMSum\\nFirst we’ll run the same summarization pipeline with PEGASUS to see what the out‐\\nput looks like. We can reuse the code we used for the CNN/DailyMail summary\\ngeneration:\\npipe_out = pipe(dataset_samsum[\"test\"][0][\"dialogue\"])\\nprint(\"Summary:\")\\nprint(pipe_out[0][\"summary_text\"].replace(\" .<n>\", \".\\\\n\"))\\nSummary:\\nAmanda: Ask Larry Amanda: He called her last time we were at the park together.\\nHannah: I\\'d rather you texted him.\\nAmanda: Just text him .\\nWe can see that the model mostly tries to summarize by extracting the key sentences\\nfrom the dialogue. This probably worked relatively well on the CNN/DailyMail data‐\\nset, but the summaries in SAMSum are more abstract. Let’s confirm this by running\\nthe full ROUGE evaluation on the test set:\\nscore = evaluate_summaries_pegasus(dataset_samsum[\"test\"], rouge_metric, model,\\n                                   tokenizer, column_text=\"dialogue\",\\n                                   column_summary=\"summary\", batch_size=8)\\nrouge_dict = dict((rn, score[rn].mid.fmeasure) for rn in rouge_names)\\npd.DataFrame(rouge_dict, index=[\"pegasus\"])\\nrouge1 rouge2 rougeL rougeLsum\\npegasus 0.296168 0.087803 0.229604 0.229514\\nWell, the results aren’t great, but this is not unexpected since we’ve moved quite a bit\\naway from the CNN/DailyMail data distribution. Nevertheless, setting up the evalua‐\\ntion pipeline before training has two advantages: we can directly measure the success\\nof training with the metric and we have a good baseline. Fine-tuning the model on\\nour dataset should result in an immediate improvement in the ROUGE metric, and if\\nthat is not the case we’ll know something is wrong with our training loop.\\nFine-Tuning PEGASUS\\nBefore we process the data for training, let’s have a quick look at the length distribu‐\\ntion of the input and outputs:\\nd_len = [len(tokenizer.encode(s)) for s in dataset_samsum[\"train\"][\"dialogue\"]]\\ns_len = [len(tokenizer.encode(s)) for s in dataset_samsum[\"train\"][\"summary\"]]\\nfig, axes = plt.subplots(1, 2, figsize=(10, 3.5), sharey=True)\\naxes[0].hist(d_len, bins=20, color=\"C0\", edgecolor=\"C0\")\\naxes[0].set_title(\"Dialogue Token Length\")\\naxes[0].set_xlabel(\"Length\")\\n158 | Chapter 6: Summarization'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 182, 'page_label': '159'}, page_content='axes[0].set_ylabel(\"Count\")\\naxes[1].hist(s_len, bins=20, color=\"C0\", edgecolor=\"C0\")\\naxes[1].set_title(\"Summary Token Length\")\\naxes[1].set_xlabel(\"Length\")\\nplt.tight_layout()\\nplt.show()\\nWe see that most dialogues are much shorter than the CNN/DailyMail articles, with\\n100–200 tokens per dialogue. Similarly, the summaries are much shorter, with around\\n20–40 tokens (the average length of a tweet).\\nLet’s keep those observations in mind as we build the data collator for the Trainer.\\nFirst we need to tokenize the dataset. For now, we’ll set the maximum lengths to 1024\\nand 128 for the dialogues and summaries, respectively:\\ndef convert_examples_to_features(example_batch):\\n    input_encodings = tokenizer(example_batch[\"dialogue\"], max_length=1024,\\n                                truncation=True)\\nTraining a Summarization Model | 159'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 183, 'page_label': '160'}, page_content='with tokenizer.as_target_tokenizer():\\n        target_encodings = tokenizer(example_batch[\"summary\"], max_length=128,\\n                                     truncation=True)\\n    return {\"input_ids\": input_encodings[\"input_ids\"],\\n            \"attention_mask\": input_encodings[\"attention_mask\"],\\n            \"labels\": target_encodings[\"input_ids\"]}\\ndataset_samsum_pt = dataset_samsum.map(convert_examples_to_features,\\n                                       batched=True)\\ncolumns = [\"input_ids\", \"labels\", \"attention_mask\"]\\ndataset_samsum_pt.set_format(type=\"torch\", columns=columns)\\nA new thing in the use of the tokenization step is the tokenizer.as_target_token\\nizer() context. Some models require special tokens in the decoder inputs, so it’s\\nimportant to differentiate between the tokenization of encoder and decoder inputs. In\\nthe with statement (called a context manager), the tokenizer knows that it is tokeniz‐\\ning for the decoder and can process sequences accordingly.\\nNow, we need to create the data collator. This function is called in the Trainer just\\nbefore the batch is fed through the model. In most cases we can use the default colla‐\\ntor, which collects all the tensors from the batch and simply stacks them. For the\\nsummarization task we need to not only stack the inputs but also prepare the targets\\non the decoder side. PEGASUS is an encoder-decoder transformer and thus has the\\nclassic seq2seq architecture. In a seq2seq setup, a common approach is to apply\\n“teacher forcing” in the decoder. With this strategy, the decoder receives input tokens\\n(like in decoder-only models such as GPT-2) that consists of the labels shifted by one\\nin addition to the encoder output; so, when making the prediction for the next token\\nthe decoder gets the ground truth shifted by one as an input, as illustrated in the fol‐\\nlowing table:\\ndecoder_input label\\nstep\\n1 [PAD] Transformers\\n2 [PAD, Transformers] are\\n3 [PAD, Transformers, are] awesome\\n4 [PAD, Transformers, are, awesome] for\\n5 [PAD, Transformers, are, awesome, for] text\\n6 [PAD, Transformers, are, awesome, for, text] summarization\\nWe shift it by one so that the decoder only sees the previous ground truth labels and\\nnot the current or future ones. Shifting alone suffices since the decoder has masked\\nself-attention that masks all inputs at present and in the future.\\n160 | Chapter 6: Summarization'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 184, 'page_label': '161'}, page_content='So, when we prepare our batch, we set up the decoder inputs by shifting the labels to\\nthe right by one. After that, we make sure the padding tokens in the labels are ignored\\nby the loss function by setting them to –100. We actually don’t have to do this man‐\\nually, though, since the DataCollatorForSeq2Seq comes to the rescue and takes care\\nof all these steps for us:\\nfrom transformers import DataCollatorForSeq2Seq\\nseq2seq_data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\\nThen, as usual, we set up a the TrainingArguments for training:\\nfrom transformers import TrainingArguments, Trainer\\ntraining_args = TrainingArguments(\\n    output_dir=\\'pegasus-samsum\\', num_train_epochs=1, warmup_steps=500,\\n    per_device_train_batch_size=1, per_device_eval_batch_size=1,\\n    weight_decay=0.01, logging_steps=10, push_to_hub=True,\\n    evaluation_strategy=\\'steps\\', eval_steps=500, save_steps=1e6,\\n    gradient_accumulation_steps=16)\\nOne thing that is different from the previous settings is that new argument,\\ngradient_accumulation_steps. Since the model is quite big, we had to set the batch\\nsize to 1. However, a batch size that is too small can hurt convergence. To resolve that\\nissue, we can use a nifty technique called gradient accumulation. As the name sug‐\\ngests, instead of calculating the gradients of the full batch all at once, we make smaller\\nbatches and aggregate the gradients. When we have aggregated enough gradients, we\\nrun the optimization step. Naturally this is a bit slower than doing it in one pass, but\\nit saves us a lot of GPU memory.\\nLet’s now make sure that we are logged in to Hugging Face so we can push the model\\nto the Hub after training:\\nfrom huggingface_hub import notebook_login\\nnotebook_login()\\nWe have now everything we need to initialize the trainer with the model, tokenizer,\\ntraining arguments, and data collator, as well as the training and evaluation sets:\\ntrainer = Trainer(model=model, args=training_args,\\n                  tokenizer=tokenizer, data_collator=seq2seq_data_collator,\\n                  train_dataset=dataset_samsum_pt[\"train\"],\\n                  eval_dataset=dataset_samsum_pt[\"validation\"])\\nWe are ready for training. After training, we can directly run the evaluation function\\non the test set to see how well the model performs:\\ntrainer.train()\\nscore = evaluate_summaries_pegasus(\\n    dataset_samsum[\"test\"], rouge_metric, trainer.model, tokenizer,\\n    batch_size=2, column_text=\"dialogue\", column_summary=\"summary\")\\nTraining a Summarization Model | 161'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 185, 'page_label': '162'}, page_content='rouge_dict = dict((rn, score[rn].mid.fmeasure) for rn in rouge_names)\\npd.DataFrame(rouge_dict, index=[f\"pegasus\"])\\nrouge1 rouge2 rougeL rougeLsum\\npegasus 0.427614 0.200571 0.340648 0.340738\\nWe see that the ROUGE scores improved considerably over the model without fine-\\ntuning, so even though the previous model was also trained for summarization, it was\\nnot well adapted for the new domain. Let’s push our model to the Hub:\\ntrainer.push_to_hub(\"Training complete!\")\\nIn the next section we’ll use the model to generate a few summaries for us.\\nY ou can also evaluate the generations as part of the training loop:\\nuse the extension of TrainingArguments called Seq2SeqTraining\\nArguments and specify predict_with_generate=True. Pass it to\\nthe dedicated Trainer called Seq2SeqTrainer, which then uses the\\ngenerate() function instead of the model’s forward pass to create\\npredictions for evaluation. Give it a try!\\nGenerating Dialogue Summaries\\nLooking at the losses and ROUGE scores, it seems the model is showing a significant\\nimprovement over the original model trained on CNN/DailyMail only. Let’s see what\\na summary generated on a sample from the test set looks like:\\ngen_kwargs = {\"length_penalty\": 0.8, \"num_beams\":8, \"max_length\": 128}\\nsample_text = dataset_samsum[\"test\"][0][\"dialogue\"]\\nreference = dataset_samsum[\"test\"][0][\"summary\"]\\npipe = pipeline(\"summarization\", model=\"transformersbook/pegasus-samsum\")\\nprint(\"Dialogue:\")\\nprint(sample_text)\\nprint(\"\\\\nReference Summary:\")\\nprint(reference)\\nprint(\"\\\\nModel Summary:\")\\nprint(pipe(sample_text, **gen_kwargs)[0][\"summary_text\"])\\nDialogue:\\nHannah: Hey, do you have Betty\\'s number?\\nAmanda: Lemme check\\nHannah: <file_gif>\\nAmanda: Sorry, can\\'t find it.\\nAmanda: Ask Larry\\nAmanda: He called her last time we were at the park together\\nHannah: I don\\'t know him well\\nHannah: <file_gif>\\n162 | Chapter 6: Summarization'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 186, 'page_label': '163'}, page_content='Amanda: Don\\'t be shy, he\\'s very nice\\nHannah: If you say so..\\nHannah: I\\'d rather you texted him\\nAmanda: Just text him \\nHannah: Urgh.. Alright\\nHannah: Bye\\nAmanda: Bye bye\\nReference Summary:\\nHannah needs Betty\\'s number but Amanda doesn\\'t have it. She needs to contact\\nLarry.\\nModel Summary:\\nAmanda can\\'t find Betty\\'s number. Larry called Betty last time they were at the\\npark together. Hannah wants Amanda to text Larry instead of calling Betty.\\nThat looks much more like the reference summary. It seems the model has learned to\\nsynthesize the dialogue into a summary without just extracting passages. Now, the\\nultimate test: how well does the model work on a custom input?\\ncustom_dialogue = \"\"\"\\\\\\nThom: Hi guys, have you heard of transformers?\\nLewis: Yes, I used them recently!\\nLeandro: Indeed, there is a great library by Hugging Face.\\nThom: I know, I helped build it ;)\\nLewis: Cool, maybe we should write a book about it. What do you think?\\nLeandro: Great idea, how hard can it be?!\\nThom: I am in!\\nLewis: Awesome, let\\'s do it together!\\n\"\"\"\\nprint(pipe(custom_dialogue, **gen_kwargs)[0][\"summary_text\"])\\nThom, Lewis and Leandro are going to write a book about transformers. Thom\\nhelped build a library by Hugging Face. They are going to do it together.\\nThe generated summary of the custom dialogue makes sense. It summarizes well that\\nall the people in the discussion want to write the book together and does not simply\\nextract single sentences. For example, it synthesizes the third and fourth lines into a\\nlogical combination.\\nConclusion\\nText summarization poses some unique challenges compared to other tasks that can\\nbe framed as classification tasks, like sentiment analysis, named entity recognition, or\\nquestion answering. Conventional metrics such as accuracy do not reflect the quality\\nof the generated text. As we saw, the BLEU and ROUGE metrics can better evaluate\\ngenerated texts; however, human judgment remains the best measure.\\nA common question when working with summarization models is how we can sum‐\\nmarize documents where the texts are longer than the model’s context length.\\nConclusion | 163'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 187, 'page_label': '164'}, page_content='6 J. Wu et al., “Recursively Summarizing Books with Human Feedback”, (2021).\\nUnfortunately, there is no single strategy to solve this problem, and to date this is still\\nan open and active research question. For example, recent work by OpenAI showed\\nhow to scale summarization by applying it recursively to long documents and using\\nhuman feedback in the loop.6\\nIn the next chapter we’ll look at question answering, which is the task of providing an\\nanswer to a question based on a text passage. In contrast to summarization, with this\\ntask there exist good strategies to deal with long or many documents, and we’ll show\\nyou how to scale question answering to thousands of documents.\\n164 | Chapter 6: Summarization'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 188, 'page_label': '165'}, page_content='CHAPTER 7\\nQuestion Answering\\nWhether you’re a researcher, analyst, or data scientist, chances are that at some point\\nyou’ve needed to wade through oceans of documents to find the information you’re\\nlooking for. To make matters worse, you’re constantly reminded by Google and Bing\\nthat there exist better ways to search! For instance, if we search for “When did Marie\\nCurie win her first Nobel Prize?” on Google, we immediately get the correct answer\\nof “1903, ” as illustrated in Figure 7-1.\\nFigure 7-1. A Google search query and corresponding answer snippet\\n165'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 189, 'page_label': '166'}, page_content='1 Although, in this particular case, everyone agrees that Drop C is the best guitar tuning.\\nIn this example, Google first retrieved around 319,000 documents that were relevant\\nto the query, and then performed an additional processing step to extract the answer\\nsnippet with the corresponding passage and web page. It’s not hard to see why these\\nanswer snippets are useful. For example, if we search for a trickier question like\\n“Which guitar tuning is the best?” Google doesn’t provide an answer, and instead we\\nhave to click on one of the web pages returned by the search engine to find it\\nourselves.1\\nThe general approach behind this technology is called question answering  (QA).\\nThere are many flavors of QA, but the most common is extractive QA, which involves\\nquestions whose answer can be identified as a span of text in a document, where the\\ndocument might be a web page, legal contract, or news article. The two-stage process\\nof first retrieving relevant documents and then extracting answers from them is also\\nthe basis for many modern QA systems, including semantic search engines, intelli‐\\ngent assistants, and automated information extractors. In this chapter, we’ll apply this\\nprocess to tackle a common problem facing ecommerce websites: helping consumers\\nanswer specific queries to evaluate a product. We’ll see that customer reviews can be\\nused as a rich and challenging source of information for QA, and along the way we’ll\\nlearn how transformers act as powerful reading comprehension models that can\\nextract meaning from text. Let’s begin by fleshing out the use case.\\nThis chapter focuses on extractive QA, but other forms of QA may\\nbe more suitable for your use case. For example, community QA\\ninvolves gathering question-answer pairs that are generated by\\nusers on forums like Stack Overflow, and then using semantic sim‐\\nilarity search to find the closest matching answer to a new ques‐\\ntion. There is also long-form QA, which aims to generate complex\\nparagraph-length answers to open-ended questions like “Why is\\nthe sky blue?” Remarkably, it is also possible to do QA over tables,\\nand transformer models like TAPAS can even perform aggrega‐\\ntions to produce the final answer!\\nBuilding a Review-Based QA System\\nIf you’ve ever purchased a product online, you probably relied on customer reviews\\nto help inform your decision. These reviews can often help answer specific questions\\nlike “Does this guitar come with a strap?” or “Can I use this camera at night?” that\\nmay be hard to answer from the product description alone. However, popular\\nproducts can have hundreds to thousands of reviews, so it can be a major drag to find\\none that is relevant. One alternative is to post your question on the community QA\\n166 | Chapter 7: Question Answering'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 190, 'page_label': '167'}, page_content='2 J. Bjerva et al., “SubjQA: A Dataset for Subjectivity and Review Comprehension”, (2020).\\n3 As we’ll soon see, there are also unanswerable questions that are designed to produce more robust models.\\nplatforms provided by websites like Amazon, but it usually takes days to get an\\nanswer (if you get one at all). Wouldn’t it be nice if we could get an immediate answer,\\nlike in the Google example from Figure 7-1 ? Let’s see if we can do this using\\ntransformers!\\nThe Dataset\\nTo build our QA system we’ll use the SubjQA dataset, 2 which consists of more than\\n10,000 customer reviews in English about products and services in six domains: Trip‐\\nAdvisor, Restaurants, Movies, Books, Electronics, and Grocery. As illustrated in\\nFigure 7-2, each review is associated with a question that can be answered using one\\nor more sentences from the review.3\\nFigure 7-2. A question about a product and the corresponding review (the answer span\\nis underlined)\\nThe interesting aspect of this dataset is that most of the questions and answers are\\nsubjective; that is, they depend on the personal experience of the users. The example\\nin Figure 7-2 shows why this feature makes the task potentially more difficult than\\nBuilding a Review-Based QA System | 167'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 191, 'page_label': '168'}, page_content='finding answers to factual questions like “What is the currency of the United King‐\\ndom?” First, the query is about “poor quality, ” which is subjective and depends on the\\nuser’s definition of quality. Second, important parts of the query do not appear in the\\nreview at all, which means it cannot be answered with shortcuts like keyword search\\nor paraphrasing the input question. These features make SubjQA a realistic dataset to\\nbenchmark our review-based QA models on, since user-generated content like that\\nshown in Figure 7-2 resembles what we might encounter in the wild.\\nQA systems are usually categorized by the domain of data that they\\nhave access to when responding to a query. Closed-domain QA\\ndeals with questions about a narrow topic (e.g., a single product\\ncategory), while open-domain QA deals with questions about\\nalmost anything (e.g., Amazon’s whole product catalog). In general,\\nclosed-domain QA involves searching through fewer documents\\nthan the open-domain case.\\nTo get started, let’s download the dataset from the Hugging Face Hub. As we did in\\nChapter 4, we can use the get_dataset_config_names() function to find out which\\nsubsets are available:\\nfrom datasets import get_dataset_config_names\\ndomains = get_dataset_config_names(\"subjqa\")\\ndomains\\n[\\'books\\', \\'electronics\\', \\'grocery\\', \\'movies\\', \\'restaurants\\', \\'tripadvisor\\']\\nFor our use case, we’ll focus on building a QA system for the Electronics domain. To\\ndownload the electronics subset, we just need to pass this value to the name argu‐\\nment of the load_dataset() function:\\nfrom datasets import load_dataset\\nsubjqa = load_dataset(\"subjqa\", name=\"electronics\")\\nLike other question answering datasets on the Hub, SubjQA stores the answers to\\neach question as a nested dictionary. For example, if we inspect one of the rows in the\\nanswers column:\\nprint(subjqa[\"train\"][\"answers\"][1])\\n{\\'text\\': [\\'Bass is weak as expected\\', \\'Bass is weak as expected, even with EQ\\nadjusted up\\'], \\'answer_start\\': [1302, 1302], \\'answer_subj_level\\': [1, 1],\\n\\'ans_subj_score\\': [0.5083333253860474, 0.5083333253860474], \\'is_ans_subjective\\':\\n[True, True]}\\nwe can see that the answers are stored in a text field, while the starting character\\nindices are provided in answer_start. To explore the dataset more easily, we’ll flatten\\n168 | Chapter 7: Question Answering'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 192, 'page_label': '169'}, page_content='4 D. Hendrycks et al., “CUAD: An Expert-Annotated NLP Dataset for Legal Contract Review”, (2021).\\nthese nested columns with the flatten() method and convert each split to a Pandas\\nDataFrame as follows:\\nimport pandas as pd\\ndfs = {split: dset.to_pandas() for split, dset in subjqa.flatten().items()}\\nfor split, df in dfs.items():\\n    print(f\"Number of questions in {split}: {df[\\'id\\'].nunique()}\")\\nNumber of questions in train: 1295\\nNumber of questions in test: 358\\nNumber of questions in validation: 255\\nNotice that the dataset is relatively small, with only 1,908 examples in total. This sim‐\\nulates a real-world scenario, since getting domain experts to label extractive QA data‐\\nsets is labor-intensive and expensive. For example, the CUAD dataset for extractive\\nQA on legal contracts is estimated to have a value of $2 million to account for the\\nlegal expertise needed to annotate its 13,000 examples!4\\nThere are quite a few columns in the SubjQA dataset, but the most interesting ones\\nfor building our QA system are shown in Table 7-1.\\nTable 7-1. Column names and their descriptions from the SubjQA dataset\\nColumn name Description\\ntitle The Amazon Standard Identification Number (ASIN) associated with each product\\nquestion The question\\nanswers.answer_text The span of text in the review labeled by the annotator\\nanswers.answer_start The start character index of the answer span\\ncontext The customer review\\nLet’s focus on these columns and take a look at a few of the training examples. We can\\nuse the sample() method to select a random sample:\\nqa_cols = [\"title\", \"question\", \"answers.text\",\\n           \"answers.answer_start\", \"context\"]\\nsample_df = dfs[\"train\"][qa_cols].sample(2, random_state=7)\\nsample_df\\nBuilding a Review-Based QA System | 169'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 193, 'page_label': '170'}, page_content='title question answers.text answers.answer_start context\\nB005DKZTMG Does the\\nkeyboard\\nlightweight?\\n[this keyboard\\nis compact]\\n[215] I really like this keyboard. I give it 4 stars\\nbecause it doesn’t have a CAPS LOCK key so I\\nnever know if my caps are on. But for the price,\\nit really suffices as a wireless keyboard. I have\\nvery large hands and this keyboard is compact,\\nbut I have no complaints.\\nB00AAIPT76 How is the\\nbattery?\\n[] [] I bought this after the first spare gopro battery I\\nbought wouldn’t hold a charge. I have very\\nrealistic expectations of this sort of product, I am\\nskeptical of amazing stories of charge time and\\nbattery life but I do expect the batteries to hold\\na charge for a couple of weeks at least and for\\nthe charger to work like a charger. In this I was\\nnot disappointed. I am a river rafter and found\\nthat the gopro burns through power in a hurry\\nso this purchase solved that issue. the batteries\\nheld a charge, on shorter trips the extra two\\nbatteries were enough and on longer trips I\\ncould use my friends JOOS Orange to recharge\\nthem.I just bought a newtrent xtreme powerpak\\nand expect to be able to charge these with that\\nso I will not run out of power again.\\nFrom these examples we can make a few observations. First, the questions are not\\ngrammatically correct, which is quite common in the FAQ sections of ecommerce\\nwebsites. Second, an empty answers.text entry denotes “unanswerable” questions\\nwhose answer cannot be found in the review. Finally, we can use the start index and\\nlength of the answer span to slice out the span of text in the review that corresponds\\nto the answer:\\nstart_idx = sample_df[\"answers.answer_start\"].iloc[0][0]\\nend_idx = start_idx + len(sample_df[\"answers.text\"].iloc[0][0])\\nsample_df[\"context\"].iloc[0][start_idx:end_idx]\\n\\'this keyboard is compact\\'\\nNext, let’s get a feel for what types of questions are in the training set by counting the\\nquestions that begin with a few common starting words:\\ncounts = {}\\nquestion_types = [\"What\", \"How\", \"Is\", \"Does\", \"Do\", \"Was\", \"Where\", \"Why\"]\\nfor q in question_types:\\n    counts[q] = dfs[\"train\"][\"question\"].str.startswith(q).value_counts()[True]\\npd.Series(counts).sort_values().plot.barh()\\nplt.title(\"Frequency of Question Types\")\\nplt.show()\\n170 | Chapter 7: Question Answering'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 194, 'page_label': '171'}, page_content='5 P . Rajpurkar et al., “SQuAD: 100,000+ Questions for Machine Comprehension of Text”, (2016).\\nWe can see that questions beginning with “How” , “What” , and “Is” are the most com‐\\nmon ones, so let’s have a look at some examples:\\nfor question_type in [\"How\", \"What\", \"Is\"]:\\n    for question in (\\n        dfs[\"train\"][dfs[\"train\"].question.str.startswith(question_type)]\\n        .sample(n=3, random_state=42)[\\'question\\']):\\n        print(question)\\nHow is the camera?\\nHow do you like the control?\\nHow fast is the charger?\\nWhat is direction?\\nWhat is the quality of the construction of the bag?\\nWhat is your impression of the product?\\nIs this how zoom works?\\nIs sound clear?\\nIs it a wireless keyboard?\\nThe Stanford Question Answering Dataset\\nThe (question, review, [answer sentences])  format of SubjQA is commonly used in\\nextractive QA datasets, and was pioneered in the Stanford Question Answering Data‐\\nset (SQuAD). 5 This is a famous dataset that is often used to test the ability of\\nmachines to read a passage of text and answer questions about it. The dataset was cre‐\\nated by sampling several hundred English articles from Wikipedia, partitioning each\\narticle into paragraphs, and then asking crowdworkers to generate a set of questions\\nBuilding a Review-Based QA System | 171'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 195, 'page_label': '172'}, page_content='6 P . Rajpurkar, R. Jia, and P . Liang, “Know What Y ou Don’t Know: Unanswerable Questions for SQuAD”,\\n(2018).\\n7 T. Kwiatkowski et al., “Natural Questions: A Benchmark for Question Answering Research, ” Transactions of\\nthe Association for Computational Linguistics 7 (March 2019): 452–466, http://dx.doi.org/10.1162/\\ntacl_a_00276.\\nand answers for each paragraph. In the first version of SQuAD, each answer to a\\nquestion was guaranteed to exist in the corresponding passage. But it wasn’t long\\nbefore sequence models started performing better than humans at extracting the cor‐\\nrect span of text with the answer. To make the task more difficult, SQuAD 2.0 was\\ncreated by augmenting SQuAD 1.1 with a set of adversarial questions that are relevant\\nto a given passage but cannot be answered from the text alone. 6 The state of the art as\\nof this book’s writing is shown in Figure 7-3, with most models since 2019 surpassing\\nhuman performance.\\nFigure 7-3. Progress on the SQuAD 2.0 benchmark (image from Papers with Code)\\nHowever, this superhuman performance does not appear to reflect genuine reading\\ncomprehension, since answers to the “unanswerable” questions can usually be identi‐\\nfied through patterns in the passages like antonyms. To address these problems Goo‐\\ngle released the Natural Questions (NQ) dataset, 7 which involves fact-seeking\\nquestions obtained from Google Search users. The answers in NQ are much longer\\nthan in SQuAD and present a more challenging benchmark.\\nNow that we’ve explored our dataset a bit, let’s dive into understanding how trans‐\\nformers can extract answers from text.\\n172 | Chapter 7: Question Answering'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 196, 'page_label': '173'}, page_content='Extracting Answers from Text\\nThe first thing we’ll need for our QA system is to find a way to identify a potential\\nanswer as a span of text in a customer review. For example, if a we have a question\\nlike “Is it waterproof?” and the review passage is “This watch is waterproof at 30m\\ndepth” , then the model should output “waterproof at 30m” . To do this we’ll need to\\nunderstand how to:\\n• Frame the supervised learning problem.\\n• Tokenize and encode text for QA tasks.\\n• Deal with long passages that exceed a model’s maximum context size.\\nLet’s start by taking a look at how to frame the problem.\\nSpan classification\\nThe most common way to extract answers from text is by framing the problem as a\\nspan classification task, where the start and end tokens of an answer span act as the\\nlabels that a model needs to predict. This process is illustrated in Figure 7-4.\\nFigure 7-4. The span classification head for QA tasks\\nSince our training set is relatively small, with only 1,295 examples, a good strategy is\\nto start with a language model that has already been fine-tuned on a large-scale QA\\ndataset like SQuAD. In general, these models have strong reading comprehension\\ncapabilities and serve as a good baseline upon which to build a more accurate system.\\nThis is a somewhat different approach to that taken in previous chapters, where we\\nBuilding a Review-Based QA System | 173'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 197, 'page_label': '174'}, page_content='typically started with a pretrained model and fine-tuned the task-specific head our‐\\nselves. For example, in Chapter 2, we had to fine-tune the classification head because\\nthe number of classes was tied to the dataset at hand. For extractive QA, we can\\nactually start with a fine-tuned model since the structure of the labels remains the\\nsame across datasets.\\nY ou can find a list of extractive QA models by navigating to the Hugging Face Hub\\nand searching for “squad” on the Models tab (Figure 7-5).\\nFigure 7-5. A selection of extractive QA models on the Hugging Face Hub\\nAs you can see, at the time of writing, there are more than 350 QA models to choose\\nfrom—so which one should you pick? In general, the answer depends on various fac‐\\ntors like whether your corpus is mono- or multilingual and the constraints of run‐\\nning the model in a production environment. Table 7-2  lists a few models that\\nprovide a good foundation to build on.\\nTable 7-2. Baseline transformer models that are fine-tuned on SQuAD 2.0\\nTransformer Description Number of\\nparameters\\nF1-score on\\nSQuAD 2.0\\nMiniLM A distilled version of BERT-base that preserves 99% of the performance\\nwhile being twice as fast\\n66M 79.5\\nRoBERTa-base RoBERTa models have better performance than their BERT counterparts\\nand can be fine-tuned on most QA datasets using a single GPU\\n125M 83.0\\nALBERT-XXL State-of-the-art performance on SQuAD 2.0, but computationally\\nintensive and difficult to deploy\\n235M 88.1\\nXLM-RoBERTa-\\nlarge\\nMultilingual model for 100 languages with strong zero-shot\\nperformance\\n570M 83.8\\n174 | Chapter 7: Question Answering'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 198, 'page_label': '175'}, page_content='8 W . Wang et al., “MINILM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained\\nTransformers”, (2020).\\n9 Note that the token_type_ids are not present in all transformer models. In the case of BERT-like models\\nsuch as MiniLM, the token_type_ids are also used during pretraining to incorporate the next sentence\\nprediction task.\\nFor the purposes of this chapter, we’ll use a fine-tuned MiniLM model since it is fast\\nto train and will allow us to quickly iterate on the techniques that we’ll be exploring. 8\\nAs usual, the first thing we need is a tokenizer to encode our texts, so let’s take a look\\nat how this works for QA tasks.\\nTokenizing text for QA\\nTo encode our texts, we’ll load the MiniLM model checkpoint from the Hugging Face\\nHub as usual:\\nfrom transformers import AutoTokenizer\\nmodel_ckpt = \"deepset/minilm-uncased-squad2\"\\ntokenizer = AutoTokenizer.from_pretrained(model_ckpt)\\nTo see the model in action, let’s first try to extract an answer from a short passage of\\ntext. In extractive QA tasks, the inputs are provided as (question, context) pairs, so we\\npass them both to the tokenizer as follows:\\nquestion = \"How much music can this hold?\"\\ncontext = \"\"\"An MP3 is about 1 MB/minute, so about 6000 hours depending on \\\\\\nfile size.\"\"\"\\ninputs = tokenizer(question, context, return_tensors=\"pt\")\\nHere we’ve returned PyTorch Tensor objects, since we’ll need them to run the for‐\\nward pass through the model. If we view the tokenized inputs as a table:\\ninput_ids 101 2129 2172 2189 2064 2023 ... 5834 2006 5371 2946 1012 102\\ntoken_type_ids 0 0 0 0 0 0 ... 1 1 1 1 1 1\\nattention_mask 1 1 1 1 1 1 ... 1 1 1 1 1 1\\nwe can see the familiar input_ids and attention_mask tensors, while the\\ntoken_type_ids tensor indicates which part of the inputs corresponds to the ques‐\\ntion and context (a 0 indicates a question token, a 1 indicates a context token).9\\nTo understand how the tokenizer formats the inputs for QA tasks, let’s decode the\\ninput_ids tensor:\\nprint(tokenizer.decode(inputs[\"input_ids\"][0]))\\nBuilding a Review-Based QA System | 175'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 199, 'page_label': '176'}, page_content='10 See Chapter 2 for details on how these hidden states can be extracted.\\n[CLS] how much music can this hold? [SEP] an mp3 is about 1 mb / minute, so\\nabout 6000 hours depending on file size. [SEP]\\nWe see that for each QA example, the inputs take the format:\\n[CLS] question tokens [SEP] context tokens [SEP]\\nwhere the location of the first [SEP] token is determined by the token_type_ids.\\nNow that our text is tokenized, we just need to instantiate the model with a QA head\\nand run the inputs through the forward pass:\\nimport torch\\nfrom transformers import AutoModelForQuestionAnswering\\nmodel = AutoModelForQuestionAnswering.from_pretrained(model_ckpt)\\nwith torch.no_grad():\\n    outputs = model(**inputs)\\nprint(outputs)\\nQuestionAnsweringModelOutput(loss=None, start_logits=tensor([[-0.9862, -4.7750,\\n         -5.4025, -5.2378, -5.2863, -5.5117, -4.9819, -6.1880,\\n         -0.9862,  0.2596, -0.2144, -1.7136,  3.7806,  4.8561, -1.0546, -3.9097,\\n         -1.7374, -4.5944, -1.4278,  3.9949,  5.0390, -0.2018, -3.0193, -4.8549,\\n         -2.3107, -3.5110, -3.5713, -0.9862]]), end_logits=tensor([[-0.9623,\\n         -5.4733, -5.0326, -5.1639, -5.4278, -5.5151, -5.1749, -4.6233,\\n         -0.9623, -3.7855, -0.8715, -3.7745, -3.0161, -1.1780,  0.1758, -2.7365,\\n          4.8934,  0.3046, -3.1761, -3.2762,  0.8937,  5.6606, -0.3623, -4.9554,\\n         -3.2531, -0.0914,  1.6211, -0.9623]]), hidden_states=None,\\nattentions=None)\\nHere we can see that we get a QuestionAnsweringModelOutput object as the output of\\nthe QA head. As illustrated in Figure 7-4, the QA head corresponds to a linear layer\\nthat takes the hidden states from the encoder and computes the logits for the start\\nand end spans.10 This means that we treat QA as a form of token classification, similar\\nto what we encountered for named entity recognition in Chapter 4. To convert the\\noutputs into an answer span, we first need to get the logits for the start and end\\ntokens:\\nstart_logits = outputs.start_logits\\nend_logits = outputs.end_logits\\nIf we compare the shapes of these logits to the input IDs:\\nprint(f\"Input IDs shape: {inputs.input_ids.size()}\")\\nprint(f\"Start logits shape: {start_logits.size()}\")\\nprint(f\"End logits shape: {end_logits.size()}\")\\n176 | Chapter 7: Question Answering'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 200, 'page_label': '177'}, page_content='Input IDs shape: torch.Size([1, 28])\\nStart logits shape: torch.Size([1, 28])\\nEnd logits shape: torch.Size([1, 28])\\nwe see that there are two logits (a start and end) associated with each input token. As\\nillustrated in Figure 7-6, larger, positive logits correspond to more likely candidates\\nfor the start and end tokens. In this example we can see that the model assigns the\\nhighest start token logits to the numbers “1” and “6000” , which makes sense since our\\nquestion is asking about a quantity. Similarly, we see that the end tokens with the\\nhighest logits are “minute” and “hours” .\\nFigure 7-6. Predicted logits for the start and end tokens; the token with the highest score\\nis colored in orange\\nTo get the final answer, we can compute the argmax over the start and end token log‐\\nits and then slice the span from the inputs. The following code performs these steps\\nand decodes the result so we can print the resulting text:\\nimport torch\\nstart_idx = torch.argmax(start_logits)\\nend_idx = torch.argmax(end_logits) + 1\\nanswer_span = inputs[\"input_ids\"][0][start_idx:end_idx]\\nanswer = tokenizer.decode(answer_span)\\nprint(f\"Question: {question}\")\\nprint(f\"Answer: {answer}\")\\nQuestion: How much music can this hold?\\nAnswer: 6000 hours\\nBuilding a Review-Based QA System | 177'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 201, 'page_label': '178'}, page_content='Great, it worked! In \\n  Transformers, all of these preprocessing and postprocessing\\nsteps are conveniently wrapped in a dedicated pipeline. We can instantiate the pipe‐\\nline by passing our tokenizer and fine-tuned model as follows:\\nfrom transformers import pipeline\\npipe = pipeline(\"question-answering\", model=model, tokenizer=tokenizer)\\npipe(question=question, context=context, topk=3)\\n[{\\'score\\': 0.26516005396842957,\\n  \\'start\\': 38,\\n  \\'end\\': 48,\\n  \\'answer\\': \\'6000 hours\\'},\\n {\\'score\\': 0.2208300083875656,\\n  \\'start\\': 16,\\n  \\'end\\': 48,\\n  \\'answer\\': \\'1 MB/minute, so about 6000 hours\\'},\\n {\\'score\\': 0.10253632068634033,\\n  \\'start\\': 16,\\n  \\'end\\': 27,\\n  \\'answer\\': \\'1 MB/minute\\'}]\\nIn addition to the answer, the pipeline also returns the model’s probability estimate in\\nthe score field (obtained by taking a softmax over the logits). This is handy when we\\nwant to compare multiple answers within a single context. We’ve also shown that we\\ncan have the model predict multiple answers by specifying the topk parameter. Some‐\\ntimes, it is possible to have questions for which no answer is possible, like the empty\\nanswers.answer_start examples in SubjQA. In these cases the model will assign a\\nhigh start and end score to the [CLS] token, and the pipeline maps this output to an\\nempty string:\\npipe(question=\"Why is there no data?\", context=context,\\n     handle_impossible_answer=True)\\n{\\'score\\': 0.9068416357040405, \\'start\\': 0, \\'end\\': 0, \\'answer\\': \\'\\'}\\nIn our simple example, we obtained the start and end indices by\\ntaking the argmax of the corresponding logits. However, this heu‐\\nristic can produce out-of-scope answers by selecting tokens that\\nbelong to the question instead of the context. In practice, the pipe‐\\nline computes the best combination of start and end indices subject\\nto various constraints such as being in-scope, requiring the start\\nindices to precede the end indices, and so on.\\n178 | Chapter 7: Question Answering'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 202, 'page_label': '179'}, page_content='Dealing with long passages\\nOne subtlety faced by reading comprehension models is that the context often con‐\\ntains more tokens than the maximum sequence length of the model (which is usually\\na few hundred tokens at most). As illustrated in Figure 7-7, a decent portion of the\\nSubjQA training set contains question-context pairs that won’t fit within MiniLM’s\\ncontext size of 512 tokens.\\nFigure 7-7. Distribution of tokens for each question-context pair in the SubjQA training\\nset\\nFor other tasks, like text classification, we simply truncated long texts under the\\nassumption that enough information was contained in the embedding of the [CLS]\\ntoken to generate accurate predictions. For QA, however, this strategy is problematic\\nbecause the answer to a question could lie near the end of the context and thus would\\nbe removed by truncation. As illustrated in Figure 7-8, the standard way to deal with\\nthis is to apply a sliding window across the inputs, where each window contains a pas‐\\nsage of tokens that fit in the model’s context.\\nBuilding a Review-Based QA System | 179'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 203, 'page_label': '180'}, page_content='Figure 7-8. How the sliding window creates multiple question-context pairs for long\\ndocuments—the first bar corresponds to the question, while the second bar is the context\\ncaptured in each window\\nIn \\n  Transformers, we can set return_overflowing_tokens=True in the tokenizer to\\nenable the sliding window. The size of the sliding window is controlled by the\\nmax_seq_length argument, and the size of the stride is controlled by doc_stride.\\nLet’s grab the first example from our training set and define a small window to illus‐\\ntrate how this works:\\nexample = dfs[\"train\"].iloc[0][[\"question\", \"context\"]]\\ntokenized_example = tokenizer(example[\"question\"], example[\"context\"],\\n                              return_overflowing_tokens=True, max_length=100,\\n                              stride=25)\\nIn this case we now get a list of input_ids, one for each window. Let’s check the num‐\\nber of tokens we have in each window:\\nfor idx, window in enumerate(tokenized_example[\"input_ids\"]):\\n    print(f\"Window #{idx} has {len(window)} tokens\")\\nWindow #0 has 100 tokens\\nWindow #1 has 88 tokens\\nFinally, we can see where two windows overlap by decoding the inputs:\\nfor window in tokenized_example[\"input_ids\"]:\\n    print(f\"{tokenizer.decode(window)} \\\\n\")\\n[CLS] how is the bass? [SEP] i have had koss headphones in the past, pro 4aa and\\nqz - 99. the koss portapro is portable and has great bass response. the work\\ngreat with my android phone and can be \" rolled up \" to be carried in my\\nmotorcycle jacket or computer bag without getting crunched. they are very light\\nand don\\'t feel heavy or bear down on your ears even after listening to music\\nwith them on all day. the sound is [SEP]\\n[CLS] how is the bass? [SEP] and don\\'t feel heavy or bear down on your ears even\\n180 | Chapter 7: Question Answering'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 204, 'page_label': '181'}, page_content='11 A vector is sparse if most of its elements are zero.\\nafter listening to music with them on all day. the sound is night and day better\\nthan any ear - bud could be and are almost as good as the pro 4aa. they are \"\\nopen air \" headphones so you cannot match the bass to the sealed types, but it\\ncomes close. for $ 32, you cannot go wrong. [SEP]\\nNow that we have some intuition about how QA models can extract answers from\\ntext, let’s look at the other components we need to build an end-to-end QA pipeline.\\nUsing Haystack to Build a QA Pipeline\\nIn our simple answer extraction example, we provided both the question and the con‐\\ntext to the model. However, in reality our system’s users will only provide a question\\nabout a product, so we need some way of selecting relevant passages from among all\\nthe reviews in our corpus. One way to do this would be to concatenate all the reviews\\nof a given product together and feed them to the model as a single, long context.\\nAlthough simple, the drawback of this approach is that the context can become\\nextremely long and thereby introduce an unacceptable latency for our users’ queries.\\nFor example, let’s suppose that on average, each product has 30 reviews and each\\nreview takes 100 milliseconds to process. If we need to process all the reviews to get\\nan answer, this would result in an average latency of 3 seconds per user query—much\\ntoo long for ecommerce websites!\\nTo handle this, modern QA systems are typically based on the retriever-reader archi‐\\ntecture, which has two main components:\\nRetriever\\nResponsible for retrieving relevant documents for a given query. Retrievers are\\nusually categorized as sparse or dense. Sparse retrievers use word frequencies to\\nrepresent each document and query as a sparse vector.11 The relevance of a query\\nand a document is then determined by computing an inner product of the vec‐\\ntors. On the other hand, dense retrievers use encoders like transformers to repre‐\\nsent the query and document as contextualized embeddings (which are dense\\nvectors). These embeddings encode semantic meaning, and allow dense retriev‐\\ners to improve search accuracy by understanding the content of the query.\\nReader\\nResponsible for extracting an answer from the documents provided by the\\nretriever. The reader is usually a reading comprehension model, although at the\\nend of the chapter we’ll see examples of models that can generate free-form\\nanswers.\\nBuilding a Review-Based QA System | 181'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 205, 'page_label': '182'}, page_content='As illustrated in Figure 7-9 , there can also be other components that apply post-\\nprocessing to the documents fetched by the retriever or to the answers extracted by\\nthe reader. For example, the retrieved documents may need reranking to eliminate\\nnoisy or irrelevant ones that can confuse the reader. Similarly, postprocessing of the\\nreader’s answers is often needed when the correct answer comes from various pas‐\\nsages in a long document.\\nFigure 7-9. The retriever-reader architecture for modern QA systems\\nTo build our QA system, we’ll use the Haystack library developed by deepset, a Ger‐\\nman company focused on NLP . Haystack is based on the retriever-reader architec‐\\nture, abstracts much of the complexity involved in building these systems, and\\nintegrates tightly with \\n  Transformers.\\nIn addition to the retriever and reader, there are two more components involved\\nwhen building a QA pipeline with Haystack:\\nDocument store\\nA document-oriented database that stores documents and metadata which are\\nprovided to the retriever at query time\\nPipeline\\nCombines all the components of a QA system to enable custom query flows,\\nmerging documents from multiple retrievers, and more\\nIn this section we’ll look at how we can use these components to quickly build a pro‐\\ntotype QA pipeline. Later, we’ll examine how we can improve its performance.\\n182 | Chapter 7: Question Answering'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 206, 'page_label': '183'}, page_content='12 The guide also provides installation instructions for macOS and Windows.\\nThis chapter was written using version 0.9.0 of the Haystack\\nlibrary. In version 0.10.0 , the pipeline and evaluation APIs were\\nredesigned to make it easier to inspect whether the retriever or\\nreader are impacting performance. To see what this chapter’s code\\nlooks like with the new API, check out the GitHub repository.\\nInitializing a document store\\nIn Haystack, there are various document stores to choose from and each one can be\\npaired with a dedicated set of retrievers. This is illustrated in Table 7-3, where the\\ncompatibility of sparse (TF-IDF , BM25) and dense (Embedding, DPR) retrievers is\\nshown for each of the available document stores. We’ll explain what all these acro‐\\nnyms mean later in this chapter.\\nTable 7-3. Compatibility of Haystack retrievers and document stores\\nIn memory Elasticsearch FAISS Milvus\\nTF-IDF Yes Yes No No\\nBM25 No Yes No No\\nEmbedding Yes Yes Yes Yes\\nDPR Yes Yes Yes Yes\\nSince we’ll be exploring both sparse and dense retrievers in this chapter, we’ll use the\\nElasticsearchDocumentStore, which is compatible with both retriever types. Elastic‐\\nsearch is a search engine that is capable of handling a diverse range of data types,\\nincluding textual, numerical, geospatial, structured, and unstructured. Its ability to\\nstore huge volumes of data and quickly filter it with full-text search features makes it\\nespecially well suited for developing QA systems. It also has the advantage of being\\nthe industry standard for infrastructure analytics, so there’s a good chance your com‐\\npany already has a cluster that you can work with.\\nTo initialize the document store, we first need to download and install Elasticsearch.\\nBy following Elasticsearch’s guide,12 we can grab the latest release for Linux with wget\\nand unpack it with the tar shell command:\\nurl = \"\"\"https://artifacts.elastic.co/downloads/elasticsearch/\\\\\\nelasticsearch-7.9.2-linux-x86_64.tar.gz\"\"\"\\n!wget -nc -q {url}\\n!tar -xzf elasticsearch-7.9.2-linux-x86_64.tar.gz\\nNext we need to start the Elasticsearch server. Since we’re running all the code in this\\nbook within Jupyter notebooks, we’ll need to use Python’s Popen() function to spawn\\nBuilding a Review-Based QA System | 183'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 207, 'page_label': '184'}, page_content='a new process. While we’re at it, let’s also run the subprocess in the background using\\nthe chown shell command:\\nimport os\\nfrom subprocess import Popen, PIPE, STDOUT\\n# Run Elasticsearch as a background process\\n!chown -R daemon:daemon elasticsearch-7.9.2\\nes_server = Popen(args=[\\'elasticsearch-7.9.2/bin/elasticsearch\\'],\\n                  stdout=PIPE, stderr=STDOUT, preexec_fn=lambda: os.setuid(1))\\n# Wait until Elasticsearch has started\\n!sleep 30\\nIn the Popen() function, the args specify the program we wish to execute, while\\nstdout=PIPE creates a new pipe for the standard output and stderr=STDOUT collects\\nthe errors in the same pipe. The preexec_fn argument specifies the ID of the subpro‐\\ncess we wish to use. By default, Elasticsearch runs locally on port 9200, so we can test\\nthe connection by sending an HTTP request to localhost:\\n!curl -X GET \"localhost:9200/?pretty\"\\n{\\n  \"name\" : \"96938eee37cd\",\\n  \"cluster_name\" : \"docker-cluster\",\\n  \"cluster_uuid\" : \"ABGDdvbbRWmMb9Umz79HbA\",\\n  \"version\" : {\\n    \"number\" : \"7.9.2\",\\n    \"build_flavor\" : \"default\",\\n    \"build_type\" : \"docker\",\\n    \"build_hash\" : \"d34da0ea4a966c4e49417f2da2f244e3e97b4e6e\",\\n    \"build_date\" : \"2020-09-23T00:45:33.626720Z\",\\n    \"build_snapshot\" : false,\\n    \"lucene_version\" : \"8.6.2\",\\n    \"minimum_wire_compatibility_version\" : \"6.8.0\",\\n    \"minimum_index_compatibility_version\" : \"6.0.0-beta1\"\\n  },\\n  \"tagline\" : \"You Know, for Search\"\\n}\\nNow that our Elasticsearch server is up and running, the next thing to do is instanti‐\\nate the document store:\\nfrom haystack.document_store.elasticsearch import ElasticsearchDocumentStore\\n# Return the document embedding for later use with dense retriever\\ndocument_store = ElasticsearchDocumentStore(return_embedding=True)\\nBy default, ElasticsearchDocumentStore creates two indices on Elasticsearch: one\\ncalled document for (you guessed it) storing documents, and another called label for\\nstoring the annotated answer spans. For now, we’ll just populate the document index\\n184 | Chapter 7: Question Answering'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 208, 'page_label': '185'}, page_content='13 For an in-depth explanation of document scoring with TF-IDF and BM25 see Chapter 23 of Speech and Lan‐\\nguage Processing, 3rd edition, by D. Jurafsky and J.H. Martin (Prentice Hall).\\nwith the SubjQA reviews, and Haystack’s document stores expect a list of dictionaries\\nwith text and meta keys as follows:\\n{\\n    \"text\": \"<the-context>\",\\n    \"meta\": {\\n        \"field_01\": \"<additional-metadata>\",\\n        \"field_02\": \"<additional-metadata>\",\\n        ...\\n    }\\n}\\nThe fields in meta can be used for applying filters during retrieval. For our purposes\\nwe’ll include the item_id and q_review_id columns of SubjQA so we can filter by\\nproduct and question ID, along with the corresponding training split. We can then\\nloop through the examples in each DataFrame and add them to the index with the\\nwrite_documents() method as follows:\\nfor split, df in dfs.items():\\n    # Exclude duplicate reviews\\n    docs = [{\"text\": row[\"context\"],\\n             \"meta\":{\"item_id\": row[\"title\"], \"question_id\": row[\"id\"],\\n                     \"split\": split}}\\n        for _,row in df.drop_duplicates(subset=\"context\").iterrows()]\\n    document_store.write_documents(docs, index=\"document\")\\nprint(f\"Loaded {document_store.get_document_count()} documents\")\\nLoaded 1615 documents\\nGreat, we’ve loaded all our reviews into an index! To search the index we’ll need a\\nretriever, so let’s look at how we can initialize one for Elasticsearch.\\nInitializing a retriever\\nThe Elasticsearch document store can be paired with any of the Haystack retrievers,\\nso let’s start by using a sparse retriever based on BM25 (short for “Best Match 25”).\\nBM25 is an improved version of the classic Term Frequency-Inverse Document Fre‐\\nquency (TF-IDF) algorithm and represents the question and context as sparse vectors\\nthat can be searched efficiently on Elasticsearch. The BM25 score measures how\\nmuch matched text is about a search query and improves on TF-IDF by saturating TF\\nvalues quickly and normalizing the document length so that short documents are\\nfavored over long ones.13\\nBuilding a Review-Based QA System | 185'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 209, 'page_label': '186'}, page_content='In Haystack, the BM25 retriever is used by default in ElasticsearchRetriever, so\\nlet’s initialize this class by specifying the document store we wish to search over:\\nfrom haystack.retriever.sparse import ElasticsearchRetriever\\nes_retriever = ElasticsearchRetriever(document_store=document_store)\\nNext, let’s look at a simple query for a single electronics product in the training set.\\nFor review-based QA systems like ours, it’s important to restrict the queries to a single\\nitem because otherwise the retriever would source reviews about products that are\\nnot related to a user’s query. For example, asking “Is the camera quality any good?”\\nwithout a product filter could return reviews about phones, when the user might be\\nasking about a specific laptop camera instead. By themselves, the ASIN values in our\\ndataset are a bit cryptic, but we can decipher them with online tools like amazon\\nASIN or by simply appending the value of item_id to the www.amazon.com/dp/ URL.\\nThe following item ID corresponds to one of Amazon’s Fire tablets, so let’s use the\\nretriever’s retrieve() method to ask if it’s any good for reading with:\\nitem_id = \"B0074BW614\"\\nquery = \"Is it good for reading?\"\\nretrieved_docs = es_retriever.retrieve(\\n    query=query, top_k=3, filters={\"item_id\":[item_id], \"split\":[\"train\"]})\\nHere we’ve specified how many documents to return with the top_k argument and\\napplied a filter on both the item_id and split keys that were included in the meta\\nfield of our documents. Each element of retrieved_docs is a Haystack Document\\nobject that is used to represent documents and includes the retriever’s query score\\nalong with other metadata. Let’s have a look at one of the retrieved documents:\\nprint(retrieved_docs[0])\\n{\\'text\\': \\'This is a gift to myself.  I have been a kindle user for 4 years and\\nthis is my third one.  I never thought I would want a fire for I mainly use it\\nfor book reading.  I decided to try the fire for when I travel I take my laptop,\\nmy phone and my iPod classic.  I love my iPod but watching movies on the plane\\nwith it can be challenging because it is so small. Laptops battery life is not\\nas good as the Kindle.  So the Fire combines for me what I needed all three to\\ndo. So far so good.\\', \\'score\\': 6.243799, \\'probability\\': 0.6857824513476455,\\n\\'question\\': None, \\'meta\\': {\\'item_id\\': \\'B0074BW614\\', \\'question_id\\':\\n\\'868e311275e26dbafe5af70774a300f3\\', \\'split\\': \\'train\\'}, \\'embedding\\': None, \\'id\\':\\n\\'252e83e25d52df7311d597dc89eef9f6\\'}\\nIn addition to the document’s text, we can see the score that Elasticsearch computed\\nfor its relevance to the query (larger scores imply a better match). Under the hood,\\nElasticsearch relies on Lucene for indexing and search, so by default it uses Lucene’s\\npractical scoring function . Y ou can find the nitty-gritty details behind the scoring\\nfunction in the Elasticsearch documentation, but in brief terms it first filters the can‐\\ndidate documents by applying a Boolean test (does the document match the query?),\\n186 | Chapter 7: Question Answering'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 210, 'page_label': '187'}, page_content='and then applies a similarity metric that’s based on representing both the document\\nand the query as vectors.\\nNow that we have a way to retrieve relevant documents, the next thing we need is a\\nway to extract answers from them. This is where the reader comes in, so let’s take a\\nlook at how we can load our MiniLM model in Haystack.\\nInitializing a reader\\nIn Haystack, there are two types of readers one can use to extract answers from a\\ngiven context:\\nFARMReader\\nBased on deepset’s FARM framework for fine-tuning and deploying transform‐\\ners. Compatible with models trained using \\n  Transformers and can load models\\ndirectly from the Hugging Face Hub.\\nTransformersReader\\nBased on the QA pipeline from \\n  Transformers. Suitable for running inference\\nonly.\\nAlthough both readers handle a model’s weights in the same way, there are some dif‐\\nferences in the way the predictions are converted to produce answers:\\n• In \\n  Transformers, the QA pipeline normalizes the start and end logits with a\\nsoftmax in each passage. This means that it is only meaningful to compare\\nanswer scores between answers extracted from the same passage, where the prob‐\\nabilities sum to 1. For example, an answer score of 0.9 from one passage is not\\nnecessarily better than a score of 0.8 in another. In FARM, the logits are not nor‐\\nmalized, so inter-passage answers can be compared more easily.\\n• The TransformersReader sometimes predicts the same answer twice, but with\\ndifferent scores. This can happen in long contexts if the answer lies across two\\noverlapping windows. In FARM, these duplicates are removed.\\nSince we will be fine-tuning the reader later in the chapter, we’ll use the FARMReader.\\nAs with \\n  Transformers, to load the model we just need to specify the MiniLM\\ncheckpoint on the Hugging Face Hub along with some QA-specific arguments:\\nfrom haystack.reader.farm import FARMReader\\nmodel_ckpt = \"deepset/minilm-uncased-squad2\"\\nmax_seq_length, doc_stride = 384, 128\\nreader = FARMReader(model_name_or_path=model_ckpt, progress_bar=False,\\n                    max_seq_len=max_seq_length, doc_stride=doc_stride,\\n                    return_no_answer=True)\\nBuilding a Review-Based QA System | 187'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 211, 'page_label': '188'}, page_content='It is also possible to fine-tune a reading comprehension model\\ndirectly in \\n  Transformers and then load it in Transformers\\nReader to run inference. For details on how to do the fine-tuning\\nstep, see the question answering tutorial in the library’s\\ndocumentation.\\nIn FARMReader, the behavior of the sliding window is controlled by the same\\nmax_seq_length and doc_stride arguments that we saw for the tokenizer. Here we’ve\\nused the values from the MiniLM paper. To confirm, let’s now test the reader on our\\nsimple example from earlier:\\nprint(reader.predict_on_texts(question=question, texts=[context], top_k=1))\\n{\\'query\\': \\'How much music can this hold?\\', \\'no_ans_gap\\': 12.648084878921509,\\n\\'answers\\': [{\\'answer\\': \\'6000 hours\\', \\'score\\': 10.69961929321289, \\'probability\\':\\n0.3988136053085327, \\'context\\': \\'An MP3 is about 1 MB/minute, so about 6000 hours\\ndepending on file size.\\', \\'offset_start\\': 38, \\'offset_end\\': 48,\\n\\'offset_start_in_doc\\': 38, \\'offset_end_in_doc\\': 48, \\'document_id\\':\\n\\'e344757014e804eff50faa3ecf1c9c75\\'}]}\\nGreat, the reader appears to be working as expected—so next, let’s tie together all our\\ncomponents using one of Haystack’s pipelines.\\nPutting it all together\\nHaystack provides a Pipeline abstraction that allows us to combine retrievers, read‐\\ners, and other components together as a graph that can be easily customized for each\\nuse case. There are also predefined pipelines analogous to those in \\n  Transformers,\\nbut specialized for QA systems. In our case, we’re interested in extracting answers, so\\nwe’ll use the ExtractiveQAPipeline, which takes a single retriever-reader pair as its\\narguments:\\nfrom haystack.pipeline import ExtractiveQAPipeline\\npipe = ExtractiveQAPipeline(reader, es_retriever)\\nEach Pipeline has a run() method that specifies how the query flow should be exe‐\\ncuted. For the ExtractiveQAPipeline we just need to pass the query, the number of\\ndocuments to retrieve with top_k_retriever, and the number of answers to extract\\nfrom these documents with top_k_reader. In our case, we also need to specify a filter\\nover the item ID, which can be done using the filters argument as we did with the\\nretriever earlier. Let’s run a simple example using our question about the Amazon\\nFire tablet again, but this time returning the extracted answers:\\nn_answers = 3\\npreds = pipe.run(query=query, top_k_retriever=3, top_k_reader=n_answers,\\n                 filters={\"item_id\": [item_id], \"split\":[\"train\"]})\\nprint(f\"Question: {preds[\\'query\\']} \\\\n\")\\n188 | Chapter 7: Question Answering'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 212, 'page_label': '189'}, page_content='for idx in range(n_answers):\\n    print(f\"Answer {idx+1}: {preds[\\'answers\\'][idx][\\'answer\\']}\")\\n    print(f\"Review snippet: ...{preds[\\'answers\\'][idx][\\'context\\']}...\")\\n    print(\"\\\\n\\\\n\")\\nQuestion: Is it good for reading?\\nAnswer 1: I mainly use it for book reading\\nReview snippet: ... is my third one.  I never thought I would want a fire for I\\nmainly use it for book reading.  I decided to try the fire for when I travel I\\ntake my la...\\nAnswer 2: the larger screen compared to the Kindle makes for easier reading\\nReview snippet: ...ght enough that I can hold it to read, but the larger screen\\ncompared to the Kindle makes for easier reading. I love the color, something I\\nnever thou...\\nAnswer 3: it is great for reading books when no light is available\\nReview snippet: ...ecoming addicted to hers! Our son LOVES it and it is great\\nfor reading books when no light is available. Amazing sound but I suggest good\\nheadphones t...\\nGreat, we now have an end-to-end QA system for Amazon product reviews! This is a\\ngood start, but notice that the second and third answers are closer to what the ques‐\\ntion is actually asking. To do better, we’ll need some metrics to quantify the perfor‐\\nmance of the retriever and reader. We’ll take a look at that next.\\nImproving Our QA Pipeline\\nAlthough much of the recent research on QA has focused on improving reading com‐\\nprehension models, in practice it doesn’t matter how good your reader is if the\\nretriever can’t find the relevant documents in the first place! In particular, the\\nretriever sets an upper bound on the performance of the whole QA system, so it’s\\nimportant to make sure it’s doing a good job. With this in mind, let’s start by intro‐\\nducing some common metrics to evaluate the retriever so that we can compare the\\nperformance of sparse and dense representations.\\nEvaluating the Retriever\\nA common metric for evaluating retrievers is recall, which measures the fraction of all\\nrelevant documents that are retrieved. In this context, “relevant” simply means\\nwhether the answer is present in a passage of text or not, so given a set of questions,\\nwe can compute recall by counting the number of times an answer appears in the top\\nk documents returned by the retriever.\\nImproving Our QA Pipeline | 189'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 213, 'page_label': '190'}, page_content='In Haystack, there are two ways to evaluate retrievers:\\n• Use the retriever’s in-built eval() method. This can be used for both open- and\\nclosed-domain QA, but not for datasets like SubjQA where each document is\\npaired with a single product and we need to filter by product ID for every query.\\n• Build a custom Pipeline that combines a retriever with the EvalRetriever class.\\nThis enables the implementation of custom metrics and query flows.\\nA complementary metric to recall is mean average precision (mAP),\\nwhich rewards retrievers that can place the correct answers higher\\nup in the document ranking.\\nSince we need to evaluate the recall per product and then aggregate across all prod‐\\nucts, we’ll opt for the second approach. Each node in the Pipeline graph represents a\\nclass that takes some inputs and produces some outputs via a run() method:\\nclass PipelineNode:\\n    def __init__(self):\\n        self.outgoing_edges = 1\\n    def run(self, **kwargs):\\n        ...\\n        return (outputs, \"outgoing_edge_name\")\\nHere kwargs corresponds to the outputs from the previous node in the graph, which\\nis manipulated within the run() method to return a tuple of the outputs for the next\\nnode, along with a name for the outgoing edge. The only other requirement is to\\ninclude an outgoing_edges attribute that indicates the number of outputs from the\\nnode (in most cases outgoing_edges=1, unless you have branches in the pipeline that\\nroute the inputs according to some criterion).\\nIn our case, we need a node to evaluate the retriever, so we’ll use the EvalRetriever\\nclass whose run() method keeps track of which documents have answers that match\\nthe ground truth. With this class we can then build up a Pipeline graph by adding\\nthe evaluation node after a node that represents the retriever itself:\\nfrom haystack.pipeline import Pipeline\\nfrom haystack.eval import EvalDocuments\\nclass EvalRetrieverPipeline:\\n    def __init__(self, retriever):\\n        self.retriever = retriever\\n        self.eval_retriever = EvalDocuments()\\n        pipe = Pipeline()\\n        pipe.add_node(component=self.retriever, name=\"ESRetriever\",\\n190 | Chapter 7: Question Answering'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 214, 'page_label': '191'}, page_content='inputs=[\"Query\"])\\n        pipe.add_node(component=self.eval_retriever, name=\"EvalRetriever\",\\n                      inputs=[\"ESRetriever\"])\\n        self.pipeline = pipe\\npipe = EvalRetrieverPipeline(es_retriever)\\nNotice that each node is given a name and a list of inputs. In most cases, each node\\nhas a single outgoing edge, so we just need to include the name of the previous node\\nin inputs.\\nNow that we have our evaluation pipeline, we need to pass some queries and their\\ncorresponding answers. To do this, we’ll add the answers to a dedicated label index\\non our document store. Haystack provides a Label object that represents the answer\\nspans and their metadata in a standardized fashion. To populate the label index,\\nwe’ll first create a list of Label objects by looping over each question in the test set\\nand extracting the matching answers and additional metadata:\\nfrom haystack import Label\\nlabels = []\\nfor i, row in dfs[\"test\"].iterrows():\\n    # Metadata used for filtering in the Retriever\\n    meta = {\"item_id\": row[\"title\"], \"question_id\": row[\"id\"]}\\n    # Populate labels for questions with answers\\n    if len(row[\"answers.text\"]):\\n        for answer in row[\"answers.text\"]:\\n            label = Label(\\n                question=row[\"question\"], answer=answer, id=i, origin=row[\"id\"],\\n                meta=meta, is_correct_answer=True, is_correct_document=True,\\n                no_answer=False)\\n            labels.append(label)\\n    # Populate labels for questions without answers\\n    else:\\n        label = Label(\\n            question=row[\"question\"], answer=\"\", id=i, origin=row[\"id\"],\\n            meta=meta, is_correct_answer=True, is_correct_document=True,\\n            no_answer=True)\\n        labels.append(label)\\nIf we peek at one of these labels:\\nprint(labels[0])\\n{\\'id\\': \\'e28f5e62-85e8-41b2-8a34-fbff63b7a466\\', \\'created_at\\': None, \\'updated_at\\':\\nNone, \\'question\\': \\'What is the tonal balance of these headphones?\\', \\'answer\\': \\'I\\nhave been a headphone fanatic for thirty years\\', \\'is_correct_answer\\': True,\\n\\'is_correct_document\\': True, \\'origin\\': \\'d0781d13200014aa25860e44da9d5ea7\\',\\n\\'document_id\\': None, \\'offset_start_in_doc\\': None, \\'no_answer\\': False,\\n\\'model_id\\': None, \\'meta\\': {\\'item_id\\': \\'B00001WRSJ\\', \\'question_id\\':\\n\\'d0781d13200014aa25860e44da9d5ea7\\'}}\\nImproving Our QA Pipeline | 191'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 215, 'page_label': '192'}, page_content='we can see the question-answer pair, along with an origin field that contains the\\nunique question ID so we can filter the document store per question. We’ve also\\nadded the product ID to the meta field so we can filter the labels by product. Now that\\nwe have our labels, we can write them to the label index on Elasticsearch as follows:\\ndocument_store.write_labels(labels, index=\"label\")\\nprint(f\"\"\"Loaded {document_store.get_label_count(index=\"label\")} \\\\\\nquestion-answer pairs\"\"\")\\nLoaded 358 question-answer pairs\\nNext, we need to build up a mapping between our question IDs and corresponding\\nanswers that we can pass to the pipeline. To get all the labels, we can use the\\nget_all_labels_aggregated() method from the document store that will aggregate\\nall question-answer pairs associated with a unique ID. This method returns a list of\\nMultiLabel objects, but in our case we only get one element since we’re filtering by\\nquestion ID. We can build up a list of aggregated labels as follows:\\nlabels_agg = document_store.get_all_labels_aggregated(\\n    index=\"label\",\\n    open_domain=True,\\n    aggregate_by_meta=[\"item_id\"]\\n)\\nprint(len(labels_agg))\\n330\\nBy peeking at one of these labels we can see that all the answers associated with a\\ngiven question are aggregated together in a multiple_answers field:\\nprint(labels_agg[109])\\n{\\'question\\': \\'How does the fan work?\\', \\'multiple_answers\\': [\\'the fan is really\\nreally good\\', \"the fan itself isn\\'t super loud. There is an adjustable dial to\\nchange fan speed\"], \\'is_correct_answer\\': True, \\'is_correct_document\\': True,\\n\\'origin\\': \\'5a9b7616541f700f103d21f8ad41bc4b\\', \\'multiple_document_ids\\': [None,\\nNone], \\'multiple_offset_start_in_docs\\': [None, None], \\'no_answer\\': False,\\n\\'model_id\\': None, \\'meta\\': {\\'item_id\\': \\'B002MU1ZRS\\'}}\\nWe now have all the ingredients for evaluating the retriever, so let’s define a function\\nthat feeds each question-answer pair associated with each product to the evaluation\\npipeline and tracks the correct retrievals in our pipe object:\\ndef run_pipeline(pipeline, top_k_retriever=10, top_k_reader=4):\\n    for l in labels_agg:\\n        _ = pipeline.pipeline.run(\\n            query=l.question,\\n            top_k_retriever=top_k_retriever,\\n            top_k_reader=top_k_reader,\\n            top_k_eval_documents=top_k_retriever,\\n            labels=l,\\n            filters={\"item_id\": [l.meta[\"item_id\"]], \"split\": [\"test\"]})\\n192 | Chapter 7: Question Answering'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 216, 'page_label': '193'}, page_content='run_pipeline(pipe, top_k_retriever=3)\\nprint(f\"Recall@3: {pipe.eval_retriever.recall:.2f}\")\\nRecall@3: 0.95\\nGreat, it works! Notice that we picked a specific value for top_k_retriever to specify\\nthe number of documents to retrieve. In general, increasing this parameter will\\nimprove the recall, but at the expense of providing more documents to the reader and\\nslowing down the end-to-end pipeline. To guide our decision on which value to pick,\\nwe’ll create a function that loops over several k values and compute the recall across\\nthe whole test set for each k:\\ndef evaluate_retriever(retriever, topk_values = [1,3,5,10,20]):\\n    topk_results = {}\\n    for topk in topk_values:\\n        # Create Pipeline\\n        p = EvalRetrieverPipeline(retriever)\\n        # Loop over each question-answers pair in test set\\n        run_pipeline(p, top_k_retriever=topk)\\n        # Get metrics\\n        topk_results[topk] = {\"recall\": p.eval_retriever.recall}\\n    return pd.DataFrame.from_dict(topk_results, orient=\"index\")\\nes_topk_df = evaluate_retriever(es_retriever)\\nIf we plot the results, we can see how the recall improves as we increase k:\\ndef plot_retriever_eval(dfs, retriever_names):\\n    fig, ax = plt.subplots()\\n    for df, retriever_name in zip(dfs, retriever_names):\\n        df.plot(y=\"recall\", ax=ax, label=retriever_name)\\n    plt.xticks(df.index)\\n    plt.ylabel(\"Top-k Recall\")\\n    plt.xlabel(\"k\")\\n    plt.show()\\nplot_retriever_eval([es_topk_df], [\"BM25\"])\\nImproving Our QA Pipeline | 193'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 217, 'page_label': '194'}, page_content='14 V . Karpukhin et al., “Dense Passage Retrieval for Open-Domain Question Answering”, (2020).\\nFrom the plot, we can see that there’s an inflection point around k= 5 and we get\\nalmost perfect recall from k= 10 onwards. Let’s now take a look at retrieving docu‐\\nments with dense vector techniques.\\nDense Passage Retrieval\\nWe’ve seen that we get almost perfect recall when our sparse retriever returns k= 10\\ndocuments, but can we do better at smaller values of k? The advantage of doing so is\\nthat we can pass fewer documents to the reader and thereby reduce the overall\\nlatency of our QA pipeline. A well-known limitation of sparse retrievers like BM25 is\\nthat they can fail to capture the relevant documents if the user query contains terms\\nthat don’t match exactly those of the review. One promising alternative is to use dense\\nembeddings to represent the question and document, and the current state of the art\\nis an architecture known as Dense Passage Retrieval  (DPR).14 The main idea behind\\nDPR is to use two BERT models as encoders for the question and the passage. As\\nillustrated in Figure 7-10 , these encoders map the input text into a d-dimensional\\nvector representation of the [CLS] token.\\n194 | Chapter 7: Question Answering'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 218, 'page_label': '195'}, page_content='Figure 7-10. DPR’s bi-encoder architecture for computing the relevance of a document\\nand query\\nIn Haystack, we can initialize a retriever for DPR in a similar way to what we did for\\nBM25. In addition to specifying the document store, we also need to pick the BERT\\nencoders for the question and passage. These encoders are trained by giving them\\nquestions with relevant (positive) passages and irrelevant (negative) passages, where\\nthe goal is to learn that relevant question-passage pairs have a higher similarity. For\\nour use case, we’ll use encoders that have been fine-tuned on the NQ corpus in this\\nway:\\nfrom haystack.retriever.dense import DensePassageRetriever\\ndpr_retriever = DensePassageRetriever(document_store=document_store,\\n    query_embedding_model=\"facebook/dpr-question_encoder-single-nq-base\",\\n    passage_embedding_model=\"facebook/dpr-ctx_encoder-single-nq-base\",\\n    embed_title=False)\\nHere we’ve also set embed_title=False since concatenating the document’s title (i.e.,\\nitem_id) doesn’t provide any additional information because we filter per product.\\nOnce we’ve initialized the dense retriever, the next step is to iterate over all the\\nindexed documents in our Elasticsearch index and apply the encoders to update the\\nembedding representation. This can be done as follows:\\ndocument_store.update_embeddings(retriever=dpr_retriever)\\nImproving Our QA Pipeline | 195'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 219, 'page_label': '196'}, page_content='We’re now set to go! We can evaluate the dense retriever in the same way we did for\\nBM25 and compare the top-k recall:\\ndpr_topk_df = evaluate_retriever(dpr_retriever)\\nplot_retriever_eval([es_topk_df, dpr_topk_df], [\"BM25\", \"DPR\"])\\nHere we can see that DPR does not provide a boost in recall over BM25 and saturates\\naround k= 3.\\nPerforming similarity search of the embeddings can be sped up by\\nusing Facebook’s FAISS library as the document store. Similarly, the\\nperformance of the DPR retriever can be improved by fine-tuning\\non the target domain. If you’ d like to learn how to fine-tune DPR,\\ncheck out the Haystack tutorial.\\nNow that we’ve explored the evaluation of the retriever, let’s turn to evaluating the\\nreader.\\nEvaluating the Reader\\nIn extractive QA, there are two main metrics that are used for evaluating readers:\\nExact Match (EM)\\nA binary metric that gives EM = 1 if the characters in the predicted and ground\\ntruth answers match exactly, and EM = 0 otherwise. If no answer is expected, the\\nmodel gets EM = 0 if it predicts any text at all.\\nF1-score\\nMeasures the harmonic mean of the precision and recall.\\n196 | Chapter 7: Question Answering'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 220, 'page_label': '197'}, page_content='Let’s see how these metrics work by importing some helper functions from FARM\\nand applying them to a simple example:\\nfrom farm.evaluation.squad_evaluation import compute_f1, compute_exact\\npred = \"about 6000 hours\"\\nlabel = \"6000 hours\"\\nprint(f\"EM: {compute_exact(label, pred)}\")\\nprint(f\"F1: {compute_f1(label, pred)}\")\\nEM: 0\\nF1: 0.8\\nUnder the hood, these functions first normalize the prediction and label by removing\\npunctuation, fixing whitespace, and converting to lowercase. The normalized strings\\nare then tokenized as a bag-of-words, before finally computing the metric at the\\ntoken level. From this simple example we can see that EM is a much stricter metric\\nthan the F1-score: adding a single token to the prediction gives an EM of zero. On the\\nother hand, the F1-score can fail to catch truly incorrect answers. For example, if our\\npredicted answer span is “about 6000 dollars” , then we get:\\npred = \"about 6000 dollars\"\\nprint(f\"EM: {compute_exact(label, pred)}\")\\nprint(f\"F1: {compute_f1(label, pred)}\")\\nEM: 0\\nF1: 0.4\\nRelying on just the F1-score is thus misleading, and tracking both metrics is a good\\nstrategy to balance the trade-off between underestimating (EM) and overestimating\\n(F1-score) model performance.\\nNow in general, there are multiple valid answers per question, so these metrics are\\ncalculated for each question-answer pair in the evaluation set, and the best score is\\nselected over all possible answers. The overall EM and F1 scores for the model are\\nthen obtained by averaging over the individual scores of each question-answer pair.\\nTo evaluate the reader we’ll create a new pipeline with two nodes: a reader node and a\\nnode to evaluate the reader. We’ll use the EvalReader class that takes the predictions\\nfrom the reader and computes the corresponding EM and F1 scores. To compare with\\nthe SQuAD evaluation, we’ll take the best answers for each query with the top_1_em\\nand top_1_f1 metrics that are stored in EvalAnswers:\\nImproving Our QA Pipeline | 197'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 221, 'page_label': '198'}, page_content='from haystack.eval import EvalAnswers\\ndef evaluate_reader(reader):\\n    score_keys = [\\'top_1_em\\', \\'top_1_f1\\']\\n    eval_reader = EvalAnswers(skip_incorrect_retrieval=False)\\n    pipe = Pipeline()\\n    pipe.add_node(component=reader, name=\"QAReader\", inputs=[\"Query\"])\\n    pipe.add_node(component=eval_reader, name=\"EvalReader\", inputs=[\"QAReader\"])\\n    for l in labels_agg:\\n        doc = document_store.query(l.question,\\n                                   filters={\"question_id\":[l.origin]})\\n        _ = pipe.run(query=l.question, documents=doc, labels=l)\\n    return {k:v for k,v in eval_reader.__dict__.items() if k in score_keys}\\nreader_eval = {}\\nreader_eval[\"Fine-tune on SQuAD\"] = evaluate_reader(reader)\\nNotice that we specified skip_incorrect_retrieval=False. This is to ensure that\\nthe retriever always passes the context to the reader (as in the SQuAD evaluation).\\nNow that we’ve run every question through the reader, let’s print the scores:\\ndef plot_reader_eval(reader_eval):\\n    fig, ax = plt.subplots()\\n    df = pd.DataFrame.from_dict(reader_eval)\\n    df.plot(kind=\"bar\", ylabel=\"Score\", rot=0, ax=ax)\\n    ax.set_xticklabels([\"EM\", \"F1\"])\\n    plt.legend(loc=\\'upper left\\')\\n    plt.show()\\nplot_reader_eval(reader_eval)\\n198 | Chapter 7: Question Answering'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 222, 'page_label': '199'}, page_content='15 D. Y ogatama et al., “Learning and Evaluating General Linguistic Intelligence”, (2019).\\nOK, it seems that the fine-tuned model performs significantly worse on SubjQA than\\non SQuAD 2.0, where MiniLM achieves EM and F1 scores of 76.1 and 79.5, respec‐\\ntively. One reason for the performance drop is that customer reviews are quite differ‐\\nent from the Wikipedia articles the SQuAD 2.0 dataset is generated from, and the\\nlanguage they use is often informal. Another factor is likely the inherent subjectivity\\nof our dataset, where both questions and answers differ from the factual information\\ncontained in Wikipedia. Let’s look at how to fine-tune a model on a dataset to get bet‐\\nter results with domain adaptation.\\nDomain Adaptation\\nAlthough models that are fine-tuned on SQuAD will often generalize well to other\\ndomains, we’ve seen that for SubjQA the EM and F1 scores of our model were much\\nworse than for SQuAD. This failure to generalize has also been observed in other\\nextractive QA datasets and is understood as evidence that transformer models are\\nparticularly adept at overfitting to SQuAD. 15 The most straightforward way to\\nimprove the reader is by fine-tuning our MiniLM model further on the SubjQA train‐\\ning set. The FARMReader has a train() method that is designed for this purpose and\\nexpects the data to be in SQuAD JSON format, where all the question-answer pairs\\nare grouped together for each item as illustrated in Figure 7-11.\\nImproving Our QA Pipeline | 199'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 223, 'page_label': '200'}, page_content='Figure 7-11. Visualization of the SQuAD JSON format\\nThis is quite a complex data format, so we’ll need a few functions and some Pandas\\nmagic to help us do the conversion. The first thing we need to do is implement a\\nfunction that can create the paragraphs array associated with each product ID. Each\\nelement in this array contains a single context (i.e., review) and a qas array of\\nquestion-answer pairs. Here’s a function that builds up the paragraphs array:\\ndef create_paragraphs(df):\\n    paragraphs = []\\n    id2context = dict(zip(df[\"review_id\"], df[\"context\"]))\\n    for review_id, review in id2context.items():\\n        qas = []\\n        # Filter for all question-answer pairs about a specific context\\n        review_df = df.query(f\"review_id == \\'{review_id}\\'\")\\n        id2question = dict(zip(review_df[\"id\"], review_df[\"question\"]))\\n        # Build up the qas array\\n200 | Chapter 7: Question Answering'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 224, 'page_label': '201'}, page_content='for qid, question in id2question.items():\\n            # Filter for a single question ID\\n            question_df = df.query(f\"id == \\'{qid}\\'\").to_dict(orient=\"list\")\\n            ans_start_idxs = question_df[\"answers.answer_start\"][0].tolist()\\n            ans_text = question_df[\"answers.text\"][0].tolist()\\n            # Fill answerable questions\\n            if len(ans_start_idxs):\\n                answers = [\\n                    {\"text\": text, \"answer_start\": answer_start}\\n                    for text, answer_start in zip(ans_text, ans_start_idxs)]\\n                is_impossible = False\\n            else:\\n                answers = []\\n                is_impossible = True\\n            # Add question-answer pairs to qas\\n            qas.append({\"question\": question, \"id\": qid,\\n                        \"is_impossible\": is_impossible, \"answers\": answers})\\n        # Add context and question-answer pairs to paragraphs\\n        paragraphs.append({\"qas\": qas, \"context\": review})\\n    return paragraphs\\nNow, when we apply to the rows of a DataFrame associated with a single product ID,\\nwe get the SQuAD format:\\nproduct = dfs[\"train\"].query(\"title == \\'B00001P4ZH\\'\")\\ncreate_paragraphs(product)\\n[{\\'qas\\': [{\\'question\\': \\'How is the bass?\\',\\n    \\'id\\': \\'2543d296da9766d8d17d040ecc781699\\',\\n    \\'is_impossible\\': True,\\n    \\'answers\\': []}],\\n  \\'context\\': \\'I have had Koss headphones ...\\',\\n    \\'id\\': \\'d476830bf9282e2b9033e2bb44bbb995\\',\\n    \\'is_impossible\\': False,\\n    \\'answers\\': [{\\'text\\': \\'Bass is weak as expected\\', \\'answer_start\\': 1302},\\n     {\\'text\\': \\'Bass is weak as expected, even with EQ adjusted up\\',\\n      \\'answer_start\\': 1302}]}],\\n  \\'context\\': \\'To anyone who hasn\\\\\\'t tried all ...\\'},\\n {\\'qas\\': [{\\'question\\': \\'How is the bass?\\',\\n    \\'id\\': \\'455575557886d6dfeea5aa19577e5de4\\',\\n    \\'is_impossible\\': False,\\n    \\'answers\\': [{\\'text\\': \\'The only fault in the sound is the bass\\',\\n      \\'answer_start\\': 650}]}],\\n  \\'context\\': \"I have had many sub-$100 headphones ...\"}]\\nThe final step is to then apply this function to each product ID in the DataFrame of\\neach split. The following convert_to_squad() function does this trick and stores the\\nresult in an electronics-{split}.json file:\\nimport json\\ndef convert_to_squad(dfs):\\n    for split, df in dfs.items():\\nImproving Our QA Pipeline | 201'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 225, 'page_label': '202'}, page_content='subjqa_data = {}\\n        # Create `paragraphs` for each product ID\\n        groups = (df.groupby(\"title\").apply(create_paragraphs)\\n            .to_frame(name=\"paragraphs\").reset_index())\\n        subjqa_data[\"data\"] = groups.to_dict(orient=\"records\")\\n        # Save the result to disk\\n        with open(f\"electronics-{split}.json\", \"w+\", encoding=\"utf-8\") as f:\\n            json.dump(subjqa_data, f)\\nconvert_to_squad(dfs)\\nNow that we have the splits in the right format, let’s fine-tune our reader by specify‐\\ning the locations of the train and dev splits, along with where to save the fine-tuned\\nmodel:\\ntrain_filename = \"electronics-train.json\"\\ndev_filename = \"electronics-validation.json\"\\nreader.train(data_dir=\".\", use_gpu=True, n_epochs=1, batch_size=16,\\n             train_filename=train_filename, dev_filename=dev_filename)\\nWith the reader fine-tuned, let’s now compare its performance on the test set against\\nour baseline model:\\nreader_eval[\"Fine-tune on SQuAD + SubjQA\"] = evaluate_reader(reader)\\nplot_reader_eval(reader_eval)\\nWow, domain adaptation has increased our EM score by a factor of six and more than\\ndoubled the F1-score! At this point, you might be wondering why we didn’t just fine-\\ntune a pretrained language model directly on the SubjQA training set. One reason is\\nthat we only have 1,295 training examples in SubjQA while SQuAD has over 100,000,\\nso we might run into challenges with overfitting. Nevertheless, let’s take a look at what\\nnaive fine-tuning produces. For a fair comparison, we’ll use the same language model\\n202 | Chapter 7: Question Answering'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 226, 'page_label': '203'}, page_content='that was used for fine-tuning our baseline on SQuAD. As before, we’ll load up the\\nmodel with the FARMReader:\\nminilm_ckpt = \"microsoft/MiniLM-L12-H384-uncased\"\\nminilm_reader = FARMReader(model_name_or_path=minilm_ckpt, progress_bar=False,\\n                           max_seq_len=max_seq_length, doc_stride=doc_stride,\\n                           return_no_answer=True)\\nNext, we fine-tune for one epoch:\\nminilm_reader.train(data_dir=\".\", use_gpu=True, n_epochs=1, batch_size=16,\\n             train_filename=train_filename, dev_filename=dev_filename)\\nand include the evaluation on the test set:\\nreader_eval[\"Fine-tune on SubjQA\"] = evaluate_reader(minilm_reader)\\nplot_reader_eval(reader_eval)\\nWe can see that fine-tuning the language model directly on SubjQA results in consid‐\\nerably worse performance than fine-tuning on SQuAD and SubjQA.\\nWhen dealing with small datasets, it is best practice to use cross-\\nvalidation when evaluating transformers as they can be prone to\\noverfitting. Y ou can find an example of how to perform cross-\\nvalidation with SQuAD-formatted datasets in the FARM\\nrepository.\\nEvaluating the Whole QA Pipeline\\nNow that we’ve seen how to evaluate the reader and retriever components individu‐\\nally, let’s tie them together to measure the overall performance of our pipeline. To do\\nso, we’ll need to augment our retriever pipeline with nodes for the reader and its\\nImproving Our QA Pipeline | 203'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 227, 'page_label': '204'}, page_content='evaluation. We’ve seen that we get almost perfect recall at k= 10, so we can fix this\\nvalue and assess the impact this has on the reader’s performance (since it will now\\nreceive multiple contexts per query compared to the SQuAD-style evaluation):\\n# Initialize retriever pipeline\\npipe = EvalRetrieverPipeline(es_retriever)\\n# Add nodes for reader\\neval_reader = EvalAnswers()\\npipe.pipeline.add_node(component=reader, name=\"QAReader\",\\n              inputs=[\"EvalRetriever\"])\\npipe.pipeline.add_node(component=eval_reader, name=\"EvalReader\",\\n              inputs=[\"QAReader\"])\\n# Evaluate!\\nrun_pipeline(pipe)\\n# Extract metrics from reader\\nreader_eval[\"QA Pipeline (top-1)\"] = {\\n    k:v for k,v in eval_reader.__dict__.items()\\n    if k in [\"top_1_em\", \"top_1_f1\"]}\\nWe can then compare the top 1 EM and F1 scores for the model to predict an answer\\nin the documents returned by the retriever in Figure 7-12.\\nFigure 7-12. Comparison of EM and F1 scores for the reader against the whole QA\\npipeline\\nFrom this plot we can see the effect that the retriever has on the overall performance.\\nIn particular, there is an overall degradation compared to matching the question-\\ncontext pairs, as is done in the SQuAD-style evaluation. This can be circumvented by\\nincreasing the number of possible answers that the reader is allowed to predict.\\nUntil now we have only extracted answer spans from the context, but in general it\\ncould be that bits and pieces of the answer are scattered throughout the document\\n204 | Chapter 7: Question Answering'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 228, 'page_label': '205'}, page_content='16 P . Lewis et al., “Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks”, (2020).\\nand we would like our model to synthesize these fragments into a single coherent\\nanswer. Let’s have a look at how we can use generative QA to succeed at this task.\\nGoing Beyond Extractive QA\\nOne interesting alternative to extracting answers as spans of text in a document is to\\ngenerate them with a pretrained language model. This approach is often referred to as\\nabstractive or generative QA and has the potential to produce better-phrased answers\\nthat synthesize evidence across multiple passages. Although less mature than extrac‐\\ntive QA, this is a fast-moving field of research, so chances are that these approaches\\nwill be widely adopted in industry by the time you are reading this! In this section\\nwe’ll briefly touch on the current state of the art: retrieval-augmented generation\\n(RAG).16\\nRAG extends the classic retriever-reader architecture that we’ve seen in this chapter\\nby swapping the reader for a generator and using DPR as the retriever. The generator\\nis a pretrained sequence-to-sequence transformer like T5 or BART that receives latent\\nvectors of documents from DPR and then iteratively generates an answer based on\\nthe query and these documents. Since DPR and the generator are differentiable, the\\nwhole process can be fine-tuned end-to-end as illustrated in Figure 7-13.\\nFigure 7-13. The RAG architecture for fine-tuning a retriever and generator end-to-end\\n(courtesy of Ethan Perez)\\nTo show RAG in action we’ll use the DPRetriever from earlier, so we just need to\\ninstantiate a generator. There are two types of RAG models to choose from:\\nRAG-Sequence\\nUses the same retrieved document to generate the complete answer. In particular,\\nthe top k documents from the retriever are fed to the generator, which produces\\nan output sequence for each document, and the result is marginalized to obtain\\nthe best answer.\\nGoing Beyond Extractive QA | 205'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 229, 'page_label': '206'}, page_content='RAG-Token\\nCan use a different document to generate each token in the answer. This allows\\nthe generator to synthesize evidence from multiple documents.\\nSince RAG-Token models tend to perform better than RAG-Sequence ones, we’ll use\\nthe token model that was fine-tuned on NQ as our generator. Instantiating a genera‐\\ntor in Haystack is similar to instantiating the reader, but instead of specifying the\\nmax_seq_length and doc_stride parameters for a sliding window over the contexts,\\nwe specify hyperparameters that control the text generation:\\nfrom haystack.generator.transformers import RAGenerator\\ngenerator = RAGenerator(model_name_or_path=\"facebook/rag-token-nq\",\\n                        embed_title=False, num_beams=5)\\nHere num_beams specifies the number of beams to use in beam search (text generation\\nis covered at length in Chapter 5). As we did with the DPR retriever, we don’t embed\\nthe document titles since our corpus is always filtered per product ID.\\nThe next thing to do is tie together the retriever and generator using Haystack’s\\nGenerativeQAPipeline:\\nfrom haystack.pipeline import GenerativeQAPipeline\\npipe = GenerativeQAPipeline(generator=generator, retriever=dpr_retriever)\\nIn RAG, both the query encoder and the generator are trained end-\\nto-end, while the context encoder is frozen. In Haystack, the\\nGenerativeQAPipeline uses the query encoder from RAGenerator\\nand the context encoder from DensePassageRetriever.\\nLet’s now give RAG a spin by feeding in some queries about the Amazon Fire tablet\\nfrom before. To simplify the querying, we’ll write a simple function that takes the\\nquery and prints out the top answers:\\ndef generate_answers(query, top_k_generator=3):\\n    preds = pipe.run(query=query, top_k_generator=top_k_generator,\\n                     top_k_retriever=5, filters={\"item_id\":[\"B0074BW614\"]})\\n    print(f\"Question: {preds[\\'query\\']} \\\\n\")\\n    for idx in range(top_k_generator):\\n        print(f\"Answer {idx+1}: {preds[\\'answers\\'][idx][\\'answer\\']}\")\\nOK, now we’re ready to give it a test:\\ngenerate_answers(query)\\nQuestion: Is it good for reading?\\nAnswer 1:  the screen is absolutely beautiful\\n206 | Chapter 7: Question Answering'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 230, 'page_label': '207'}, page_content='Answer 2:  the Screen is absolutely beautiful\\nAnswer 3:  Kindle fire\\nThis result isn’t too bad for an answer, but it does suggest that the subjective nature of\\nthe question is confusing the generator. Let’s try with something a bit more factual:\\ngenerate_answers(\"What is the main drawback?\")\\nQuestion: What is the main drawback?\\nAnswer 1:  the price\\nAnswer 2:  no flash support\\nAnswer 3:  the cost\\nThis is more sensible! To get better results we could fine-tune RAG end-to-end on\\nSubjQA; we’ll leave this as an exercise, but if you’re interested in exploring it there are\\nscripts in the \\n  Transformers repository to help you get started.\\nConclusion\\nWell, that was a whirlwind tour of QA, and you probably have many more questions\\nthat you’ d like answered (pun intended!). In this chapter, we discussed two\\napproaches to QA (extractive and generative) and examined two different retrieval\\nalgorithms (BM25 and DPR). Along the way, we saw that domain adaptation can be a\\nsimple technique to boost the performance of our QA system by a significant margin,\\nand we looked at a few of the most common metrics that are used for evaluating such\\nsystems. Although we focused on closed-domain QA (i.e., a single domain of elec‐\\ntronic products), the techniques in this chapter can easily be generalized to the open-\\ndomain case; we recommend reading Cloudera’s excellent Fast Forward QA series to\\nsee what’s involved.\\nDeploying QA systems in the wild can be a tricky business to get right, and our expe‐\\nrience is that a significant part of the value comes from first providing end users with\\nuseful search capabilities, followed by an extractive component. In this respect, the\\nreader can be used in novel ways beyond answering on-demand user queries. For\\nexample, researchers at Grid Dynamics were able to use their reader to automatically\\nextract a set of pros and cons for each product in a client’s catalog. They also showed\\nthat a reader can be used to extract named entities in a zero-shot fashion by creating\\nqueries like “What kind of camera?” Given its infancy and subtle failure modes, we\\nrecommend exploring generative QA only once the other two approaches have been\\nexhausted. This “hierarchy of needs” for tackling QA problems is illustrated in\\nFigure 7-14.\\nConclusion | 207'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 231, 'page_label': '208'}, page_content='17 A. Talmor et al., “MultiModalQA: Complex Question Answering over Text, Tables and Images”, (2021).\\n18 P . Lewis et al., “PAQ: 65 Million Probably-Asked Questions and What Y ou Can Do with Them”, (2021); A.\\nRiabi et al., “Synthetic Data Augmentation for Zero-Shot Cross-Lingual Question Answering”, (2020).\\nFigure 7-14. The QA hierarchy of needs\\nLooking ahead, one exciting research area is multimodal QA, which involves QA over\\nmultiple modalities like text, tables, and images. As described in the MultiModalQA\\nbenchmark,17 such systems could enable users to answer complex questions that inte‐\\ngrate information across different modalities, like “When was the famous painting\\nwith two touching fingers completed?” Another area with practical business applica‐\\ntions is QA over a knowledge graph, where the nodes of the graph correspond to real-\\nworld entities and their relations are defined by the edges. By encoding factoids as\\n(subject, predicate, object) triples, one can use the graph to answer questions about a\\nmissing element. For an example that combines transformers with knowledge graphs,\\nsee the Haystack tutorials. One more promising direction is automatic question gener‐\\nation as a way to do some form of unsupervised/weakly supervised training using\\nunlabeled data or data augmentation. Two recent examples include the papers on the\\nProbably Answered Questions (PAQ) benchmark and synthetic data augmentation\\nfor cross-lingual settings.18\\nIn this chapter we’ve seen that in order to successfully use QA models for real-world\\nuse cases we need to apply a few tricks, such as implementing a fast retrieval pipeline\\nto make predictions in near real time. Still, applying a QA model to a handful of pre‐\\nselected documents can take a couple of seconds on production hardware. Although\\nthis may not sound like much, imagine how different your experience would be if you\\nhad to wait a few seconds to get the results of a Google search—a few seconds of wait\\ntime can decide the fate of your transformer-powered application. In the next chapter\\nwe’ll have a look at a few methods to accelerate model predictions further.\\n208 | Chapter 7: Question Answering'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 232, 'page_label': '209'}, page_content='CHAPTER 8\\nMaking Transformers Efficient\\nin Production\\nIn the previous chapters, you’ve seen how transformers can be fine-tuned to produce\\ngreat results on a wide range of tasks. However, in many situations accuracy (or what‐\\never metric you’re optimizing for) is not enough; your state-of-the-art model is not\\nvery useful if it’s too slow or large to meet the business requirements of your applica‐\\ntion. An obvious alternative is to train a faster and more compact model, but the\\nreduction in model capacity is often accompanied by a degradation in performance.\\nSo what can you do when you need a fast, compact, yet highly accurate model?\\nIn this chapter we will explore four complementary techniques that can be used to\\nspeed up the predictions and reduce the memory footprint of your transformer mod‐\\nels: knowledge distillation, quantization, pruning, and graph optimization with the\\nOpen Neural Network Exchange (ONNX) format and ONNX Runtime (ORT). We’ll\\nalso see how some of these techniques can be combined to produce significant per‐\\nformance gains. For example, this was the approach taken by the Roblox engineering\\nteam in their article “How We Scaled Bert to Serve 1+ Billion Daily Requests on\\nCPUs”, who as shown in Figure 8-1 found that combining knowledge distillation and\\nquantization enabled them to improve the latency and throughput of their BERT clas‐\\nsifier by over a factor of 30!\\n209'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 233, 'page_label': '210'}, page_content='Figure 8-1. How Roblox scaled BERT with knowledge distillation, dynamic padding, and\\nweight quantization (photo courtesy of Roblox employees Quoc N. Le and Kip Kaehler)\\nTo illustrate the benefits and trade-offs associated with each technique, we’ll use\\nintent detection as a case study; this is an important component of text-based assis‐\\ntants, where low latencies are critical for maintaining a conversation in real time.\\nAlong the way you’ll learn how to create custom trainers, perform efficient hyper‐\\nparameter search, and gain a sense of what it takes to implement cutting-edge\\nresearch with \\n  Transformers. Let’s dive in!\\nIntent Detection as a Case Study\\nLet’s suppose that we’re trying to build a text-based assistant for our company’s call\\ncenter so that customers can request their account balance or make bookings without\\nneeding to speak with a human agent. In order to understand the goals of a customer,\\nour assistant will need to be able to classify a wide variety of natural language text\\ninto a set of predefined actions or intents. For example, a customer might send a mes‐\\nsage like the following about an upcoming trip:\\nHey, I’ d like to rent a vehicle from Nov 1st to Nov 15th in Paris and I need a 15 passen‐\\nger van\\nand our intent classifier could automatically categorize this as a Car Rental intent,\\nwhich then triggers an action and response. To be robust in a production environ‐\\nment, our classifier will also need to be able to handle out-of-scope queries, where a\\ncustomer makes a query that doesn’t belong to any of the predefined intents and the\\nsystem should yield a fallback response. For example, in the second case shown in\\nFigure 8-2, a customer asks a question about sports (which is out of scope), and the\\ntext assistant mistakenly classifies it as one of the known in-scope intents and returns\\n210 | Chapter 8: Making Transformers Efficient in Production'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 234, 'page_label': '211'}, page_content='1 S. Larson et al., “ An Evaluation Dataset for Intent Classification and Out-of-Scope Prediction”, (2019).\\nthe payday response. In the third case, the text assistant has been trained to detect\\nout-of-scope queries (usually labeled as a separate class) and informs the customer\\nabout which topics it can answer questions about.\\nFigure 8-2. Three exchanges between a human (right) and a text-based assistant (left)\\nfor personal finance (courtesy of Stefan Larson et al.)\\nAs a baseline, we’ve fine-tuned a BERT-base model that achieves around 94% accu‐\\nracy on the CLINC150 dataset. 1 This dataset includes 22,500 in-scope queries across\\n150 intents and 10 domains like banking and travel, and also includes 1,200 out-of-\\nscope queries that belong to an oos intent class. In practice we would also gather our\\nown in-house dataset, but using public data is a great way to iterate quickly and gen‐\\nerate preliminary results.\\nTo get started, let’s download our fine-tuned model from the Hugging Face Hub and\\nwrap it in a pipeline for text classification:\\nfrom transformers import pipeline\\nbert_ckpt = \"transformersbook/bert-base-uncased-finetuned-clinc\"\\npipe = pipeline(\"text-classification\", model=bert_ckpt)\\nNow that we have a pipeline, we can pass a query to get the predicted intent and con‐\\nfidence score from the model:\\nIntent Detection as a Case Study | 211'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 235, 'page_label': '212'}, page_content='2 As described by Emmanuel Ameisen in Building Machine Learning Powered Applications (O’Reilly), business\\nor product metrics are the most important ones to consider. After all, it doesn’t matter how accurate your\\nmodel is if it doesn’t solve a problem your business cares about. In this chapter we’ll assume that you have\\nalready defined the metrics that matter for your application and focus on optimizing the model metrics.\\nquery = \"\"\"Hey, I\\'d like to rent a vehicle from Nov 1st to Nov 15th in\\nParis and I need a 15 passenger van\"\"\"\\npipe(query)\\n[{\\'label\\': \\'car_rental\\', \\'score\\': 0.549003541469574}]\\nGreat, the car_rental intent makes sense. Let’s now look at creating a benchmark\\nthat we can use to evaluate the performance of our baseline model.\\nCreating a Performance Benchmark\\nLike other machine learning models, deploying transformers in production environ‐\\nments involves a trade-off among several constraints, the most common being:2\\nModel performance\\nHow well does our model perform on a well-crafted test set that reflects produc‐\\ntion data? This is especially important when the cost of making errors is large\\n(and best mitigated with a human in the loop), or when we need to run inference\\non millions of examples and small improvements to the model metrics can trans‐\\nlate into large gains in aggregate.\\nLatency\\nHow fast can our model deliver predictions? We usually care about latency in\\nreal-time environments that deal with a lot of traffic, like how Stack Overflow\\nneeded a classifier to quickly detect unwelcome comments on the website.\\nMemory\\nHow can we deploy billion-parameter models like GPT-2 or T5 that require giga‐\\nbytes of disk storage and RAM? Memory plays an especially important role in\\nmobile or edge devices, where a model has to generate predictions without access\\nto a powerful cloud server.\\nFailing to address these constraints can have a negative impact on the user experience\\nof your application. More commonly, it can lead to ballooning costs from running\\nexpensive cloud servers that may only need to handle a few requests. To explore how\\neach of these constraints can be optimized with various compression techniques, let’s\\nbegin by creating a simple benchmark that measures each quantity for a given pipe‐\\nline and test set. A skeleton of what we’ll need is given by the following class:\\nclass PerformanceBenchmark:\\n    def __init__(self, pipeline, dataset, optim_type=\"BERT baseline\"):\\n        self.pipeline = pipeline\\n212 | Chapter 8: Making Transformers Efficient in Production'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 236, 'page_label': '213'}, page_content='self.dataset = dataset\\n        self.optim_type = optim_type\\n    def compute_accuracy(self):\\n        # We\\'ll define this later\\n        pass\\n    def compute_size(self):\\n        # We\\'ll define this later\\n        pass\\n    def time_pipeline(self):\\n        # We\\'ll define this later\\n        pass\\n    def run_benchmark(self):\\n        metrics = {}\\n        metrics[self.optim_type] = self.compute_size()\\n        metrics[self.optim_type].update(self.time_pipeline())\\n        metrics[self.optim_type].update(self.compute_accuracy())\\n        return metrics\\nWe’ve defined an optim_type parameter to keep track of the different optimization\\ntechniques that we’ll cover in this chapter. We’ll use the run_benchmark() method to\\ncollect all the metrics in a dictionary, with keys given by optim_type.\\nLet’s now put some flesh on the bones of this class by computing the model accuracy\\non the test set. First we need some data to test on, so let’s download the CLINC150\\ndataset that was used to fine-tune our baseline model. We can get the dataset from the\\nHub with \\n  Datasets as follows:\\nfrom datasets import load_dataset\\nclinc = load_dataset(\"clinc_oos\", \"plus\")\\nHere, the plus configuration refers to the subset that contains the out-of-scope train‐\\ning examples. Each example in the CLINC150 dataset consists of a query in the text\\ncolumn and its corresponding intent. We’ll use the test set to benchmark our models,\\nso let’s take a look at one of the dataset’s examples:\\nsample = clinc[\"test\"][42]\\nsample\\n{\\'intent\\': 133, \\'text\\': \\'transfer $100 from my checking to saving account\\'}\\nThe intents are provided as IDs, but we can easily get the mapping to strings (and\\nvice versa) by accessing the features attribute of the dataset:\\nintents = clinc[\"test\"].features[\"intent\"]\\nintents.int2str(sample[\"intent\"])\\n\\'transfer\\'\\nCreating a Performance Benchmark | 213'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 237, 'page_label': '214'}, page_content='Now that we have a basic understanding of the contents in the CLINC150 dataset,\\nlet’s implement the compute_accuracy() method of PerformanceBenchmark. Since\\nthe dataset is balanced across the intent classes, we’ll use accuracy as our metric. We\\ncan load this metric with \\n  Datasets as follows:\\nfrom datasets import load_metric\\naccuracy_score = load_metric(\"accuracy\")\\nThe accuracy metric expects the predictions and references (i.e., the ground truth\\nlabels) to be integers. We can use the pipeline to extract the predictions from the text\\nfield and then use the str2int() method of our intents object to map each predic‐\\ntion to its corresponding ID. The following code collects all the predictions and labels\\nin lists before returning the accuracy on the dataset. Let’s also add it to our Perform\\nanceBenchmark class:\\ndef compute_accuracy(self):\\n    \"\"\"This overrides the PerformanceBenchmark.compute_accuracy() method\"\"\"\\n    preds, labels = [], []\\n    for example in self.dataset:\\n        pred = self.pipeline(example[\"text\"])[0][\"label\"]\\n        label = example[\"intent\"]\\n        preds.append(intents.str2int(pred))\\n        labels.append(label)\\n    accuracy = accuracy_score.compute(predictions=preds, references=labels)\\n    print(f\"Accuracy on test set - {accuracy[\\'accuracy\\']:.3f}\")\\n    return accuracy\\nPerformanceBenchmark.compute_accuracy = compute_accuracy\\nNext, let’s compute the size of our model by using the torch.save() function from\\nPyTorch to serialize the model to disk. Under the hood, torch.save() uses Python’s\\npickle module and can be used to save anything from models to tensors to ordinary\\nPython objects. In PyTorch, the recommended way to save a model is by using its\\nstate_dict, which is a Python dictionary that maps each layer in a model to its\\nlearnable parameters (i.e., weights and biases). Let’s see what is stored in the\\nstate_dict of our baseline model:\\nlist(pipe.model.state_dict().items())[42]\\n(\\'bert.encoder.layer.2.attention.self.value.weight\\',\\n tensor([[-1.0526e-02, -3.2215e-02,  2.2097e-02,  ..., -6.0953e-03,\\n           4.6521e-03,  2.9844e-02],\\n         [-1.4964e-02, -1.0915e-02,  5.2396e-04,  ...,  3.2047e-05,\\n          -2.6890e-02, -2.1943e-02],\\n         [-2.9640e-02, -3.7842e-03, -1.2582e-02,  ..., -1.0917e-02,\\n           3.1152e-02, -9.7786e-03],\\n         ...,\\n         [-1.5116e-02, -3.3226e-02,  4.2063e-02,  ..., -5.2652e-03,\\n           1.1093e-02,  2.9703e-03],\\n214 | Chapter 8: Making Transformers Efficient in Production'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 238, 'page_label': '215'}, page_content='[-3.6809e-02,  5.6848e-02, -2.6544e-02,  ..., -4.0114e-02,\\n           6.7487e-03,  1.0511e-03],\\n         [-2.4961e-02,  1.4747e-03, -5.4271e-02,  ...,  2.0004e-02,\\n           2.3981e-02, -4.2880e-02]]))\\nWe can clearly see that each key/value pair corresponds to a specific layer and tensor\\nin BERT. So if we save our model with:\\ntorch.save(pipe.model.state_dict(), \"model.pt\")\\nwe can then use the Path.stat() function from Python’s pathlib module to get\\ninformation about the underlying files. In particular, Path(\"model.pt\").stat().\\nst_size will give us the model size in bytes. Let’s put this all together in the compute_\\nsize() function and add it to PerformanceBenchmark:\\nimport torch\\nfrom pathlib import Path\\ndef compute_size(self):\\n    \"\"\"This overrides the PerformanceBenchmark.compute_size() method\"\"\"\\n    state_dict = self.pipeline.model.state_dict()\\n    tmp_path = Path(\"model.pt\")\\n    torch.save(state_dict, tmp_path)\\n    # Calculate size in megabytes\\n    size_mb = Path(tmp_path).stat().st_size / (1024 * 1024)\\n    # Delete temporary file\\n    tmp_path.unlink()\\n    print(f\"Model size (MB) - {size_mb:.2f}\")\\n    return {\"size_mb\": size_mb}\\nPerformanceBenchmark.compute_size = compute_size\\nFinally let’s implement the time_pipeline() function so that we can time the average\\nlatency per query. For this application, latency refers to the time it takes to feed a text\\nquery to the pipeline and return the predicted intent from the model. Under the hood\\nthe pipeline also tokenizes the text, but this is around one thousand times faster than\\ngenerating the predictions and thus adds a negligible contribution to the overall\\nlatency. A simple way to measure the execution time of a code snippet is to use the\\nperf_counter() function from Python’s time module. This function has a better time\\nresolution than the time.time() function and is well suited for getting precise\\nresults.\\nWe can use perf_counter() to time our pipeline by passing our test query and calcu‐\\nlating the time difference in milliseconds between the start and end:\\nfrom time import perf_counter\\nfor _ in range(3):\\n    start_time = perf_counter()\\n    _ = pipe(query)\\nCreating a Performance Benchmark | 215'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 239, 'page_label': '216'}, page_content='latency = perf_counter() - start_time\\n    print(f\"Latency (ms) - {1000 * latency:.3f}\")\\nLatency (ms) - 85.367\\nLatency (ms) - 85.241\\nLatency (ms) - 87.275\\nThese results exhibit quite some spread in the latencies and suggest that timing a sin‐\\ngle pass through the pipeline can give wildly different results each time we run the\\ncode. So instead, we’ll collect the latencies over many runs and then use the resulting\\ndistribution to calculate the mean and standard deviation, which will give us an idea\\nabout the spread in values. The following code does what we need and includes a\\nphase to warm up the CPU before performing the actual timed run:\\nimport numpy as np\\ndef time_pipeline(self, query=\"What is the pin number for my account?\"):\\n    \"\"\"This overrides the PerformanceBenchmark.time_pipeline() method\"\"\"\\n    latencies = []\\n    # Warmup\\n    for _ in range(10):\\n        _ = self.pipeline(query)\\n    # Timed run\\n    for _ in range(100):\\n        start_time = perf_counter()\\n        _ = self.pipeline(query)\\n        latency = perf_counter() - start_time\\n        latencies.append(latency)\\n    # Compute run statistics\\n    time_avg_ms = 1000 * np.mean(latencies)\\n    time_std_ms = 1000 * np.std(latencies)\\n    print(f\"Average latency (ms) - {time_avg_ms:.2f} +\\\\- {time_std_ms:.2f}\")\\n    return {\"time_avg_ms\": time_avg_ms, \"time_std_ms\": time_std_ms}\\nPerformanceBenchmark.time_pipeline = time_pipeline\\nTo keeps things simple, we’ll use the same query value to benchmark all our models.\\nIn general, the latency will depend on the query length, and a good practice is to\\nbenchmark your models with queries that they’re likely to encounter in production\\nenvironments.\\nNow that our PerformanceBenchmark class is complete, let’s give it a spin! Let’s start\\nby benchmarking our BERT baseline. For the baseline model, we just need to pass the\\npipeline and the dataset we wish to perform the benchmark on. We’ll collect the\\nresults in the perf_metrics dictionary to keep track of each model’s performance:\\npb = PerformanceBenchmark(pipe, clinc[\"test\"])\\nperf_metrics = pb.run_benchmark()\\nModel size (MB) - 418.16\\nAverage latency (ms) - 54.20 +\\\\- 1.91\\nAccuracy on test set - 0.867\\n216 | Chapter 8: Making Transformers Efficient in Production'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 240, 'page_label': '217'}, page_content='3 C. Buciluă et al., “Model Compression, ” Proceedings of the 12th ACM SIGKDD International Conference on\\nKnowledge Discovery and Data Mining (August 2006): 535–541, https://doi.org/10.1145/1150402.1150464.\\n4 G. Hinton, O. Vinyals, and J. Dean, “Distilling the Knowledge in a Neural Network”, (2015).\\n5 W . Fedus, B. Zoph, and N. Shazeer, “Switch Transformers: Scaling to Trillion Parameter Models with Simple\\nand Efficient Sparsity”, (2021).\\n6 Geoff Hinton coined this term in a talk to refer to the observation that softened probabilities reveal the hid‐\\nden knowledge of the teacher.\\nNow that we have a reference point, let’s look at our first compression technique:\\nknowledge distillation.\\nThe average latency values will differ depending on what type of\\nhardware you are running on. For example, you can usually get\\nbetter performance by running inference on a GPU since it enables\\nbatch processing. For the purposes of this chapter, what’s important\\nis the relative difference in latencies between models. Once we have\\ndetermined the best-performing model, we can then explore differ‐\\nent backends to reduce the absolute latency if needed.\\nMaking Models Smaller via Knowledge Distillation\\nKnowledge distillation is a general-purpose method for training a smaller student\\nmodel to mimic the behavior of a slower, larger, but better-performing teacher. Origi‐\\nnally introduced in 2006 in the context of ensemble models, 3 it was later popularized\\nin a famous 2015 paper that generalized the method to deep neural networks and\\napplied it to image classification and automatic speech recognition.4\\nGiven the trend toward pretraining language models with ever-increasing parameter\\ncounts (the largest at the time of writing having over one trillion parameters),5 knowl‐\\nedge distillation has also become a popular strategy to compress these huge models\\nand make them more suitable for building practical applications.\\nKnowledge Distillation for Fine-Tuning\\nSo how is knowledge actually “distilled” or transferred from the teacher to the student\\nduring training? For supervised tasks like fine-tuning, the main idea is to augment\\nthe ground truth labels with a distribution of “soft probabilities” from the teacher\\nwhich provide complementary information for the student to learn from. For exam‐\\nple, if our BERT-base classifier assigns high probabilities to multiple intents, then this\\ncould be a sign that these intents lie close to each other in the feature space. By train‐\\ning the student to mimic these probabilities, the goal is to distill some of this “dark\\nknowledge”6 that the teacher has learned—that is, knowledge that is not available\\nfrom the labels alone.\\nMaking Models Smaller via Knowledge Distillation | 217'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 241, 'page_label': '218'}, page_content='7 We also encountered temperature in the context of text generation in Chapter 5.\\nMathematically, the way this works is as follows. Suppose we feed an input sequence x\\nto the teacher to generate a vector of logits \\ud835x = [ z1 x , ...,zN x ]. We can convert\\nthese logits into probabilities by applying a softmax function:\\nexp zi x\\n∑j exp zi x\\nThis isn’t quite what we want, though, because in many cases the teacher will assign a\\nhigh probability to one class, with all other class probabilities close to zero. When that\\nhappens, the teacher doesn’t provide much additional information beyond the\\nground truth labels, so instead we “soften” the probabilities by scaling the logits with\\na temperature hyperparameter T before applying the softmax:7\\npi x =\\nexp zi x /T\\n∑j exp zi x /T\\nAs shown in Figure 8-3, higher values of T produce a softer probability distribution\\nover the classes and reveal much more information about the decision boundary that\\nthe teacher has learned for each training example. When T= 1 we recover the origi‐\\nnal softmax distribution.\\nFigure 8-3. Comparison of a hard label that is one-hot encoded (left), softmax probabili‐\\nties (middle), and softened class probabilities (right)\\nSince the student also produces softened probabilities qix of its own, we can use the\\nKullback–Leibler (KL) divergence to measure the difference between the two proba‐\\nbility distributions:\\nDKL p, q = ∑\\ni\\npi x log\\npi x\\nqi x\\n218 | Chapter 8: Making Transformers Efficient in Production'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 242, 'page_label': '219'}, page_content='With the KL divergence we can calculate how much is lost when we approximate the\\nprobability distribution of the teacher with the student. This allows us to define a\\nknowledge distillation loss:\\nLKD = T2DKL\\nwhere T2 is a normalization factor to account for the fact that the magnitude of the\\ngradients produced by soft labels scales as 1/T2. For classification tasks, the student\\nloss is then a weighted average of the distillation loss with the usual cross-entropy loss\\nLCE of the ground truth labels:\\nLstudent = αLCE + 1 − α LKD\\nwhere α is a hyperparameter that controls the relative strength of each loss. A dia‐\\ngram of the whole process is shown in Figure 8-4; the temperature is set to 1 at infer‐\\nence time to recover the standard softmax probabilities.\\nFigure 8-4. The knowledge distillation process\\nMaking Models Smaller via Knowledge Distillation | 219'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 243, 'page_label': '220'}, page_content='8 V . Sanh et al., “DistilBERT, a Distilled Version of BERT: Smaller, Faster, Cheaper and Lighter”, (2019).\\nKnowledge Distillation for Pretraining\\nKnowledge distillation can also be used during pretraining to create a general-\\npurpose student that can be subsequently fine-tuned on downstream tasks. In this\\ncase, the teacher is a pretrained language model like BERT, which transfers its knowl‐\\nedge about masked language modeling to the student. For example, in the DistilBERT\\npaper,8 the masked language modeling loss Lmlm is augmented with a term from\\nknowledge distillation and a cosine embedding loss Lcos = 1 − cos hs, ht  to align the\\ndirections of the hidden state vectors between the teacher and student:\\nLDistilBERT = αLmlm + βLKD + γLcos\\nSince we already have a fine-tuned BERT-base model, let’s see how we can use knowl‐\\nedge distillation to fine-tune a smaller and faster model. To do that we’ll need a way\\nto augment the cross-entropy loss with an LKD term. Fortunately we can do this by\\ncreating our own trainer!\\nCreating a Knowledge Distillation Trainer\\nTo implement knowledge distillation we need to add a few things to the Trainer base\\nclass:\\n• The new hyperparameters α and T, which control the relative weight of the distil‐\\nlation loss and how much the probability distribution of the labels should be\\nsmoothed\\n• The fine-tuned teacher model, which in our case is BERT-base\\n• A new loss function that combines the cross-entropy loss with the knowledge\\ndistillation loss\\nAdding the new hyperparameters is quite simple, since we just need to subclass\\nTrainingArguments and include them as new attributes:\\nfrom transformers import TrainingArguments\\nclass DistillationTrainingArguments(TrainingArguments):\\n    def __init__(self, *args, alpha=0.5, temperature=2.0, **kwargs):\\n        super().__init__(*args, **kwargs)\\n        self.alpha = alpha\\n        self.temperature = temperature\\n220 | Chapter 8: Making Transformers Efficient in Production'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 244, 'page_label': '221'}, page_content='For the trainer itself, we need a new loss function. The way to implement this is by\\nsubclassing Trainer and overriding the compute_loss() method to include the\\nknowledge distillation loss term LKD:\\nimport torch.nn as nn\\nimport torch.nn.functional as F\\nfrom transformers import Trainer\\nclass DistillationTrainer(Trainer):\\n    def __init__(self, *args, teacher_model=None, **kwargs):\\n        super().__init__(*args, **kwargs)\\n        self.teacher_model = teacher_model\\n    def compute_loss(self, model, inputs, return_outputs=False):\\n        outputs_stu = model(**inputs)\\n        # Extract cross-entropy loss and logits from student\\n        loss_ce = outputs_stu.loss\\n        logits_stu = outputs_stu.logits\\n        # Extract logits from teacher\\n        with torch.no_grad():\\n            outputs_tea = self.teacher_model(**inputs)\\n            logits_tea = outputs_tea.logits\\n        # Soften probabilities and compute distillation loss\\n        loss_fct = nn.KLDivLoss(reduction=\"batchmean\")\\n        loss_kd = self.args.temperature ** 2 * loss_fct(\\n            F.log_softmax(logits_stu / self.args.temperature, dim=-1),\\n            F.softmax(logits_tea / self.args.temperature, dim=-1))\\n        # Return weighted student loss\\n        loss = self.args.alpha * loss_ce + (1. - self.args.alpha) * loss_kd\\n        return (loss, outputs_stu) if return_outputs else loss\\nLet’s unpack this code a bit. When we instantiate DistillationTrainer we pass a\\nteacher_model argument with a teacher that has already been fine-tuned on our task.\\nNext, in the compute_loss() method we extract the logits from the student and\\nteacher, scale them by the temperature, and then normalize them with a softmax\\nbefore passing them to PyTorch’s nn.KLDivLoss() function for computing the KL\\ndivergence. One quirk with nn.KLDivLoss() is that it expects the inputs in the form\\nof log probabilities and the labels as normal probabilities. That’s why we’ve used the\\nF.log_softmax() function to normalize the student’s logits, while the teacher’s logits\\nare converted to probabilities with a standard softmax. The reduction=batchmean\\nargument in nn.KLDivLoss() specifies that we average the losses over the batch\\ndimension.\\nY ou can also perform knowledge distillation with the Keras API of\\nthe \\n  Transformers library. To do this, you’ll need to implement a\\ncustom Distiller class that overrides the train_step(),\\ntest_step(), and compile() methods of tf.keras.Model(). See\\nthe Keras documentation for an example of how to do this.\\nMaking Models Smaller via Knowledge Distillation | 221'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 245, 'page_label': '222'}, page_content='9 Y . Kim and H. Awadalla, “FastFormers: Highly Efficient Transformer Models for Natural Language Under‐\\nstanding”, (2020).\\n10 By default, the Trainer looks for a column called labels when fine-tuning on classification tasks. Y ou can\\nalso override this behavior by specifying the label_names argument of TrainingArguments.\\nChoosing a Good Student Initialization\\nNow that we have our custom trainer, the first question you might have is which pre‐\\ntrained language model should we pick for the student? In general we should pick a\\nsmaller model for the student to reduce the latency and memory footprint. A good\\nrule of thumb from the literature is that knowledge distillation works best when the\\nteacher and student are of the same model type.9 One possible reason for this is that\\ndifferent model types, say BERT and RoBERTa, can have different output embedding\\nspaces, which hinders the ability of the student to mimic the teacher. In our case\\nstudy the teacher is BERT, so DistilBERT is a natural candidate to initialize the stu‐\\ndent with since it has 40% fewer parameters and has been shown to achieve strong\\nresults on downstream tasks.\\nFirst we’ll need to tokenize and encode our queries, so let’s instantiate the tokenizer\\nfrom DistilBERT and create a simple tokenize_text() function to take care of the\\npreprocessing:\\nfrom transformers import AutoTokenizer\\nstudent_ckpt = \"distilbert-base-uncased\"\\nstudent_tokenizer = AutoTokenizer.from_pretrained(student_ckpt)\\ndef tokenize_text(batch):\\n    return student_tokenizer(batch[\"text\"], truncation=True)\\nclinc_enc = clinc.map(tokenize_text, batched=True, remove_columns=[\"text\"])\\nclinc_enc = clinc_enc.rename_column(\"intent\", \"labels\")\\nHere we’ve removed the text column since we no longer need it, and we’ve also\\nrenamed the intent column to labels so it can be automatically detected by the\\ntrainer.10\\nNow that we’ve processed our texts, the next thing we need to do is define the hyper‐\\nparameters and compute_metrics() function for our DistillationTrainer. We’ll\\nalso push all of our models to the Hugging Face Hub, so let’s start by logging in to our\\naccount:\\nfrom huggingface_hub import notebook_login\\nnotebook_login()\\n222 | Chapter 8: Making Transformers Efficient in Production'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 246, 'page_label': '223'}, page_content='11 This approach of fine-tuning a general-purpose, distilled language model is sometimes referred to as “task-\\nagnostic” distillation.\\nNext, we’ll define the metrics to track during training. As we did in the performance\\nbenchmark, we’ll use accuracy as the main metric. This means we can reuse our\\naccuracy_score() function in the compute_metrics() function that we’ll include in\\nDistillationTrainer:\\ndef compute_metrics(pred):\\n    predictions, labels = pred\\n    predictions = np.argmax(predictions, axis=1)\\n    return accuracy_score.compute(predictions=predictions, references=labels)\\nIn this function, the predictions from the sequence modeling head come in the form\\nof logits, so we use the np.argmax() function to find the most confident class predic‐\\ntion and compare that against the ground truth label.\\nNext we need to define the training arguments. To warm up, we’ll set α= 1 to see how\\nwell DistilBERT performs without any signal from the teacher. 11 Then we will push\\nour fine-tuned model to a new repository called distilbert-base-uncased-\\nfinetuned-clinc, so we just need to specify that in the output_dir argument of\\nDistillationTrainingArguments:\\nbatch_size = 48\\nfinetuned_ckpt = \"distilbert-base-uncased-finetuned-clinc\"\\nstudent_training_args = DistillationTrainingArguments(\\n    output_dir=finetuned_ckpt, evaluation_strategy = \"epoch\",\\n    num_train_epochs=5, learning_rate=2e-5,\\n    per_device_train_batch_size=batch_size,\\n    per_device_eval_batch_size=batch_size, alpha=1, weight_decay=0.01,\\n    push_to_hub=True)\\nWe’ve also tweaked a few of the default hyperparameter values, like the number of\\nepochs, the weight decay, and the learning rate. The next thing to do is initialize a\\nstudent model. Since we will be doing multiple runs with the trainer, we’ll create a\\nstudent_init() function to initialize the model with each new run. When we pass\\nthis function to the DistillationTrainer, this will ensure we initialize a new model\\neach time we call the train() method.\\nOne other thing we need to do is provide the student model with the mappings\\nbetween each intent and label ID. These mappings can be obtained from our BERT-\\nbase model that we downloaded in the pipeline:\\nid2label = pipe.model.config.id2label\\nlabel2id = pipe.model.config.label2id\\nMaking Models Smaller via Knowledge Distillation | 223'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 247, 'page_label': '224'}, page_content='With these mappings, we can now create a custom model configuration with the\\nAutoConfig class hat we encountered in Chapters 3 and 4. Let’s use this to create a\\nconfiguration for our student with the information about the label mappings:\\nfrom transformers import AutoConfig\\nnum_labels = intents.num_classes\\nstudent_config = (AutoConfig\\n                  .from_pretrained(student_ckpt, num_labels=num_labels,\\n                                   id2label=id2label, label2id=label2id))\\nHere we’ve also specified the number of classes our model should expect. We can then\\nprovide this configuration to the from_pretrained() function of the AutoModelFor\\nSequenceClassification class as follows:\\nimport torch\\nfrom transformers import AutoModelForSequenceClassification\\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\\ndef student_init():\\n    return (AutoModelForSequenceClassification\\n            .from_pretrained(student_ckpt, config=student_config).to(device))\\nWe now have all the ingredients needed for our distillation trainer, so let’s load the\\nteacher and fine-tune:\\nteacher_ckpt = \"transformersbook/bert-base-uncased-finetuned-clinc\"\\nteacher_model = (AutoModelForSequenceClassification\\n                 .from_pretrained(teacher_ckpt, num_labels=num_labels)\\n                 .to(device))\\ndistilbert_trainer = DistillationTrainer(model_init=student_init,\\n    teacher_model=teacher_model, args=student_training_args,\\n    train_dataset=clinc_enc[\\'train\\'], eval_dataset=clinc_enc[\\'validation\\'],\\n    compute_metrics=compute_metrics, tokenizer=student_tokenizer)\\ndistilbert_trainer.train()\\nEpoch Training Loss Validation Loss Accuracy\\n1 4.2923 3.289337 0.742258\\n2 2.6307 1.883680 0.828065\\n3 1.5483 1.158315 0.896774\\n4 1.0153 0.861815 0.909355\\n5 0.7958 0.777289 0.917419\\nThe 92% accuracy on the validation set looks quite good compared to the 94% that\\nthe BERT-base teacher achieves. Now that we’ve fine-tuned DistilBERT, let’s push the\\nmodel to the Hub so we can reuse it later:\\n224 | Chapter 8: Making Transformers Efficient in Production'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 248, 'page_label': '225'}, page_content='distilbert_trainer.push_to_hub(\"Training completed!\")\\nWith our model now safely stored on the Hub, we can immediately use it in a pipeline\\nfor our performance benchmark:\\nfinetuned_ckpt = \"transformersbook/distilbert-base-uncased-finetuned-clinc\"\\npipe = pipeline(\"text-classification\", model=finetuned_ckpt)\\nWe can then pass this pipeline to our PerformanceBenchmark class to compute the\\nmetrics associated with this model:\\noptim_type = \"DistilBERT\"\\npb = PerformanceBenchmark(pipe, clinc[\"test\"], optim_type=optim_type)\\nperf_metrics.update(pb.run_benchmark())\\nModel size (MB) - 255.89\\nAverage latency (ms) - 27.53 +\\\\- 0.60\\nAccuracy on test set - 0.858\\nTo compare these results against our baseline, let’s create a scatter plot of the accuracy\\nagainst the latency, with the radius of each point corresponding to the size of the\\nmodel on disk. The following function does what we need and marks the current\\noptimization type as a dashed circle to aid the comparison to previous results:\\nimport pandas as pd\\ndef plot_metrics(perf_metrics, current_optim_type):\\n    df = pd.DataFrame.from_dict(perf_metrics, orient=\\'index\\')\\n    for idx in df.index:\\n        df_opt = df.loc[idx]\\n        # Add a dashed circle around the current optimization type\\n        if idx == current_optim_type:\\n            plt.scatter(df_opt[\"time_avg_ms\"], df_opt[\"accuracy\"] * 100,\\n                        alpha=0.5, s=df_opt[\"size_mb\"], label=idx,\\n                        marker=\\'$\\\\u25CC$\\')\\n        else:\\n            plt.scatter(df_opt[\"time_avg_ms\"], df_opt[\"accuracy\"] * 100,\\n                        s=df_opt[\"size_mb\"], label=idx, alpha=0.5)\\n    legend = plt.legend(bbox_to_anchor=(1,1))\\n    for handle in legend.legendHandles:\\n        handle.set_sizes([20])\\n    plt.ylim(80,90)\\n    # Use the slowest model to define the x-axis range\\n    xlim = int(perf_metrics[\"BERT baseline\"][\"time_avg_ms\"] + 3)\\n    plt.xlim(1, xlim)\\n    plt.ylabel(\"Accuracy (%)\")\\n    plt.xlabel(\"Average latency (ms)\")\\n    plt.show()\\nplot_metrics(perf_metrics, optim_type)\\nMaking Models Smaller via Knowledge Distillation | 225'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 249, 'page_label': '226'}, page_content='12 T. Akiba et al., “Optuna: A Next-Generation Hyperparameter Optimization Framework”, (2019).\\nFrom the plot we can see that by using a smaller model we’ve managed to signifi‐\\ncantly decrease the average latency. And all this at the price of just over a 1% reduc‐\\ntion in accuracy! Let’s see if we can close that last gap by including the distillation loss\\nof the teacher and finding good values for α and T.\\nFinding Good Hyperparameters with Optuna\\nTo find good values for α and T, we could do a grid search over the 2D parameter\\nspace. But a much better alternative is to use Optuna,12 which is an optimization\\nframework designed for just this type of task. Optuna formulates the search problem\\nin terms of an objective function that is optimized through multiple trials. For exam‐\\nple, suppose we wished to minimize Rosenbrock’s “banana function”:\\nf x, y = 1 − x 2 + 100 y − x2 2\\nwhich is a famous test case for optimization frameworks. As shown in Figure 8-5, the\\nfunction gets its name from the curved contours and has a global minimum at\\nx, y = 1, 1 . Finding the valley is an easy optimization problem, but converging to\\nthe global minimum is not.\\n226 | Chapter 8: Making Transformers Efficient in Production'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 250, 'page_label': '227'}, page_content='Figure 8-5. Plot of the Rosenbrock function of two variables\\nIn Optuna, we can find the minimum of fx,y by defining an objective() function\\nthat returns the value of fx,y:\\ndef objective(trial):\\n    x = trial.suggest_float(\"x\", -2, 2)\\n    y = trial.suggest_float(\"y\", -2, 2)\\n    return (1 - x) ** 2 + 100 * (y - x ** 2) ** 2\\nThe trial.suggest_float object specifies the parameter ranges to sample uniformly\\nfrom; Optuna also provides suggest_int and suggest_categorical for integer and\\ncategorical parameters, respectively. Optuna collects multiple trials as a study, so to\\ncreate one we just pass the objective() function to study.optimize() as follows:\\nimport optuna\\nstudy = optuna.create_study()\\nstudy.optimize(objective, n_trials=1000)\\nOnce the study is completed, we can then find the best parameters as follows:\\nstudy.best_params\\n{\\'x\\': 1.003024865971437, \\'y\\': 1.00315167589307}\\nWe see that with one thousand trials, Optuna has managed to find values for x and y\\nthat are reasonably close to the global minimum. To use Optuna in \\n  Transformers,\\nwe use similar logic by first defining the hyperparameter space that we wish to opti‐\\nmize over. In addition to α and T, we’ll include the number of training epochs as\\nfollows:\\nMaking Models Smaller via Knowledge Distillation | 227'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 251, 'page_label': '228'}, page_content='def hp_space(trial):\\n    return {\"num_train_epochs\": trial.suggest_int(\"num_train_epochs\", 5, 10),\\n        \"alpha\": trial.suggest_float(\"alpha\", 0, 1),\\n        \"temperature\": trial.suggest_int(\"temperature\", 2, 20)}\\nRunning the hyperparameter search with the Trainer is then quite simple; we just\\nneed to specify the number of trials to run and a direction to optimize for. Because we\\nwant the best possible accuracy, we specify direction=\"maximize\" in the hyper\\nparameter_search() method of the trainer and pass the hyperparameter search\\nspace as follows:\\nbest_run = distilbert_trainer.hyperparameter_search(\\n    n_trials=20, direction=\"maximize\", hp_space=hp_space)\\nThe hyperparameter_search() method returns a BestRun object, which contains the\\nvalue of the objective that was maximized (by default, the sum of all metrics) and the\\nhyperparameters it used for that run:\\nprint(best_run)\\nBestRun(run_id=\\'1\\', objective=0.927741935483871,\\nhyperparameters={\\'num_train_epochs\\': 10, \\'alpha\\': 0.12468168730193585,\\n\\'temperature\\': 7})\\nThis value of α tells us that most of the training signal is coming from the knowledge\\ndistillation term. Let’s update our training arguments with these values and run the\\nfinal training run:\\nfor k,v in best_run.hyperparameters.items():\\n    setattr(student_training_args, k, v)\\n# Define a new repository to store our distilled model\\ndistilled_ckpt = \"distilbert-base-uncased-distilled-clinc\"\\nstudent_training_args.output_dir = distilled_ckpt\\n# Create a new Trainer with optimal parameters\\ndistil_trainer = DistillationTrainer(model_init=student_init,\\n    teacher_model=teacher_model, args=student_training_args,\\n    train_dataset=clinc_enc[\\'train\\'], eval_dataset=clinc_enc[\\'validation\\'],\\n    compute_metrics=compute_metrics, tokenizer=student_tokenizer)\\ndistil_trainer.train();\\n228 | Chapter 8: Making Transformers Efficient in Production'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 252, 'page_label': '229'}, page_content='Epoch Training Loss Validation Loss Accuracy\\n1 0.9031 0.574540 0.736452\\n2 0.4481 0.285621 0.874839\\n3 0.2528 0.179766 0.918710\\n4 0.1760 0.139828 0.929355\\n5 0.1416 0.121053 0.934839\\n6 0.1243 0.111640 0.934839\\n7 0.1133 0.106174 0.937742\\n8 0.1075 0.103526 0.938710\\n9 0.1039 0.101432 0.938065\\n10 0.1018 0.100493 0.939355\\nRemarkably, we’ve been able to train the student to match the accuracy of the teacher,\\ndespite it having almost half the number of parameters! Let’s push the model to the\\nHub for future use:\\ndistil_trainer.push_to_hub(\"Training complete\")\\nBenchmarking Our Distilled Model\\nNow that we have an accurate student, let’s create a pipeline and redo our benchmark\\nto see how we perform on the test set:\\ndistilled_ckpt = \"transformersbook/distilbert-base-uncased-distilled-clinc\"\\npipe = pipeline(\"text-classification\", model=distilled_ckpt)\\noptim_type = \"Distillation\"\\npb = PerformanceBenchmark(pipe, clinc[\"test\"], optim_type=optim_type)\\nperf_metrics.update(pb.run_benchmark())\\nModel size (MB) - 255.89\\nAverage latency (ms) - 25.96 +\\\\- 1.63\\nAccuracy on test set - 0.868\\nTo put these results in context, let’s also visualize them with our plot_metrics()\\nfunction:\\nplot_metrics(perf_metrics, optim_type)\\nMaking Models Smaller via Knowledge Distillation | 229'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 253, 'page_label': '230'}, page_content='As expected, the model size and latency remain essentially unchanged compared to\\nthe DistilBERT benchmark, but the accuracy has improved and even surpassed the\\nperformance of the teacher! One way to interpret this surprising result is that the\\nteacher has likely not been fine-tuned as systematically as the student. This is great,\\nbut we can actually compress our distilled model even further using a technique\\nknown as quantization. That’s the topic of the next section.\\nMaking Models Faster with Quantization\\nWe’ve now seen that with knowledge distillation we can reduce the computational\\nand memory cost of running inference by transferring the information from a\\nteacher into a smaller student. Quantization takes a different approach; instead of\\nreducing the number of computations, it makes them much more efficient by repre‐\\nsenting the weights and activations with low-precision data types like 8-bit integer\\n(INT8) instead of the usual 32-bit floating point (FP32). Reducing the number of bits\\nmeans the resulting model requires less memory storage, and operations like matrix\\nmultiplication can be performed much faster with integer arithmetic. Remarkably,\\nthese performance gains can be realized with little to no loss in accuracy!\\n230 | Chapter 8: Making Transformers Efficient in Production'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 254, 'page_label': '231'}, page_content='A Primer on Floating-Point and Fixed-Point Numbers\\nMost transformers today are pretrained and fine-tuned with floating-point numbers\\n(usually FP32 or a mix of FP16 and FP32), since they provide the precision needed to\\naccommodate the very different ranges of weights, activations, and gradients. A\\nfloating-point number like FP32 represents a sequence of 32 bits that are grouped in\\nterms of a sign, exponent, and significand. The sign determines whether the number is\\npositive or negative, while the significand corresponds to the number of significant\\ndigits, which are scaled using the exponent in some fixed base (usually 2 for binary or\\n10 for decimal).\\nFor example, the number 137.035 can be expressed as a decimal floating-point num‐\\nber through the following arithmetic:\\n137 . 035 = − 1 0 × 1 . 37035 × 102\\nwhere the 1.37035 is the significand and 2 is the exponent of the base 10. Through the\\nexponent we can represent a wide range of real numbers, and the decimal or binary\\npoint can be placed anywhere relative to the significant digits (hence the name\\n“floating-point”).\\nHowever, once a model is trained, we only need the forward pass to run inference, so\\nwe can reduce the precision of the data types without impacting the accuracy too\\nmuch. For neural networks it is common to use a fixed-point format  for the low-\\nprecision data types, where real numbers are represented as B-bit integers that are\\nscaled by a common factor for all variables of the same type. For example, 137.035\\ncan be represented as the integer 137,035 that is scaled by 1/1,000. We can control the\\nrange and precision of a fixed-point number by adjusting the scaling factor.\\nThe basic idea behind quantization is that we can “discretize” the floating-point val‐\\nues f in each tensor by mapping their range [ f max, f min] into a smaller one\\n[qmax, qmin] of fixed-point numbers q, and linearly distributing all values in between.\\nMathematically, this mapping is described by the following equation:\\nf =\\nf max − f min\\nqmax − qmin\\nq − Z = S q − Z\\nwhere the scale factor S is a positive floating-point number and the constant Z has the\\nsame type as q and is called the zero point because it corresponds to the quantized\\nvalue of the floating-point value f= 0. Note that the map needs to be affine so that we\\nMaking Models Faster with Quantization | 231'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 255, 'page_label': '232'}, page_content='13 An affine map is just a fancy name for the y = Ax + b map that you’re familiar with in the linear layers of a\\nneural network.\\nget back floating-point numbers when we dequantize the fixed-point ones. 13 An illus‐\\ntration of the conversion is shown in Figure 8-6.\\nFigure 8-6. Quantizing floating-point numbers as unsigned 8-bit integers (courtesy of\\nManas Sahni)\\nNow, one of the main reasons why transformers (and deep neural networks more\\ngenerally) are prime candidates for quantization is that the weights and activations\\ntend to take values in relatively small ranges. This means we don’t have to squeeze the\\nwhole range of possible FP32 numbers into, say, the 28 = 256 numbers represented by\\nINT8. To see this, let’s pick out one of the attention weight matrices from our distilled\\nmodel and plot the frequency distribution of the values:\\nimport matplotlib.pyplot as plt\\nstate_dict = pipe.model.state_dict()\\nweights = state_dict[\"distilbert.transformer.layer.0.attention.out_lin.weight\"]\\nplt.hist(weights.flatten().numpy(), bins=250, range=(-0.3,0.3), edgecolor=\"C0\")\\nplt.show()\\n232 | Chapter 8: Making Transformers Efficient in Production'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 256, 'page_label': '233'}, page_content='As we can see, the values of the weights are distributed in the small range [ −0 . 1, 0 . 1]\\naround zero. Now, suppose we want to quantize this tensor as a signed 8-bit integer.\\nIn that case, the range of possible values for our integers is [ qmax, qmin] = [−128, 127].\\nThe zero point coincides with the zero of FP32 and the scale factor is calculated\\naccording to the previous equation:\\nzero_point = 0\\nscale = (weights.max() - weights.min()) / (127 - (-128))\\nTo obtain the quantized tensor, we just need to invert the mapping q = f /S + Z,\\nclamp the values, round them to the nearest integer, and represent the result in the\\ntorch.int8 data type using the Tensor.char() function:\\n(weights / scale + zero_point).clamp(-128, 127).round().char()\\ntensor([[ -5,  -8,   0,  ...,  -6,  -4,   8],\\n        [  8,   3,   1,  ...,  -4,   7,   0],\\n        [ -9,  -6,   5,  ...,   1,   5,  -3],\\n        ...,\\n        [  6,   0,  12,  ...,   0,   6,  -1],\\n        [  0,  -2, -12,  ...,  12,  -7, -13],\\n        [-13,  -1, -10,  ...,   8,   2,  -2]], dtype=torch.int8)\\nGreat, we’ve just quantized our first tensor! In PyTorch we can simplify the conver‐\\nsion by using the quantize_per_tensor() function together with a quantized data\\ntype, torch.qint, that is optimized for integer arithmetic operations:\\nfrom torch import quantize_per_tensor\\ndtype = torch.qint8\\nquantized_weights = quantize_per_tensor(weights, scale, zero_point, dtype)\\nquantized_weights.int_repr()\\ntensor([[ -5,  -8,   0,  ...,  -6,  -4,   8],\\n        [  8,   3,   1,  ...,  -4,   7,   0],\\n        [ -9,  -6,   5,  ...,   1,   5,  -3],\\n        ...,\\n        [  6,   0,  12,  ...,   0,   6,  -1],\\n        [  0,  -2, -12,  ...,  12,  -7, -13],\\n        [-13,  -1, -10,  ...,   8,   2,  -2]], dtype=torch.int8)\\nThe plot in Figure 8-7  shows very clearly the discretization that’s induced by only\\nmapping some of the weight values precisely and rounding the rest.\\nMaking Models Faster with Quantization | 233'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 257, 'page_label': '234'}, page_content='Figure 8-7. Effect of quantization on a transformer’s weights\\nTo round out our little analysis, let’s compare how long it takes to compute the multi‐\\nplication of two weight tensors with FP32 and INT8 values. For the FP32 tensors, we\\ncan multiply them using PyTorch’s nifty @ operator:\\n%%timeit\\nweights @ weights\\n393 µs ± 3.84 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\\nFor the quantized tensors we need the QFunctional wrapper class so that we can per‐\\nform operations with the special torch.qint8 data type:\\nfrom torch.nn.quantized import QFunctional\\nq_fn = QFunctional()\\nThis class supports various elementary operations, like addition, and in our case we\\ncan time the multiplication of our quantized tensors as follows:\\n%%timeit\\nq_fn.mul(quantized_weights, quantized_weights)\\n23.3 µs ± 298 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)\\nCompared to our FP32 computation, using the INT8 tensors is almost 100 times\\nfaster! Even larger gains can be obtained by using dedicated backends for running\\nquantized operators efficiently. As of this book’s writing, PyTorch supports:\\n• x86 CPUs with AVX2 support or higher\\n• ARM CPUs (typically found in mobile/embedded devices)\\n234 | Chapter 8: Making Transformers Efficient in Production'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 258, 'page_label': '235'}, page_content='Since INT8 numbers have four times fewer bits than FP32 numbers, quantization also\\nreduces the memory storage requirements by up to a factor of four. In our simple\\nexample we can verify this by comparing the underlying storage size of our weight\\ntensor and its quantized cousin by using the Tensor.storage() function and the get\\nsizeof() function from Python’s sys module:\\nimport sys\\nsys.getsizeof(weights.storage()) / sys.getsizeof(quantized_weights.storage())\\n3.999633833760527\\nFor a full-scale transformer, the actual compression rate depends on which layers are\\nquantized (as we’ll see in the next section it is only the linear layers that typically get\\nquantized).\\nSo what’s the catch with quantization? Changing the precision for all computations in\\nour model introduces small disturbances at each point in the model’s computational\\ngraph, which can compound and affect the model’s performance. There are several\\nways to quantize a model, which all have pros and cons. For deep neural networks,\\nthere are typically three main approaches to quantization:\\nDynamic quantization\\nWhen using dynamic quantization nothing is changed during training and the\\nadaptations are only performed during inference. Like with all the quantization\\nmethods we will discuss, the weights of the model are converted to INT8 ahead\\nof inference time. In addition to the weights, the model’s activations are also\\nquantized. This approach is dynamic because the quantization happens on the fly.\\nThis means that all the matrix multiplications can be calculated with highly opti‐\\nmized INT8 functions. Of all the quantization methods discussed here, dynamic\\nquantization is the simplest one. However, with dynamic quantization the activa‐\\ntions are written and read to memory in floating-point format. This conversion\\nbetween integer and floating point can be a performance bottleneck.\\nStatic quantization\\nInstead of computing the quantization of the activations on the fly, we can avoid\\nthe conversion to floating point by precomputing the quantization scheme. Static\\nquantization achieves this by observing the activation patterns on a representa‐\\ntive sample of the data ahead of inference time. The ideal quantization scheme is\\ncalculated and then saved. This enables us to skip the conversion between INT8\\nand FP32 values and speeds up the computations. However, it requires access to a\\ngood data sample and introduces an additional step in the pipeline, since we now\\nneed to train and determine the quantization scheme before we can perform\\ninference. There is also one aspect that static quantization does not address: the\\ndiscrepancy between the precision during training and inference, which leads to\\na performance drop in the model’s metrics (e.g., accuracy).\\nMaking Models Faster with Quantization | 235'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 259, 'page_label': '236'}, page_content='Quantization-aware training\\nThe effect of quantization can be effectively simulated during training by “fake”\\nquantization of the FP32 values. Instead of using INT8 values during training,\\nthe FP32 values are rounded to mimic the effect of quantization. This is done\\nduring both the forward and the backward pass and improves performance in\\nterms of model metrics over static and dynamic quantization.\\nThe main bottleneck for running inference with transformers is the compute and\\nmemory bandwidth associated with the enormous numbers of weights in these mod‐\\nels. For this reason, dynamic quantization is currently the best approach for\\ntransformer-based models in NLP . In smaller computer vision models the limiting\\nfactor is the memory bandwidth of the activations, which is why static quantization is\\ngenerally used (or quantization-aware training in cases where the performance drops\\nare too significant).\\nImplementing dynamic quantization in PyTorch is quite simple and can be done with\\na single line of code:\\nfrom torch.quantization import quantize_dynamic\\nmodel_ckpt = \"transformersbook/distilbert-base-uncased-distilled-clinc\"\\ntokenizer = AutoTokenizer.from_pretrained(model_ckpt)\\nmodel = (AutoModelForSequenceClassification\\n         .from_pretrained(model_ckpt).to(\"cpu\"))\\nmodel_quantized = quantize_dynamic(model, {nn.Linear}, dtype=torch.qint8)\\nHere we pass to quantize_dynamic() the full-precision model and specify the set of\\nPyTorch layer classes in that model that we want to quantize. The dtype argument\\nspecifies the target precision and can be fp16 or qint8. A good practice is to pick the\\nlowest precision that you can tolerate with respect to your evaluation metrics. In this\\nchapter we’ll use INT8, which as we’ll soon see has little impact on our model’s\\naccuracy.\\nBenchmarking Our Quantized Model\\nWith our model now quantized, let’s pass it through the benchmark and visualize the\\nresults:\\npipe = pipeline(\"text-classification\", model=model_quantized,\\n                tokenizer=tokenizer)\\noptim_type = \"Distillation + quantization\"\\npb = PerformanceBenchmark(pipe, clinc[\"test\"], optim_type=optim_type)\\nperf_metrics.update(pb.run_benchmark())\\nModel size (MB) - 132.40\\nAverage latency (ms) - 12.54 +\\\\- 0.73\\nAccuracy on test set - 0.876\\n236 | Chapter 8: Making Transformers Efficient in Production'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 260, 'page_label': '237'}, page_content='14 There is a separate standard called ONNX-ML that is designed for traditional machine learning models like\\nrandom forests and frameworks like Scikit-learn.\\nplot_metrics(perf_metrics, optim_type)\\nNice, the quantized model is almost half the size of our distilled one and has even\\ngained a slight accuracy boost! Let’s see if we can push our optimization to the limit\\nwith a powerful framework called the ONNX Runtime.\\nOptimizing Inference with ONNX and the ONNX Runtime\\nONNX is an open standard that defines a common set of operators and a common\\nfile format to represent deep learning models in a wide variety of frameworks, includ‐\\ning PyTorch and TensorFlow.14 When a model is exported to the ONNX format, these\\noperators are used to construct a computational graph (often called an intermediate\\nrepresentation) that represents the flow of data through the neural network. An exam‐\\nple of such a graph for BERT-base is shown in Figure 8-8, where each node receives\\nsome input, applies an operation like Add or Squeeze, and then feeds the output to the\\nnext set of nodes.\\nOptimizing Inference with ONNX and the ONNX Runtime | 237'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 261, 'page_label': '238'}, page_content='15 Other popular accelerators include NVIDIA ’s TensorRT and Apache TVM.\\n16 A fused operation involves merging one operator (usually an activation function) into another so that they\\ncan be executed together. For example, suppose we want to apply an activation f to a matrix product A × B.\\nNormally the result of the product needs to be written back to the GPU memory before the activation is com‐\\nputed. Operator fusion allows as to compute fA×B in a single step. Constant folding refers to the process\\nof evaluating constant expressions at compile time instead of runtime.\\nFigure 8-8. A section of the ONNX graph for BERT-base, visualized in Netron\\nBy exposing a graph with standardized operators and data types, ONNX makes it\\neasy to switch between frameworks. For example, a model trained in PyTorch can be\\nexported to ONNX format and then imported in TensorFlow (and vice versa).\\nWhere ONNX really shines is when it is coupled with a dedicated accelerator like\\nONNX Runtime , or ORT for short. 15 ORT provides tools to optimize the ONNX\\ngraph through techniques like operator fusion and constant folding, 16 and defines an\\ninterface to execution providers that allow you to run the model on different types of\\nhardware. This is a powerful abstraction. Figure 8-9 shows the high-level architecture\\nof the ONNX and ORT ecosystem.\\n238 | Chapter 8: Making Transformers Efficient in Production'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 262, 'page_label': '239'}, page_content='Figure 8-9. Architecture of the ONNX and ONNX Runtime ecosystem (courtesy of the\\nONNX Runtime team)\\nTo see ORT in action, the first thing we need to do is convert our distilled model into\\nthe ONNX format. The \\n  Transformers library has a built-in function called\\nconvert_graph_to_onnx.convert() that simplifies the process by taking the follow‐\\ning steps:\\n1. Initialize the model as a Pipeline.\\n2. Run placeholder inputs through the pipeline so that ONNX can record the com‐\\nputational graph.\\n3. Define dynamic axes to handle dynamic sequence lengths.\\n4. Save the graph with network parameters.\\nTo use this function, we first need to set some OpenMP environment variables for\\nONNX:\\nimport os\\nfrom psutil import cpu_count\\nos.environ[\"OMP_NUM_THREADS\"] = f\"{cpu_count()}\"\\nos.environ[\"OMP_WAIT_POLICY\"] = \"ACTIVE\"\\nOpenMP is an API designed for developing highly parallelized applications. The\\nOMP_NUM_THREADS environment variable sets the number of threads to use for parallel\\ncomputations in the ONNX Runtime, while OMP_WAIT_POLICY=ACTIVE specifies that\\nwaiting threads should be active (i.e., using CPU processor cycles).\\nNext, let’s convert our distilled model to the ONNX format. Here we need to specify\\nthe argument pipeline_name=\"text-classification\" since convert() wraps the\\nOptimizing Inference with ONNX and the ONNX Runtime | 239'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 263, 'page_label': '240'}, page_content='model in a \\n  Transformers pipeline() function during the conversion. In addition\\nto the model_ckpt, we also pass the tokenizer to initialize the pipeline:\\nfrom transformers.convert_graph_to_onnx import convert\\nmodel_ckpt = \"transformersbook/distilbert-base-uncased-distilled-clinc\"\\nonnx_model_path = Path(\"onnx/model.onnx\")\\nconvert(framework=\"pt\", model=model_ckpt, tokenizer=tokenizer,\\n        output=onnx_model_path, opset=12, pipeline_name=\"text-classification\")\\nONNX uses operator sets to group together immutable operator specifications, so\\nopset=12 corresponds to a specific version of the ONNX library.\\nNow that we have our model saved, we need to create an InferenceSession instance\\nto feed inputs to the model:\\nfrom onnxruntime import (GraphOptimizationLevel, InferenceSession,\\n                         SessionOptions)\\ndef create_model_for_provider(model_path, provider=\"CPUExecutionProvider\"):\\n    options = SessionOptions()\\n    options.intra_op_num_threads = 1\\n    options.graph_optimization_level = GraphOptimizationLevel.ORT_ENABLE_ALL\\n    session = InferenceSession(str(model_path), options, providers=[provider])\\n    session.disable_fallback()\\n    return session\\nonnx_model = create_model_for_provider(onnx_model_path)\\nNow when we call onnx_model.run(), we can get the class logits from the ONNX\\nmodel. Let’s test this out with an example from the test set. Since the output from\\nconvert() tells us that ONNX expects just the input_ids and attention_mask as\\ninputs, we need to drop the label column from our sample:\\ninputs = clinc_enc[\"test\"][:1]\\ndel inputs[\"labels\"]\\nlogits_onnx = onnx_model.run(None, inputs)[0]\\nlogits_onnx.shape\\n(1, 151)\\nOnce we have the logits, we can easily get the predicted label by taking the argmax:\\nnp.argmax(logits_onnx)\\n61\\nwhich indeed agrees with the ground truth label:\\nclinc_enc[\"test\"][0][\"labels\"]\\n61\\nThe ONNX model is not compatible with the text-classification pipeline, so we’ll\\ncreate our own class that mimics the core behavior:\\n240 | Chapter 8: Making Transformers Efficient in Production'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 264, 'page_label': '241'}, page_content='from scipy.special import softmax\\nclass OnnxPipeline:\\n    def __init__(self, model, tokenizer):\\n        self.model = model\\n        self.tokenizer = tokenizer\\n    def __call__(self, query):\\n        model_inputs = self.tokenizer(query, return_tensors=\"pt\")\\n        inputs_onnx = {k: v.cpu().detach().numpy()\\n                       for k, v in model_inputs.items()}\\n        logits = self.model.run(None, inputs_onnx)[0][0, :]\\n        probs = softmax(logits)\\n        pred_idx = np.argmax(probs).item()\\n        return [{\"label\": intents.int2str(pred_idx), \"score\": probs[pred_idx]}]\\nWe can then test this on our simple query to see if we recover the car_rental intent:\\npipe = OnnxPipeline(onnx_model, tokenizer)\\npipe(query)\\n[{\\'label\\': \\'car_rental\\', \\'score\\': 0.7848334}]\\nGreat, our pipeline works as expected. The next step is to create a performance\\nbenchmark for ONNX models. Here we can build on the work we did with the\\nPerformanceBenchmark class by simply overriding the compute_size() method and\\nleaving the compute_accuracy() and time_pipeline() methods intact. The reason\\nwe need to override the compute_size() method is that we cannot rely on the\\nstate_dict and torch.save() to measure a model’s size, since onnx_model is techni‐\\ncally an ONNX InferenceSession object that doesn’t have access to the attributes of\\nPyTorch’s nn.Module. In any case, the resulting logic is simple and can be imple‐\\nmented as follows:\\nclass OnnxPerformanceBenchmark(PerformanceBenchmark):\\n    def __init__(self, *args, model_path, **kwargs):\\n        super().__init__(*args, **kwargs)\\n        self.model_path = model_path\\n    def compute_size(self):\\n        size_mb = Path(self.model_path).stat().st_size / (1024 * 1024)\\n        print(f\"Model size (MB) - {size_mb:.2f}\")\\n        return {\"size_mb\": size_mb}\\nWith our new benchmark, let’s see how our distilled model performs when converted\\nto ONNX format:\\noptim_type = \"Distillation + ORT\"\\npb = OnnxPerformanceBenchmark(pipe, clinc[\"test\"], optim_type,\\n                              model_path=\"onnx/model.onnx\")\\nperf_metrics.update(pb.run_benchmark())\\nOptimizing Inference with ONNX and the ONNX Runtime | 241'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 265, 'page_label': '242'}, page_content='Model size (MB) - 255.88\\nAverage latency (ms) - 21.02 +\\\\- 0.55\\nAccuracy on test set - 0.868\\nplot_metrics(perf_metrics, optim_type)\\nRemarkably, converting to the ONNX format and using the ONNX Runtime has\\ngiven our distilled model (i.e. the “Distillation” circle in the plot) a boost in latency!\\nLet’s see if we can squeeze out a bit more performance by adding quantization to the\\nmix.\\nSimilar to PyTorch, ORT offers three ways to quantize a model: dynamic, static, and\\nquantization-aware training. As we did with PyTorch, we’ll apply dynamic quantiza‐\\ntion to our distilled model. In ORT, the quantization is applied through the\\nquantize_dynamic() function, which requires a path to the ONNX model to quan‐\\ntize, a target path to save the quantized model to, and the data type to reduce the\\nweights to:\\nfrom onnxruntime.quantization import quantize_dynamic, QuantType\\nmodel_input = \"onnx/model.onnx\"\\nmodel_output = \"onnx/model.quant.onnx\"\\nquantize_dynamic(model_input, model_output, weight_type=QuantType.QInt8)\\nNow that the model is quantized, let’s run it through our benchmark:\\nonnx_quantized_model = create_model_for_provider(model_output)\\npipe = OnnxPipeline(onnx_quantized_model, tokenizer)\\noptim_type = \"Distillation + ORT (quantized)\"\\npb = OnnxPerformanceBenchmark(pipe, clinc[\"test\"], optim_type,\\n                              model_path=model_output)\\nperf_metrics.update(pb.run_benchmark())\\n242 | Chapter 8: Making Transformers Efficient in Production'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 266, 'page_label': '243'}, page_content='Model size (MB) - 64.20\\nAverage latency (ms) - 9.24 +\\\\- 0.29\\nAccuracy on test set - 0.877\\nplot_metrics(perf_metrics, optim_type)\\nORT quantization has reduced the model size and latency by around 30% compared\\nto the model obtained from PyTorch quantization (the distillation + quantization\\nblob). One reason for this is that PyTorch only optimizes the nn.Linear modules,\\nwhile ONNX quantizes the embedding layer as well. From the plot we can also see\\nthat applying ORT quantization to our distilled model has provided an almost three-\\nfold gain compared to our BERT baseline!\\nThis concludes our analysis of techniques to speed up transformers for inference. We\\nhave seen that methods such as quantization reduce the model size by reducing the\\nprecision of the representation. Another strategy to reduce the size is to remove some\\nweights altogether. This technique is called weight pruning, and it’s the focus of the\\nnext section.\\nMaking Models Sparser with Weight Pruning\\nSo far we’ve seen that knowledge distillation and weight quantization are quite effec‐\\ntive at producing faster models for inference, but in some cases you might also have\\nstrong constraints on the memory footprint of your model. For example, if our prod‐\\nuct manager suddenly decides that our text assistant needs to be deployed on a\\nmobile device, then we’ll need our intent classifier to take up as little storage space as\\npossible. To round out our survey of compression methods, let’s take a look at how\\nwe can shrink the number of parameters in our model by identifying and removing\\nthe least important weights in the network.\\nMaking Models Sparser with Weight Pruning | 243'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 267, 'page_label': '244'}, page_content='Sparsity in Deep Neural Networks\\nAs shown in Figure 8-10, the main idea behind pruning is to gradually remove weight\\nconnections (and potentially neurons) during training such that the model becomes\\nprogressively sparser. The resulting pruned model has a smaller number of nonzero\\nparameters, which can then be stored in a compact sparse matrix format. Pruning can\\nbe also combined with quantization to obtain further compression.\\nFigure 8-10. Weights and neurons before and after pruning (courtesy of Song Han)\\nWeight Pruning Methods\\nMathematically, the way most weight pruning methods work is to calculate a matrix \\ud835\\nof importance scores and then select the top k percent of weights by importance:\\nTopk \\ud835 ij =\\n1 if Sij in topk %\\n0 otherwise\\nIn effect, k acts as a new hyperparameter to control the amount of sparsity in the\\nmodel—that is, the proportion of weights that are zero-valued. Lower values of k cor‐\\nrespond to sparser matrices. From these scores we can then define a mask matrix \\ud835\\nthat masks the weights Wij during the forward pass with some input xi and effectively\\ncreates a sparse network of activations ai:\\nai = ∑\\nk\\nWikMikxk\\n244 | Chapter 8: Making Transformers Efficient in Production'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 268, 'page_label': '245'}, page_content='17 B. Hassibi and D. Stork, “Second Order Derivatives for Network Pruning: Optimal Brain Surgeon, ” Proceed‐\\nings of the 5th International Conference on Neural Information Processing Systems (November 1992): 164–171,\\nhttps://papers.nips.cc/paper/1992/hash/303ed4c69846ab36c2904d3ba8573050-Abstract.html.\\n18 S. Han et al., “Learning Both Weights and Connections for Efficient Neural Networks”, (2015).\\n19 M. Zhu and S. Gupta, “To Prune, or Not to Prune: Exploring the Efficacy of Pruning for Model Compression”,\\n(2017).\\nAs discussed in the tongue-in-cheek “Optimal Brain Surgeon” paper, 17 at the heart of\\neach pruning method are a set of questions that need to be considered:\\n• Which weights should be eliminated?\\n• How should the remaining weights be adjusted for best performance?\\n• How can such network pruning be done in a computationally efficient way?\\nThe answers to these questions inform how the score matrix \\ud835 is computed, so let’s\\nbegin by looking at one of the earliest and most popular pruning methods: magnitude\\npruning.\\nMagnitude pruning\\nAs the name suggests, magnitude pruning calculates the scores according to the mag‐\\nnitude of the weights \\ud835 = ∣ Wij ∣ 1 ≤ j, j ≤ n and then derives the masks from\\n\\ud835 = Topk \\ud835 . In the literature it is common to apply magnitude pruning in an itera‐\\ntive fashion by first training the model to learn which connections are important and\\npruning the weights of least importance.18 The sparse model is then retrained and the\\nprocess repeated until the desired sparsity is reached.\\nOne drawback with this approach is that it is computationally demanding: at every\\nstep of pruning we need to train the model to convergence. For this reason it is gener‐\\nally better to gradually increase the initial sparsity si (which is usually zero) to a final\\nvalue sf after some number of steps N:19\\nst = sf + si − sf 1 −\\nt − t0\\nNΔt\\n3\\nfor t ∈ t0, t0 + Δt, ...,t0 + NΔt\\nHere the idea is to update the binary masks \\ud835 every Δt steps to allow masked weights\\nto reactivate during training and recover from any potential loss in accuracy that is\\ninduced by the pruning process. As shown in Figure 8-11 , the cubic factor implies\\nthat the rate of weight pruning is highest in the early phases (when the number of\\nredundant weights is large) and gradually tapers off.\\nMaking Models Sparser with Weight Pruning | 245'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 269, 'page_label': '246'}, page_content='20 V . Sanh, T. Wolf, and A.M. Rush, “Movement Pruning: Adaptive Sparsity by Fine-Tuning”, (2020).\\n21 There is also a “soft” version of movement pruning where instead of picking the top k% of weights, one uses a\\nglobal threshold τ to define the binary mask: \\ud835 = \\ud835 > τ .\\nFigure 8-11. The cubic sparsity scheduler used for pruning\\nOne problem with magnitude pruning is that it is really designed for pure supervised\\nlearning, where the importance of each weight is directly related to the task at hand.\\nBy contrast, in transfer learning the importance of the weights is primarily deter‐\\nmined by the pretraining phase, so magnitude pruning can remove connections that\\nare important for the fine-tuning task. Recently, an adaptive approach called move‐\\nment pruning has been proposed by Hugging Face researchers—let’s take a look.20\\nMovement pruning\\nThe basic idea behind movement pruning is to gradually remove weights during fine-\\ntuning such that the model becomes progressively sparser. The key novelty is that\\nboth the weights and the scores are learned during fine-tuning. So, instead of being\\nderived directly from the weights (like with magnitude pruning), the scores in move‐\\nment pruning are arbitrary and are learned through gradient descent like any other\\nneural network parameter. This implies that in the backward pass, we also track the\\ngradient of the loss L with respect to the scores Sij.\\nOnce the scores are learned, it is then straightforward to generate the binary mask\\nusing \\ud835 = Topk \\ud835 .21\\nThe intuition behind movement pruning is that the weights that are “moving” the\\nmost from zero are the most important ones to keep. In other words, the positive\\n246 | Chapter 8: Making Transformers Efficient in Production'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 270, 'page_label': '247'}, page_content='weights increase during fine-tuning (and vice versa for the negative weights), which is\\nequivalent to saying that the scores increase as the weights move away from zero. As\\nshown in Figure 8-12, this behavior differs from magnitude pruning, which selects as\\nthe most important weights those that are furthest from zero.\\nFigure 8-12. Comparison of weights removed during magnitude pruning (left) and\\nmovement pruning (right)\\nThese differences between the two pruning methods are also evident in the distribu‐\\ntion of the remaining weights. As shown in Figure 8-13, magnitude pruning produces\\ntwo clusters of weights, while movement pruning produces a smoother distribution.\\nAs of this book’s writing, \\n  Transformers does not support pruning methods out of\\nthe box. Fortunately, there is a nifty library called Neural Networks Block Movement\\nPruning that implements many of these ideas, and we recommend checking it out if\\nmemory constraints are a concern.\\nMaking Models Sparser with Weight Pruning | 247'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 271, 'page_label': '248'}, page_content='Figure 8-13. Distribution of remaining weights for magnitude pruning (MaP) and move‐\\nment pruning (MvP)\\nConclusion\\nWe’ve seen that optimizing transformers for deployment in production environments\\ninvolves compression along two dimensions: latency and memory footprint. Starting\\nfrom a fine-tuned model, we applied distillation, quantization, and optimizations\\nthrough ORT to significantly reduce both of these. In particular, we found that quan‐\\ntization and conversion in ORT gave the largest gains with minimal effort.\\nAlthough pruning is an effective strategy for reducing the storage size of transformer\\nmodels, current hardware is not optimized for sparse matrix operations, which limits\\nthe usefulness of this technique. However, this is an active area of research, and by the\\ntime this book hits the shelves many of these limitations may have been resolved.\\nSo where to from here? All of the techniques in this chapter can be adapted to other\\ntasks, such as question answering, named entity recognition, or language modeling. If\\nyou find yourself struggling to meet the latency requirements or your model is eating\\nup all your compute budget, we suggest giving one of them a try.\\nIn the next chapter, we’ll switch gears away from performance optimization and\\nexplore every data scientist’s worst nightmare: dealing with few to no labels.\\n248 | Chapter 8: Making Transformers Efficient in Production'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 272, 'page_label': '249'}, page_content='CHAPTER 9\\nDealing with Few to No Labels\\nThere is one question so deeply ingrained into every data scientist’s mind that it’s usu‐\\nally the first thing they ask at the start of a new project: is there any labeled data?\\nMore often than not, the answer is “no” or “a little bit, ” followed by an expectation\\nfrom the client that your team’s fancy machine learning models should still perform\\nwell. Since training models on very small datasets does not typically yield good\\nresults, one obvious solution is to annotate more data. However, this takes time and\\ncan be very expensive, especially if each annotation requires domain expertise to\\nvalidate.\\nFortunately, there are several methods that are well suited for dealing with few to no\\nlabels! Y ou may already be familiar with some of them, such as zero-shot or few-shot\\nlearning, as witnessed by GPT-3’s impressive ability to perform a diverse range of\\ntasks with just a few dozen examples.\\nIn general, the best-performing method will depend on the task, the amount of avail‐\\nable data, and what fraction of that data is labeled. The decision tree shown in\\nFigure 9-1  can help guide us through the process of picking the most appropriate\\nmethod.\\n249'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 273, 'page_label': '250'}, page_content='1 Q. Xie et al., “Unsupervised Data Augmentation for Consistency Training”, (2019); S. Mukherjee and A.H.\\nAwadallah, “Uncertainty-Aware Self-Training for Few-Shot Text Classification”, (2020).\\nFigure 9-1. Several techniques that can be used to improve model performance in the\\nabsence of large amounts of labeled data\\nLet’s walk through this decision tree step-by-step:\\n1. Do you have labeled data?\\nEven a handful of labeled samples can make a difference with regard to which\\nmethod works best. If you have no labeled data at all, you can start with the zero-\\nshot learning approach, which often sets a strong baseline to work from.\\n2. How many labels?\\nIf labeled data is available, the deciding factor is how much. If you have a lot of\\ntraining data available you can use the standard fine-tuning approach discussed\\nin Chapter 2.\\n3. Do you have unlabeled data?\\nIf you only have a handful of labeled samples it can help immensely if you have\\naccess to large amounts of unlabeled data. If you have access to unlabeled data\\nyou can either use it to fine-tune the language model on the domain before train‐\\ning a classifier, or you can use more sophisticated methods such as unsupervised\\ndata augmentation (UDA) or uncertainty-aware self-training (UST).1 If you don’t\\nhave any unlabeled data available, you don’t have the option of annotating more\\n250 | Chapter 9: Dealing with Few to No Labels'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 274, 'page_label': '251'}, page_content='data. In this case you can use few-shot learning or use the embeddings from a\\npretrained language model to perform lookups with a nearest neighbor search.\\nIn this chapter we’ll work our way through this decision tree by tackling a common\\nproblem facing many support teams that use issue trackers like Jira or GitHub to\\nassist their users: tagging issues with metadata based on the issue’s description. These\\ntags might define the issue type, the product causing the problem, or which team is\\nresponsible for handling the reported issue. Automating this process can have a big\\nimpact on productivity and enables the support teams to focus on helping their users.\\nAs a running example, we’ll use the GitHub issues associated with a popular open\\nsource project: \\n  Transformers! Let’s now take a look at what information is con‐\\ntained in these issues, how to frame the task, and how to get the data.\\nThe methods presented in this chapter work well for text classifica‐\\ntion, but other techniques such as data augmentation may be nec‐\\nessary for tackling more complex tasks like named entity\\nrecognition, question answering, or summarization.\\nBuilding a GitHub Issues Tagger\\nIf you navigate to the Issues tab of the \\n  Transformers repository, you’ll find issues\\nlike the one shown in Figure 9-2, which contains a title, a description, and a set of\\ntags or labels that characterize the issue. This suggests a natural way to frame the\\nsupervised learning task: given a title and description of an issue, predict one or more\\nlabels. Since each issue can be assigned a variable number of labels, this means we are\\ndealing with a multilabel text classification problem. This is usually more challenging\\nthan the multiclass problem that we encountered in Chapter 2, where each tweet was\\nassigned to only one emotion.\\nBuilding a GitHub Issues Tagger | 251'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 275, 'page_label': '252'}, page_content='Figure 9-2. A typical GitHub issue on the \\n  Transformers repository\\nNow that we’ve seen what the GitHub issues look like, let’s see how we can download\\nthem to create our dataset.\\nGetting the Data\\nTo grab all the repository’s issues, we’ll use the GitHub REST API to poll the Issues\\nendpoint. This endpoint returns a list of JSON objects, with each containing a large\\nnumber of fields about the issue at hand, including its state (open or closed), who\\nopened the issue, as well as the title, body, and labels we saw in Figure 9-2.\\nSince it takes a while to fetch all the issues, we’ve included a github-issues-\\ntransformers.jsonl file in this book’s GitHub repository, along with a fetch_issues()\\nfunction that you can use to download them yourself.\\n252 | Chapter 9: Dealing with Few to No Labels'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 276, 'page_label': '253'}, page_content='The GitHub REST API treats pull requests as issues, so our dataset\\ncontains a mix of both. To keep things simple, we’ll develop our\\nclassifier for both types of issue, although in practice you might\\nconsider building two separate classifiers to have more fine-grained\\ncontrol over the model’s performance.\\nNow that we know how to grab the data, let’s take a look at cleaning it up.\\nPreparing the Data\\nOnce we’ve downloaded all the issues, we can load them using Pandas:\\nimport pandas as pd\\ndataset_url = \"https://git.io/nlp-with-transformers\"\\ndf_issues = pd.read_json(dataset_url, lines=True)\\nprint(f\"DataFrame shape: {df_issues.shape}\")\\nDataFrame shape: (9930, 26)\\nThere are almost 10,000 issues in our dataset, and by looking at a single row we can\\nsee that the information retrieved from the GitHub API contains many fields such as\\nURLs, IDs, dates, users, title, body, as well as labels:\\ncols = [\"url\", \"id\", \"title\", \"user\", \"labels\", \"state\", \"created_at\", \"body\"]\\ndf_issues.loc[2, cols].to_frame()\\n2\\nurl https://api.github.com/repos/huggingface/trans...\\nid 849529761\\ntitle [DeepSpeed] ZeRO stage 3 integration: getting ...\\nuser {\\'login’: ’stas00\\', ‘id’: 10676103, ‘node_id’:...\\nlabels [{\\'id’: 2659267025, ‘node_id’: ‘MDU6TGFiZWwyNj...\\nstate open\\ncreated_at 2021-04-02 23:40:42\\nbody **[This is not yet alive, preparing for the re...\\nThe labels column is the thing that we’re interested in, and each row contains a list\\nof JSON objects with metadata about each label:\\n[\\n   {\\n      \"id\":2659267025,\\n      \"node_id\":\"MDU6TGFiZWwyNjU5MjY3MDI1\",\\n      \"url\":\"https://api.github.com/repos/huggingface...\",\\n      \"name\":\"DeepSpeed\",\\n      \"color\":\"4D34F7\",\\nBuilding a GitHub Issues Tagger | 253'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 277, 'page_label': '254'}, page_content='\"default\":false,\\n      \"description\":\"\"\\n   }\\n]\\nFor our purposes, we’re only interested in the name field of each label object, so let’s\\noverwrite the labels column with just the label names:\\ndf_issues[\"labels\"] = (df_issues[\"labels\"]\\n                       .apply(lambda x: [meta[\"name\"] for meta in x]))\\ndf_issues[[\"labels\"]].head()\\nlabels\\n0 []\\n1 []\\n2 [DeepSpeed]\\n3 []\\n4 []\\nNow each row in the labels column is a list of GitHub labels, so we can compute the\\nlength of each row to find the number of labels per issue:\\ndf_issues[\"labels\"].apply(lambda x : len(x)).value_counts().to_frame().T\\n0 1 2 3 4 5\\nlabels 6440 3057 305 100 25 3\\nThis shows that the majority of issues have zero or one label, and much fewer have\\nmore than one. Next let’s take a look at the top 10 most frequent labels in the dataset.\\nIn Pandas we can do this by “exploding” the labels column so that each label in the\\nlist becomes a row, and then simply counting the occurrences of each label:\\ndf_counts = df_issues[\"labels\"].explode().value_counts()\\nprint(f\"Number of labels: {len(df_counts)}\")\\n# Display the top-8 label categories\\ndf_counts.to_frame().head(8).T\\nNumber of labels: 65\\nwontfix model\\ncard\\nCore:\\nTokenization\\nNew\\nmodel\\nCore:\\nModeling\\nHelp\\nwanted\\nGood First\\nIssue\\nUsage\\nlabels 2284 649 106 98 64 52 50 46\\nWe can see that there are 65 unique labels in the dataset and that the classes are very\\nimbalanced, with wontfix and model card being the most common labels. To make\\nthe classification problem more tractable, we’ll focus on building a tagger for a subset\\n254 | Chapter 9: Dealing with Few to No Labels'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 278, 'page_label': '255'}, page_content='of the labels. For example, some labels, such as Good First Issue or Help Wanted,\\nare potentially very difficult to predict from the issue’s description, while others, such\\nas model card, could be classified with a simple rule that detects when a model card\\nis added on the Hugging Face Hub.\\nThe following code filters the dataset for the subset of labels that we’ll work with,\\nalong with a standardization of the names to make them easier to read:\\nlabel_map = {\"Core: Tokenization\": \"tokenization\",\\n             \"New model\": \"new model\",\\n             \"Core: Modeling\": \"model training\",\\n             \"Usage\": \"usage\",\\n             \"Core: Pipeline\": \"pipeline\",\\n             \"TensorFlow\": \"tensorflow or tf\",\\n             \"PyTorch\": \"pytorch\",\\n             \"Examples\": \"examples\",\\n             \"Documentation\": \"documentation\"}\\ndef filter_labels(x):\\n    return [label_map[label] for label in x if label in label_map]\\ndf_issues[\"labels\"] = df_issues[\"labels\"].apply(filter_labels)\\nall_labels = list(label_map.values())\\nNow let’s look at the distribution of the new labels:\\ndf_counts = df_issues[\"labels\"].explode().value_counts()\\ndf_counts.to_frame().T\\ntokenization new\\nmodel\\nmodel\\ntraining\\nusage pipeline tensorflow\\nor tf\\npytorch documentation examples\\nlabels 106 98 64 46 42 41 37 28 24\\nLater in this chapter we’ll find it useful to treat the unlabeled issues as a separate\\ntraining split, so let’s create a new column that indicates whether the issue is unla‐\\nbeled or not:\\ndf_issues[\"split\"] = \"unlabeled\"\\nmask = df_issues[\"labels\"].apply(lambda x: len(x)) > 0\\ndf_issues.loc[mask, \"split\"] = \"labeled\"\\ndf_issues[\"split\"].value_counts().to_frame()\\nsplit\\nunlabeled 9489\\nlabeled 441\\nBuilding a GitHub Issues Tagger | 255'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 279, 'page_label': '256'}, page_content='Let’s now take a look at an example:\\nfor column in [\"title\", \"body\", \"labels\"]:\\n    print(f\"{column}: {df_issues[column].iloc[26][:500]}\\\\n\")\\ntitle: Add new CANINE model\\nbody: # \\n  New model addition\\n## Model description\\nGoogle recently proposed a new **C**haracter **A**rchitecture with **N**o\\n tokenization **I**n **N**eural **E**ncoders architecture (CANINE). Not only\\n the title is exciting:\\nPipelined NLP systems have largely been superseded by end-to-end neural\\n modeling, yet nearly all commonly-used models still require an explicit\\n tokenization step. While recent tokenization approaches based on data-derived\\n subword lexicons are less brittle than manually en\\nlabels: [\\'new model\\']\\nIn this example a new model architecture is proposed, so the new model tag makes\\nsense. We can also see that the title contains information that will be useful for our\\nclassifier, so let’s concatenate it with the issue’s description in the body field:\\ndf_issues[\"text\"] = (df_issues\\n                     .apply(lambda x: x[\"title\"] + \"\\\\n\\\\n\" + x[\"body\"], axis=1))\\nBefore we look at the rest of the data, let’s check for any duplicates in the data and\\ndrop them with the drop_duplicates() method:\\nlen_before = len(df_issues)\\ndf_issues = df_issues.drop_duplicates(subset=\"text\")\\nprint(f\"Removed {(len_before-len(df_issues))/len_before:.2%} duplicates.\")\\nRemoved 1.88% duplicates.\\nWe can see that there were a few duplicate issues in our dataset, but they only repre‐\\nsented a small percentage. As we’ve done in other chapters, it’s also a good idea to\\nhave a quick look at the number of words in our texts to see if we’ll lose much infor‐\\nmation when we truncate to each model’s context size:\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\n(df_issues[\"text\"].str.split().apply(len)\\n .hist(bins=np.linspace(0, 500, 50), grid=False, edgecolor=\"C0\"))\\nplt.title(\"Words per issue\")\\nplt.xlabel(\"Number of words\")\\nplt.ylabel(\"Number of issues\")\\nplt.show()\\n256 | Chapter 9: Dealing with Few to No Labels'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 280, 'page_label': '257'}, page_content='The distribution has the long tail characteristic of many text datasets. Most of the\\ntexts are fairly short, but there are also issues with more than 500 words. It is com‐\\nmon to have some very long issues, especially when error messages and code snippets\\nare posted along with them. Given that most transformer models have a context size\\nof 512 tokens or larger, truncating a handful of long issues is not likely to affect the\\noverall performance. Now that we’ve explored and cleaned up our dataset, the final\\nthing to do is define our training and validation sets to benchmark our classifiers.\\nLet’s take a look at how to do this.\\nCreating Training Sets\\nCreating training and validation sets is a bit trickier for multlilabel problems because\\nthere is no guaranteed balance for all labels. However, it can be approximated, and we\\ncan use the Scikit-multilearn library, which is specifically set up for this purpose. The\\nfirst thing we need to do is transform our set of labels, like pytorch and tokeniza\\ntion, into a format that the model can process. Here we can use Scikit-learn’s Multi\\nLabelBinarizer class, which takes a list of label names and creates a vector with\\nzeros for absent labels and ones for present labels. We can test this by fitting Multi\\nLabelBinarizer on all_labels to learn the mapping from label name to ID as\\nfollows:\\nfrom sklearn.preprocessing import MultiLabelBinarizer\\nmlb = MultiLabelBinarizer()\\nmlb.fit([all_labels])\\nmlb.transform([[\"tokenization\", \"new model\"], [\"pytorch\"]])\\nBuilding a GitHub Issues Tagger | 257'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 281, 'page_label': '258'}, page_content='array([[0, 0, 0, 1, 0, 0, 0, 1, 0],\\n       [0, 0, 0, 0, 0, 1, 0, 0, 0]])\\nIn this simple example we can see the first row has two ones corresponding to the\\ntokenization and new model labels, while the second row has just one hit with\\npytorch.\\nTo create the splits we can use the iterative_train_test_split() function from\\nScikit-multilearn, which creates the train/test splits iteratively to achieve balanced\\nlabels. We wrap it in a function that we can apply to DataFrames. Since the function\\nexpects a two-dimensional feature matrix, we need to add a dimension to the possible\\nindices before making the split:\\nfrom skmultilearn.model_selection import iterative_train_test_split\\ndef balanced_split(df, test_size=0.5):\\n    ind = np.expand_dims(np.arange(len(df)), axis=1)\\n    labels = mlb.transform(df[\"labels\"])\\n    ind_train, _, ind_test, _ = iterative_train_test_split(ind, labels,\\n                                                           test_size)\\n    return df.iloc[ind_train[:, 0]], df.iloc[ind_test[:,0]]\\nArmed with the balanced_split() function, we can split the data into supervised\\nand unsupervised datasets, and then create balanced training, validation, and test sets\\nfor the supervised part:\\nfrom sklearn.model_selection import train_test_split\\ndf_clean = df_issues[[\"text\", \"labels\", \"split\"]].reset_index(drop=True).copy()\\ndf_unsup = df_clean.loc[df_clean[\"split\"] == \"unlabeled\", [\"text\", \"labels\"]]\\ndf_sup = df_clean.loc[df_clean[\"split\"] == \"labeled\", [\"text\", \"labels\"]]\\nnp.random.seed(0)\\ndf_train, df_tmp = balanced_split(df_sup, test_size=0.5)\\ndf_valid, df_test = balanced_split(df_tmp, test_size=0.5)\\nFinally, let’s create a DatasetDict with all the splits so that we can easily tokenize the\\ndataset and integrate with the Trainer. Here we’ll use the nifty from_pandas()\\nmethod to load each split directly from the corresponding Pandas DataFrame:\\nfrom datasets import Dataset, DatasetDict\\nds = DatasetDict({\\n    \"train\": Dataset.from_pandas(df_train.reset_index(drop=True)),\\n    \"valid\": Dataset.from_pandas(df_valid.reset_index(drop=True)),\\n    \"test\": Dataset.from_pandas(df_test.reset_index(drop=True)),\\n    \"unsup\": Dataset.from_pandas(df_unsup.reset_index(drop=True))})\\nThis looks good, so the last thing to do is to create some training slices so that we can\\nevaluate the performance of each classifier as a function of the training set size.\\n258 | Chapter 9: Dealing with Few to No Labels'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 282, 'page_label': '259'}, page_content='Creating Training Slices\\nThe dataset has the two characteristics that we’ d like to investigate in this chapter:\\nsparse labeled data and multilabel classification. The training set consists of only 220\\nexamples to train with, which is certainly a challenge even with transfer learning. To\\ndrill down into how each method in this chapter performs with little labeled data,\\nwe’ll also create slices of the training data with even fewer samples. We can then plot\\nthe number of samples against the performance and investigate various regimes. We’ll\\nstart with only eight samples per label and build up until the slice covers the full\\ntraining set using the iterative_train_test_split() function:\\nnp.random.seed(0)\\nall_indices = np.expand_dims(list(range(len(ds[\"train\"]))), axis=1)\\nindices_pool = all_indices\\nlabels = mlb.transform(ds[\"train\"][\"labels\"])\\ntrain_samples = [8, 16, 32, 64, 128]\\ntrain_slices, last_k = [], 0\\nfor i, k in enumerate(train_samples):\\n    # Split off samples necessary to fill the gap to the next split size\\n    indices_pool, labels, new_slice, _ = iterative_train_test_split(\\n        indices_pool, labels, (k-last_k)/len(labels))\\n    last_k = k\\n    if i==0: train_slices.append(new_slice)\\n    else: train_slices.append(np.concatenate((train_slices[-1], new_slice)))\\n# Add full dataset as last slice\\ntrain_slices.append(all_indices), train_samples.append(len(ds[\"train\"]))\\ntrain_slices = [np.squeeze(train_slice) for train_slice in train_slices]\\nNote that this iterative approach only approximately splits the samples to the desired\\nsize, since it is not always possible to find a balanced split at a given split size:\\nprint(\"Target split sizes:\")\\nprint(train_samples)\\nprint(\"Actual split sizes:\")\\nprint([len(x) for x in train_slices])\\nTarget split sizes:\\n[8, 16, 32, 64, 128, 223]\\nActual split sizes:\\n[10, 19, 36, 68, 134, 223]\\nWe’ll use the specified split sizes as the labels for the following plots. Great, we’ve\\nfinally prepared our dataset into training splits—let’s next take a look at training a\\nstrong baseline model!\\nBuilding a GitHub Issues Tagger | 259'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 283, 'page_label': '260'}, page_content='Implementing a Naive Bayesline\\nWhenever you start a new NLP project, it’s always a good idea to implement a set of\\nstrong baselines. There are two main reasons for this:\\n1. A baseline based on regular expressions, handcrafted rules, or a very simple\\nmodel might already work really well to solve the problem. In these cases, there is\\nno reason to bring out big guns like transformers, which are generally more com‐\\nplex to deploy and maintain in production environments.\\n2. The baselines provide quick checks as you explore more complex models. For\\nexample, suppose you train BERT-large and get an accuracy of 80% on your vali‐\\ndation set. Y ou might write it off as a hard dataset and call it a day. But what if\\nyou knew that a simple classifier like logistic regression gets 95% accuracy? That\\nwould raise your suspicions and prompt you to debug your model.\\nSo let’s start our analysis by training a baseline model. For text classification, a great\\nbaseline is a Naive Bayes classifier as it is very simple, quick to train, and fairly robust\\nto perturbations in the inputs. The Scikit-learn implementation of Naive Bayes does\\nnot support multilabel classification out of the box, but fortunately we can again use\\nthe Scikit-multilearn library to cast the problem as a one-versus-rest classification\\ntask where we train L binary classifiers for L labels. First, let’s use a multilabel binar‐\\nizer to create a new label_ids column in our training sets. We can use the map()\\nfunction to take care of all the processing in one go:\\ndef prepare_labels(batch):\\n    batch[\"label_ids\"] = mlb.transform(batch[\"labels\"])\\n    return batch\\nds = ds.map(prepare_labels, batched=True)\\nTo measure the performance of our classifiers, we’ll use the micro and macro\\nF1-scores, where the former tracks performance on the frequent labels and the latter\\non all labels disregarding the frequency. Since we’ll be evaluating each model across\\ndifferent-sized training splits, let’s create a defaultdict with a list to store the scores\\nper split:\\nfrom collections import defaultdict\\nmacro_scores, micro_scores = defaultdict(list), defaultdict(list)\\nNow we’re finally ready to train our baseline! Here’s the code to train the model and\\nevaluate our classifier across increasing training set sizes:\\nfrom sklearn.naive_bayes import MultinomialNB\\nfrom sklearn.metrics import classification_report\\nfrom skmultilearn.problem_transform import BinaryRelevance\\nfrom sklearn.feature_extraction.text import CountVectorizer\\n260 | Chapter 9: Dealing with Few to No Labels'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 284, 'page_label': '261'}, page_content='for train_slice in train_slices:\\n    # Get training slice and test data\\n    ds_train_sample = ds[\"train\"].select(train_slice)\\n    y_train = np.array(ds_train_sample[\"label_ids\"])\\n    y_test = np.array(ds[\"test\"][\"label_ids\"])\\n    # Use a simple count vectorizer to encode our texts as token counts\\n    count_vect = CountVectorizer()\\n    X_train_counts = count_vect.fit_transform(ds_train_sample[\"text\"])\\n    X_test_counts = count_vect.transform(ds[\"test\"][\"text\"])\\n    # Create and train our model!\\n    classifier = BinaryRelevance(classifier=MultinomialNB())\\n    classifier.fit(X_train_counts, y_train)\\n    # Generate predictions and evaluate\\n    y_pred_test = classifier.predict(X_test_counts)\\n    clf_report = classification_report(\\n        y_test, y_pred_test, target_names=mlb.classes_, zero_division=0,\\n        output_dict=True)\\n    # Store metrics\\n    macro_scores[\"Naive Bayes\"].append(clf_report[\"macro avg\"][\"f1-score\"])\\n    micro_scores[\"Naive Bayes\"].append(clf_report[\"micro avg\"][\"f1-score\"])\\nThere’s quite a lot going on in this block of code, so let’s unpack it. First, we get the\\ntraining slice and encode the labels. Then we use a count vectorizer to encode the\\ntexts by simply creating a vector of the size of the vocabulary where each entry corre‐\\nsponds to the frequency with which a token appeared in the text. This is called a bag-\\nof-words approach, since all information on the order of the words is lost. Then we\\ntrain the classifier and use the predictions on the test set to get the micro and macro\\nF1-scores via the classification report.\\nWith the following helper function we can plot the results of this experiment:\\nimport matplotlib.pyplot as plt\\ndef plot_metrics(micro_scores, macro_scores, sample_sizes, current_model):\\n    fig, (ax0, ax1) = plt.subplots(1, 2, figsize=(10, 4), sharey=True)\\n    for run in micro_scores.keys():\\n        if run == current_model:\\n            ax0.plot(sample_sizes, micro_scores[run], label=run, linewidth=2)\\n            ax1.plot(sample_sizes, macro_scores[run], label=run, linewidth=2)\\n        else:\\n            ax0.plot(sample_sizes, micro_scores[run], label=run,\\n                     linestyle=\"dashed\")\\n            ax1.plot(sample_sizes, macro_scores[run], label=run,\\n                     linestyle=\"dashed\")\\n    ax0.set_title(\"Micro F1 scores\")\\n    ax1.set_title(\"Macro F1 scores\")\\n    ax0.set_ylabel(\"Test set F1 score\")\\n    ax0.legend(loc=\"lower right\")\\n    for ax in [ax0, ax1]:\\nImplementing a Naive Bayesline | 261'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 285, 'page_label': '262'}, page_content='ax.set_xlabel(\"Number of training samples\")\\n        ax.set_xscale(\"log\")\\n        ax.set_xticks(sample_sizes)\\n        ax.set_xticklabels(sample_sizes)\\n        ax.minorticks_off()\\n    plt.tight_layout()\\n    plt.show()\\nplot_metrics(micro_scores, macro_scores, train_samples, \"Naive Bayes\")\\n262 | Chapter 9: Dealing with Few to No Labels'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 286, 'page_label': '263'}, page_content='2 We thank Joe Davison for suggesting this approach to us.\\nNote that we plot the number of samples on a logarithmic scale. From the figure we\\ncan see that the micro and macro F1-scores both improve as we increase the number\\nof training samples. With so few samples to train on, the results are also slightly noisy\\nsince each slice can have a different class distribution. Nevertheless, what’s important\\nhere is the trend, so let’s now see how these results fare against transformer-based\\napproaches!\\nWorking with No Labeled Data\\nThe first technique that we’ll consider is zero-shot classification, which is suitable in\\nsettings where you have no labeled data at all. This is surprisingly common in indus‐\\ntry, and might occur because there is no historic data with labels or because acquiring\\nthe labels for the data is difficult. We will cheat a bit in this section since we will still\\nuse the test data to measure the performance, but we will not use any data to train the\\nmodel (otherwise the comparison to the following approaches would be difficult).\\nThe goal of zero-shot classification is to make use of a pretrained model without any\\nadditional fine-tuning on your task-specific corpus. To get a better idea of how this\\ncould work, recall that language models like BERT are pretrained to predict masked\\ntokens in text on thousands of books and large Wikipedia dumps. To successfully\\npredict a missing token, the model needs to be aware of the topic in the context. We\\ncan try to trick the model into classifying a document for us by providing a sentence\\nlike:\\n“This section was about the topic [MASK]. ”\\nThe model should then give a reasonable suggestion for the document’s topic, since\\nthis is a natural text to occur in the dataset.2\\nLet’s illustrate this further with the following toy problem: suppose you have two chil‐\\ndren, and one of them likes movies with cars while the other enjoys movies with ani‐\\nmals better. Unfortunately, they have already seen all the ones you know, so you want\\nto build a function that tells you what topic a new movie is about. Naturally, you turn\\nto transformers for this task. The first thing to try is to load BERT-base in the fill-\\nmask pipeline, which uses the masked language model to predict the content of the\\nmasked tokens:\\nfrom transformers import pipeline\\npipe = pipeline(\"fill-mask\", model=\"bert-base-uncased\")\\nWorking with No Labeled Data | 263'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 287, 'page_label': '264'}, page_content='Next, let’s construct a little movie description and add a prompt to it with a masked\\nword. The goal of the prompt is to guide the model to help us make a classification.\\nThe fill-mask pipeline returns the most likely tokens to fill in the masked spot:\\nmovie_desc = \"The main characters of the movie madacascar \\\\\\nare a lion, a zebra, a giraffe, and a hippo. \"\\nprompt = \"The movie is about [MASK].\"\\noutput = pipe(movie_desc + prompt)\\nfor element in output:\\n    print(f\"Token {element[\\'token_str\\']}:\\\\t{element[\\'score\\']:.3f}%\")\\nToken animals:  0.103%\\nToken lions:    0.066%\\nToken birds:    0.025%\\nToken love:     0.015%\\nToken hunting:  0.013%\\nClearly, the model predicts only tokens that are related to animals. We can also turn\\nthis around, and instead of getting the most likely tokens we can query the pipeline\\nfor the probability of a few given tokens. For this task we might choose cars and\\nanimals, so we can pass them to the pipeline as targets:\\noutput = pipe(movie_desc + prompt, targets=[\"animals\", \"cars\"])\\nfor element in output:\\n    print(f\"Token {element[\\'token_str\\']}:\\\\t{element[\\'score\\']:.3f}%\")\\nToken animals:  0.103%\\nToken cars:     0.001%\\nUnsurprisingly, the predicted probability for the token cars is much smaller than for\\nanimals. Let’s see if this also works for a description that is closer to cars:\\nmovie_desc = \"In the movie transformers aliens \\\\\\ncan morph into a wide range of vehicles.\"\\noutput = pipe(movie_desc + prompt, targets=[\"animals\", \"cars\"])\\nfor element in output:\\n    print(f\"Token {element[\\'token_str\\']}:\\\\t{element[\\'score\\']:.3f}%\")\\nToken cars:     0.139%\\nToken animals:  0.006%\\nIt does! This is only a simple example, and if we want to make sure it works well we\\nshould test it thoroughly, but it illustrates the key idea of many approaches discussed\\nin this chapter: find a way to adapt a pretrained model for another task without train‐\\ning it. In this case we set up a prompt with a mask in such a way that we can use a\\nmasked language model directly for classification. Let’s see if we can do better by\\nadapting a model that has been fine-tuned on a task that’s closer to text classification:\\nnatural language inference (NLI).\\n264 | Chapter 9: Dealing with Few to No Labels'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 288, 'page_label': '265'}, page_content='3 A. Williams, N. Nangia, and S.R. Bowman, “ A Broad-Coverage Challenge Corpus for Sentence Understanding\\nThrough Inference”, (2018); A. Conneau et al., “XNLI: Evaluating Cross-Lingual Sentence Representations”,\\n(2018).\\nUsing the masked language model for classification is a nice trick, but we can do bet‐\\nter still by using a model that has been trained on a task that is closer to classification.\\nThere is a neat proxy task called text entailment that fits the bill. In text entailment,\\nthe model needs to determine whether two text passages are likely to follow or con‐\\ntradict each other. Models are typically trained to detect entailments and contradic‐\\ntions with datasets such as Multi-Genre NLI Corpus (MNLI) or Cross-Lingual NLI\\nCorpus (XNLI).3\\nEach sample in these datasets is composed of three parts: a premise, a hypothesis, and\\na label, which can be one of entailment, neutral, or contradiction. The entail\\nment label is assigned when the hypothesis text is necessarily true under the premise.\\nThe contradiction label is used when the hypothesis is necessarily false or inappro‐\\npriate under the premise. If neither of these cases applies, then the neutral label is\\nassigned. See Table 9-1 for examples of each.\\nTable 9-1. The three classes in the MLNI dataset\\nPremise Hypothesis Label\\nHis favourite color is blue. He is into heavy metal music. neutral\\nShe finds the joke hilarious. She thinks the joke is not funny at all. contradiction\\nThe house was recently built. The house is new. entailment\\nNow, it turns out that we can hijack a model trained on the MNLI dataset to build a\\nclassifier without needing any labels at all! The key idea is to treat the text we wish to\\nclassify as the premise, and then formulate the hypothesis as:\\n“This example is about {label}. ”\\nwhere we insert the class name for the label. The entailment score then tells us how\\nlikely that premise is to be about that topic, and we can run this for any number of\\nclasses sequentially. The downside of this approach is that we need to execute a for‐\\nward pass for each class, which makes it less efficient than a standard classifier.\\nAnother slightly tricky aspect is that the choice of label names can have a large impact\\non the accuracy, and choosing labels with semantic meaning is generally the best\\napproach. For example, if the label is simply Class 1, the model has no hint what this\\nmight mean and whether this constitutes a contradiction or entailment.\\n Transformers has an MNLI model for zero-shot classification built in. We can ini‐\\ntialize it via a pipeline as follows:\\nWorking with No Labeled Data | 265'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 289, 'page_label': '266'}, page_content='from transformers import pipeline\\npipe = pipeline(\"zero-shot-classification\", device=0)\\nThe setting device=0 makes sure that the model runs on the GPU instead of the\\ndefault CPU to speed up inference. To classify a text, we simply need to pass it to the\\npipeline along with the label names. In addition, we can set multi_label=True to\\nensure that all the scores are returned and not only the maximum for single-label\\nclassification:\\nsample = ds[\"train\"][0]\\nprint(f\"Labels: {sample[\\'labels\\']}\")\\noutput = pipe(sample[\"text\"], all_labels, multi_label=True)\\nprint(output[\"sequence\"][:400])\\nprint(\"\\\\nPredictions:\")\\nfor label, score in zip(output[\"labels\"], output[\"scores\"]):\\n    print(f\"{label}, {score:.2f}\")\\nLabels: [\\'new model\\']\\nAdd new CANINE model\\n# \\n  New model addition\\n## Model description\\nGoogle recently proposed a new **C**haracter **A**rchitecture with **N**o\\ntokenization **I**n **N**eural **E**ncoders architecture (CANINE). Not only the\\ntitle is exciting:\\n> Pipelined NLP systems have largely been superseded by end-to-end neural\\nmodeling, yet nearly all commonly-used models still require an explicit tokeni\\nPredictions:\\nnew model, 0.98\\ntensorflow or tf, 0.37\\nexamples, 0.34\\nusage, 0.30\\npytorch, 0.25\\ndocumentation, 0.25\\nmodel training, 0.24\\ntokenization, 0.17\\npipeline, 0.16\\nSince we are using a subword tokenizer, we can even pass code to\\nthe model! The tokenization might not be very efficient because\\nonly a small fraction of the pretraining dataset for the zero-shot\\npipeline consists of code snippets, but since code is also made up of\\na lot of natural words this is not a big issue. Also, the code block\\nmight contain important information, such as the framework\\n(PyTorch or TensorFlow).\\n266 | Chapter 9: Dealing with Few to No Labels'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 290, 'page_label': '267'}, page_content='We can see that the model is very confident that this text is about a new model, but it\\nalso produces relatively high scores for the other labels. An important aspect for zero-\\nshot classification is the domain we’re operating in. The texts we are dealing with here\\nare very technical and mostly about coding, which makes them quite different from\\nthe original text distribution in the MNLI dataset. Thus, it is not surprising that this is\\na challenging task for the model; it might work much better for some domains than\\nothers, depending on how close they are to the training data.\\nLet’s write a function that feeds a single example through the zero-shot pipeline, and\\nthen scale it out to the whole validation set by running map():\\ndef zero_shot_pipeline(example):\\n    output = pipe(example[\"text\"], all_labels, multi_label=True)\\n    example[\"predicted_labels\"] = output[\"labels\"]\\n    example[\"scores\"] = output[\"scores\"]\\n    return example\\nds_zero_shot = ds[\"valid\"].map(zero_shot_pipeline)\\nNow that we have our scores, the next step is to determine which set of labels should\\nbe assigned to each example. There are a few options we can experiment with:\\n• Define a threshold and select all labels above the threshold.\\n• Pick the top k labels with the k highest scores.\\nTo help us determine which method is best, let’s write a get_preds() function that\\napplies one of the approaches to retrieve the predictions:\\ndef get_preds(example, threshold=None, topk=None):\\n    preds = []\\n    if threshold:\\n        for label, score in zip(example[\"predicted_labels\"], example[\"scores\"]):\\n            if score >= threshold:\\n                preds.append(label)\\n    elif topk:\\n        for i in range(topk):\\n            preds.append(example[\"predicted_labels\"][i])\\n    else:\\n        raise ValueError(\"Set either `threshold` or `topk`.\")\\n    return {\"pred_label_ids\": list(np.squeeze(mlb.transform([preds])))}\\nNext, let’s write a second function, get_clf_report(), that returns the Scikit-learn\\nclassification report from a dataset with the predicted labels:\\ndef get_clf_report(ds):\\n    y_true = np.array(ds[\"label_ids\"])\\n    y_pred = np.array(ds[\"pred_label_ids\"])\\n    return classification_report(\\n        y_true, y_pred, target_names=mlb.classes_, zero_division=0,\\n        output_dict=True)\\nWorking with No Labeled Data | 267'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 291, 'page_label': '268'}, page_content='Armed with these two functions, let’s start with the top- k method by increasing k for\\nseveral values and then plotting the micro and macro F1-scores across the validation\\nset:\\nmacros, micros = [], []\\ntopks = [1, 2, 3, 4]\\nfor topk in topks:\\n    ds_zero_shot = ds_zero_shot.map(get_preds, batched=False,\\n                                    fn_kwargs={\\'topk\\': topk})\\n    clf_report = get_clf_report(ds_zero_shot)\\n    micros.append(clf_report[\\'micro avg\\'][\\'f1-score\\'])\\n    macros.append(clf_report[\\'macro avg\\'][\\'f1-score\\'])\\nplt.plot(topks, micros, label=\\'Micro F1\\')\\nplt.plot(topks, macros, label=\\'Macro F1\\')\\nplt.xlabel(\"Top-k\")\\nplt.ylabel(\"F1-score\")\\nplt.legend(loc=\\'best\\')\\nplt.show()\\nFrom the plot we can see that the best results are obtained by selecting the label with\\nthe highest score per example (top 1). This is perhaps not so surprising, given that\\nmost of the examples in our datasets have only one label. Let’s now compare this\\nagainst setting a threshold, so we can potentially predict more than one label per\\nexample:\\nmacros, micros = [], []\\nthresholds = np.linspace(0.01, 1, 100)\\nfor threshold in thresholds:\\n    ds_zero_shot = ds_zero_shot.map(get_preds,\\n                                    fn_kwargs={\"threshold\": threshold})\\n    clf_report = get_clf_report(ds_zero_shot)\\n268 | Chapter 9: Dealing with Few to No Labels'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 292, 'page_label': '269'}, page_content='micros.append(clf_report[\"micro avg\"][\"f1-score\"])\\n    macros.append(clf_report[\"macro avg\"][\"f1-score\"])\\nplt.plot(thresholds, micros, label=\"Micro F1\")\\nplt.plot(thresholds, macros, label=\"Macro F1\")\\nplt.xlabel(\"Threshold\")\\nplt.ylabel(\"F1-score\")\\nplt.legend(loc=\"best\")\\nplt.show()\\nbest_t, best_micro = thresholds[np.argmax(micros)], np.max(micros)\\nprint(f\\'Best threshold (micro): {best_t} with F1-score {best_micro:.2f}.\\')\\nbest_t, best_macro = thresholds[np.argmax(macros)], np.max(macros)\\nprint(f\\'Best threshold (micro): {best_t} with F1-score {best_macro:.2f}.\\')\\nBest threshold (micro): 0.75 with F1-score 0.46.\\nBest threshold (micro): 0.72 with F1-score 0.42.\\nThis approach fares somewhat worse than the top-1 results, but we can see the preci‐\\nsion/recall trade-off clearly in this graph. If we set the threshold too low, then there\\nare too many predictions, which leads to a low precision. If we set the threshold too\\nhigh, then we will make hardly any predictions, which produces a low recall. From\\nthe plot we can see that a threshold value of around 0.8 is the sweet spot between the\\ntwo.\\nSince the top-1 method performs best, let’s use this to compare zero-shot classifica‐\\ntion against Naive Bayes on the test set:\\nds_zero_shot = ds[\\'test\\'].map(zero_shot_pipeline)\\nds_zero_shot = ds_zero_shot.map(get_preds, fn_kwargs={\\'topk\\': 1})\\nclf_report = get_clf_report(ds_zero_shot)\\nfor train_slice in train_slices:\\nWorking with No Labeled Data | 269'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 293, 'page_label': '270'}, page_content='macro_scores[\\'Zero Shot\\'].append(clf_report[\\'macro avg\\'][\\'f1-score\\'])\\n    micro_scores[\\'Zero Shot\\'].append(clf_report[\\'micro avg\\'][\\'f1-score\\'])\\nplot_metrics(micro_scores, macro_scores, train_samples, \"Zero Shot\")\\nComparing the zero-shot pipeline to the baseline, we observe two things:\\n1. If we have less than 50 labeled samples, the zero-shot pipeline handily outper‐\\nforms the baseline.\\n2. Even above 50 samples, the performance of the zero-shot pipeline is superior\\nwhen considering both the micro and macro F1-scores. The results for the micro\\nF1-score tell us that the baseline performs well on the frequent classes, while the\\n270 | Chapter 9: Dealing with Few to No Labels'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 294, 'page_label': '271'}, page_content='zero-shot pipeline excels at those since it does not require any examples to learn\\nfrom.\\nY ou might notice a slight paradox in this section: although we talk\\nabout dealing with no labels, we still use the validation and test\\nsets. We use them to showcase different techniques and to make\\nthe results comparable between them. Even in a real use case, it\\nmakes sense to gather a handful of labeled examples to run some\\nquick evaluations. The important point is that we did not adapt the\\nparameters of the model with the data; instead, we just adapted\\nsome hyperparameters.\\nIf you find it difficult to get good results on your own dataset, here are a few things\\nyou can do to improve the zero-shot pipeline:\\n• The way the pipeline works makes it very sensitive to the names of the labels. If\\nthe names don’t make much sense or are not easily connected to the texts, the\\npipeline will likely perform poorly. Either try using different names or use several\\nnames in parallel and aggregate them in an extra step.\\n• Another thing you can improve is the form of the hypothesis. By default it is\\nhypothesis=\"This is example is about {}\" , but you can pass any other text\\nto the pipeline. Depending on the use case, this might improve the performance.\\nLet’s now turn to the regime where we have a few labeled examples we can use to\\ntrain a model.\\nWorking with a Few Labels\\nIn most NLP projects, you’ll have access to at least a few labeled examples. The labels\\nmight come directly from a client or cross-company team, or you might decide to just\\nsit down and annotate a few examples yourself. Even for the previous approach, we\\nneeded a few labeled examples to evaluate how well the zero-shot approach worked.\\nIn this section, we’ll have a look at how we can best leverage the few, precious labeled\\nexamples that we have. Let’s start by looking at a technique known as data augmenta‐\\ntion that can help us multiply the little labeled data that we have.\\nData Augmentation\\nOne simple but effective way to boost the performance of text classifiers on small\\ndatasets is to apply data augmentation techniques to generate new training examples\\nfrom the existing ones. This is a common strategy in computer vision, where images\\nare randomly perturbed without changing the meaning of the data (e.g., a slightly\\nWorking with a Few Labels | 271'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 295, 'page_label': '272'}, page_content='4 J. Wei and K. Zou, “EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classifica‐\\ntion Tasks”, (2019).\\nrotated cat is still a cat). For text, data augmentation is somewhat trickier because per‐\\nturbing the words or characters can completely change the meaning. For example, the\\ntwo questions “ Are elephants heavier than mice?” and “ Are mice heavier than ele‐\\nphants?” differ by a single word swap, but have opposite answers. However, if the text\\nconsists of more than a few sentences (like our GitHub issues do), then the noise\\nintroduced by these types of transformations will generally not affect the label. In\\npractice, there are two types of data augmentation techniques that are commonly\\nused:\\nBack translation\\nTake a text in the source language, translate it into one or more target languages\\nusing machine translation, and then translate it back to the source language. Back\\ntranslation tends to works best for high-resource languages or corpora that don’t\\ncontain too many domain-specific words.\\nToken perturbations\\nGiven a text from the training set, randomly choose and perform simple trans‐\\nformations like random synonym replacement, word insertion, swap, or\\ndeletion.4\\nExamples of these transformations are shown in Table 9-2. For a detailed list of other\\ndata augmentation techniques for NLP , we recommend reading Amit Chaudhary’s\\nblog post “ A Visual Survey of Data Augmentation in NLP”.\\nTable 9-2. Different types of data augmentation techniques for text\\nAugmentation Sentence\\nNone Even if you defeat me Megatron, others will rise to defeat your tyranny\\nSynonym replace Even if you kill me Megatron, others will prove to defeat your tyranny\\nRandom insert Even if you defeat me Megatron, others humanity will rise to defeat your tyranny\\nRandom swap You even if defeat me Megatron, others will rise defeat to tyranny your\\nRandom delete Even if you me Megatron, others to defeat tyranny\\nBack translate (German) Even if you defeat me, others will rise up to defeat your tyranny\\nY ou can implement back translation using machine translation models like M2M100,\\nwhile libraries like NlpAug and TextAttack provide various recipes for token pertur‐\\nbations. In this section, we’ll focus on using synonym replacement as it’s simple to\\nimplement and gets across the main idea behind data augmentation.\\n272 | Chapter 9: Dealing with Few to No Labels'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 296, 'page_label': '273'}, page_content='We’ll use the ContextualWordEmbsAug augmenter from NlpAug to leverage the con‐\\ntextual word embeddings of DistilBERT for our synonym replacements. Let’s start\\nwith a simple example:\\nfrom transformers import set_seed\\nimport nlpaug.augmenter.word as naw\\nset_seed(3)\\naug = naw.ContextualWordEmbsAug(model_path=\"distilbert-base-uncased\",\\n                                device=\"cpu\", action=\"substitute\")\\ntext = \"Transformers are the most popular toys\"\\nprint(f\"Original text: {text}\")\\nprint(f\"Augmented text: {aug.augment(text)}\")\\nOriginal text: Transformers are the most popular toys\\nAugmented text: transformers\\'the most popular toys\\nHere we can see how the word “are” has been replaced with an apostrophe to generate\\na new synthetic training example. We can wrap this augmentation in a simple func‐\\ntion as follows:\\ndef augment_text(batch, transformations_per_example=1):\\n    text_aug, label_ids = [], []\\n    for text, labels in zip(batch[\"text\"], batch[\"label_ids\"]):\\n        text_aug += [text]\\n        label_ids += [labels]\\n        for _ in range(transformations_per_example):\\n            text_aug += [aug.augment(text)]\\n            label_ids += [labels]\\n    return {\"text\": text_aug, \"label_ids\": label_ids}\\nNow when we pass this function to the map() method, we can generate any number\\nof new examples with the transformations_per_example argument. We can use this\\nfunction in our code to train the Naive Bayes classifier by simply adding one line after\\nwe select the slice:\\nds_train_sample = ds_train_sample.map(augment_text, batched=True,\\n    remove_columns=ds_train_sample.column_names).shuffle(seed=42)\\nIncluding this and rerunning the analysis produces the plot shown here:\\nplot_metrics(micro_scores, macro_scores, train_samples, \"Naive Bayes + Aug\")\\nWorking with a Few Labels | 273'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 297, 'page_label': '274'}, page_content='From the figure, we can see that a small amount of data augmentation improves the\\nF1-score of the Naive Bayes classifier by around 5 points, and it overtakes the zero-\\nshot pipeline for the macro scores once we have around 170 training samples. Let’s\\nnow take a look at a method based on using the embeddings of large language\\nmodels.\\n274 | Chapter 9: Dealing with Few to No Labels'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 298, 'page_label': '275'}, page_content='Using Embeddings as a Lookup Table\\nLarge language models such as GPT-3 have been shown to be excellent at solving\\ntasks with limited data. The reason is that these models learn useful representations\\nof text that encode information across many dimensions, such as sentiment, topic,\\ntext structure, and more. For this reason, the embeddings of large language models\\ncan be used to develop a semantic search engine, find similar documents or com‐\\nments, or even classify text.\\nIn this section we’ll create a text classifier that’s modeled after the OpenAI API classi‐\\nfication endpoint. The idea follows a three-step process:\\n1. Use the language model to embed all labeled texts.\\n2. Perform a nearest neighbor search over the stored embeddings.\\n3. Aggregate the labels of the nearest neighbors to get a prediction.\\nThe process is illustrated in Figure 9-3, which shows how labeled data is embedded\\nwith a model and stored with the labels. When a new text needs to be classified it is\\nembedded as well, and the label is given based on the labels of the nearest neighbors.\\nIt is important to calibrate the number of neighbors to be searched for, as too few\\nmight be noisy and too many might mix in neighboring groups.\\nFigure 9-3. An illustration of nearest neighbor embedding lookup\\nThe beauty of this approach is that no model fine-tuning is necessary to leverage the\\nfew available labeled data points. Instead, the main decision to make this approach\\nwork is to select an appropriate model that is ideally pretrained on a similar domain\\nto your dataset.\\nWorking with a Few Labels | 275'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 299, 'page_label': '276'}, page_content='Since GPT-3 is only available through the OpenAI API, we’ll use GPT-2 to test the\\ntechnique. Specifically, we’ll use a variant of GPT-2 that was trained on Python code,\\nwhich will hopefully capture some of the context contained in our GitHub issues.\\nLet’s write a helper function that takes a list of texts and uses the model to create a\\nsingle-vector representation for each text. One problem we have to deal with is that\\ntransformer models like GPT-2 will actually return one embedding vector per token.\\nFor example, given the sentence “I took my dog for a walk” , we can expect several\\nembedding vectors, one for each token. But what we really want is a single embed‐\\nding vector for the whole sentence (or GitHub issue in our application). To deal with\\nthis, we can use a technique called pooling. One of the simplest pooling methods is to\\naverage the token embeddings, which is called mean pooling. With mean pooling, the\\nonly thing we need to watch out for is that we don’t include padding tokens in the\\naverage, so we can use the attention mask to handle that.\\nTo see how this works, let’s load a GPT-2 tokenizer and model, define the mean pool‐\\ning operation, and wrap the whole process in a simple embed_text() function:\\nimport torch\\nfrom transformers import AutoTokenizer, AutoModel\\nmodel_ckpt = \"miguelvictor/python-gpt2-large\"\\ntokenizer = AutoTokenizer.from_pretrained(model_ckpt)\\nmodel = AutoModel.from_pretrained(model_ckpt)\\ndef mean_pooling(model_output, attention_mask):\\n    # Extract the token embeddings\\n    token_embeddings = model_output[0]\\n    # Compute the attention mask\\n    input_mask_expanded = (attention_mask\\n                           .unsqueeze(-1)\\n                           .expand(token_embeddings.size())\\n                           .float())\\n    # Sum the embeddings, but ignore masked tokens\\n    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\\n    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\\n    # Return the average as a single vector\\n    return sum_embeddings / sum_mask\\ndef embed_text(examples):\\n    inputs = tokenizer(examples[\"text\"], padding=True, truncation=True,\\n                       max_length=128, return_tensors=\"pt\")\\n    with torch.no_grad():\\n        model_output = model(**inputs)\\n    pooled_embeds = mean_pooling(model_output, inputs[\"attention_mask\"])\\n    return {\"embedding\": pooled_embeds.cpu().numpy()}\\nNow we can get the embeddings for each split. Note that GPT-style models don’t have\\na padding token, and therefore we need to add one before we can get the embeddings\\n276 | Chapter 9: Dealing with Few to No Labels'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 300, 'page_label': '277'}, page_content='5 J. Johnson, M. Douze, and H. Jégou, “Billion-Scale Similarity Search with GPUs”, (2017).\\nin a batched fashion as implemented in the preceding code. We’ll just recycle the end-\\nof-string token for this purpose:\\ntokenizer.pad_token = tokenizer.eos_token\\nembs_train = ds[\"train\"].map(embed_text, batched=True, batch_size=16)\\nembs_valid = ds[\"valid\"].map(embed_text, batched=True, batch_size=16)\\nembs_test = ds[\"test\"].map(embed_text, batched=True, batch_size=16)\\nNow that we have all the embeddings, we need to set up a system to search them. We\\ncould write a function that calculates, say, the cosine similarity between a new text\\nembedding that we’ll query and the existing embeddings in the training set. Alterna‐\\ntively, we can use a built-in structure of \\n  Datasets called a FAISS index.5 We already\\nencountered FAISS in Chapter 7. Y ou can think of this as a search engine for embed‐\\ndings, and we’ll have a closer look at how it works in a minute. We can use an existing\\nfield of the dataset to create a FAISS index with add_faiss_index(), or we can load\\nnew embeddings into the dataset with add_faiss_index_from_external_arrays().\\nLet’s use the former function to add our training embeddings to the dataset as\\nfollows:\\nembs_train.add_faiss_index(\"embedding\")\\nThis created a new FAISS index called embedding. We can now perform a nearest\\nneighbor lookup by calling the function get_nearest_examples(). It returns the\\nclosest neighbors as well as the matching score for each neighbor. We need to specify\\nthe query embedding as well as the number of nearest neighbors to retrieve. Let’s give\\nit a spin and have a look at the documents that are closest to an example:\\ni, k = 0, 3 # Select the first query and 3 nearest neighbors\\nrn, nl = \"\\\\r\\\\n\\\\r\\\\n\", \"\\\\n\" # Used to remove newlines in text for compact display\\nquery =  np.array(embs_valid[i][\"embedding\"], dtype=np.float32)\\nscores, samples = embs_train.get_nearest_examples(\"embedding\", query, k=k)\\nprint(f\"QUERY LABELS: {embs_valid[i][\\'labels\\']}\")\\nprint(f\"QUERY TEXT:\\\\n{embs_valid[i][\\'text\\'][:200].replace(rn, nl)} [...]\\\\n\")\\nprint(\"=\"*50)\\nprint(f\"Retrieved documents:\")\\nfor score, label, text in zip(scores, samples[\"labels\"], samples[\"text\"]):\\n    print(\"=\"*50)\\n    print(f\"TEXT:\\\\n{text[:200].replace(rn, nl)} [...]\")\\n    print(f\"SCORE: {score:.2f}\")\\n    print(f\"LABELS: {label}\")\\nQUERY LABELS: [\\'new model\\']\\nQUERY TEXT:\\nImplementing efficient self attention in T5\\nWorking with a Few Labels | 277'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 301, 'page_label': '278'}, page_content=\"# \\n  New model addition\\nMy teammates and I (including @ice-americano) would like to use efficient self\\nattention methods such as Linformer, Performer and [...]\\n==================================================\\nRetrieved documents:\\n==================================================\\nTEXT:\\nAdd Linformer model\\n# \\n  New model addition\\n## Model description\\n### Linformer: Self-Attention with Linear Complexity\\nPaper published June 9th on ArXiv: https://arxiv.org/abs/2006.04768\\nLa [...]\\nSCORE: 54.92\\nLABELS: ['new model']\\n==================================================\\nTEXT:\\nAdd FAVOR+ / Performer attention\\n# \\n  FAVOR+ / Performer attention addition\\nAre there any plans to add this new attention approximation block to\\nTransformers library?\\n## Model description\\nThe n [...]\\nSCORE: 57.90\\nLABELS: ['new model']\\n==================================================\\nTEXT:\\nImplement DeLighT: Very Deep and Light-weight Transformers\\n# \\n  New model addition\\n## Model description\\nDeLight, that delivers similar or better performance than transformer-based\\nmodels with sign [...]\\nSCORE: 60.12\\nLABELS: ['new model']\\nNice! This is exactly what we hoped for: the three retrieved documents that we got via\\nembedding lookup all have the same labels and we can already see from the titles that\\nthey are all very similar. The query as well as the retrieved documents revolve around\\nadding new and efficient transformer models. The question remains, however, what\\nis the best value for k? Similarly, how we should then aggregate the labels of the\\nretrieved documents? Should we, for example, retrieve three documents and assign all\\nlabels that occurred at least twice? Or should we go for 20 and use all labels that\\nappeared at least 5 times? Let’s investigate this systematically: we’ll try several values\\nfor k and then vary the threshold m<k for label assignment with a helper function.\\nWe’ll record the macro and micro performance for each setting so we can decide later\\nwhich run performed best. Instead of looping over each sample in the validation set\\n278 | Chapter 9: Dealing with Few to No Labels\"), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 302, 'page_label': '279'}, page_content='we can make use of the function get_nearest_examples_batch(), which accepts a\\nbatch of queries:\\ndef get_sample_preds(sample, m):\\n    return (np.sum(sample[\"label_ids\"], axis=0) >= m).astype(int)\\ndef find_best_k_m(ds_train, valid_queries, valid_labels, max_k=17):\\n    max_k = min(len(ds_train), max_k)\\n    perf_micro = np.zeros((max_k, max_k))\\n    perf_macro = np.zeros((max_k, max_k))\\n    for k in range(1, max_k):\\n        for m in range(1, k + 1):\\n            _, samples = ds_train.get_nearest_examples_batch(\"embedding\",\\n                                                             valid_queries, k=k)\\n            y_pred = np.array([get_sample_preds(s, m) for s in samples])\\n            clf_report = classification_report(valid_labels, y_pred,\\n                target_names=mlb.classes_, zero_division=0, output_dict=True)\\n            perf_micro[k, m] = clf_report[\"micro avg\"][\"f1-score\"]\\n            perf_macro[k, m] = clf_report[\"macro avg\"][\"f1-score\"]\\n    return perf_micro, perf_macro\\nLet’s check what the best values would be with all the training samples and visualize\\nthe scores for all k and m configurations:\\nvalid_labels = np.array(embs_valid[\"label_ids\"])\\nvalid_queries = np.array(embs_valid[\"embedding\"], dtype=np.float32)\\nperf_micro, perf_macro = find_best_k_m(embs_train, valid_queries, valid_labels)\\nfig, (ax0, ax1) = plt.subplots(1, 2, figsize=(10, 3.5), sharey=True)\\nax0.imshow(perf_micro)\\nax1.imshow(perf_macro)\\nax0.set_title(\"micro scores\")\\nax0.set_ylabel(\"k\")\\nax1.set_title(\"macro scores\")\\nfor ax in [ax0, ax1]:\\n    ax.set_xlim([0.5, 17 - 0.5])\\n    ax.set_ylim([17 - 0.5, 0.5])\\n    ax.set_xlabel(\"m\")\\nplt.show()\\nWorking with a Few Labels | 279'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 303, 'page_label': '280'}, page_content='From the plots we can see that there is a pattern: choosing m too large or small for a\\ngiven k yields suboptimal results. The best performance is achieved when choosing a\\nratio of approximately m/k = 1/3 . Let’s see which k and m give the best result overall:\\nk, m = np.unravel_index(perf_micro.argmax(), perf_micro.shape)\\nprint(f\"Best k: {k}, best m: {m}\")\\nBest k: 15, best m: 5\\nThe perfomance is best when we choose k= 15 and m= 5, or in other words when\\nwe retrieve the 15 nearest neighbors and then assign the labels that occurred at least 5\\ntimes. Now that we have a good method for finding the best values for the embedding\\nlookup, we can play the same game as with the Naive Bayes classifier where we go\\nthrough the slices of the training set and evaluate the performance. Before we can\\nslice the dataset, we need to remove the index since we cannot slice a FAISS index like\\nthe dataset. The rest of the loops stay exactly the same, with the addition of using the\\nvalidation set to get the best k and m values:\\nembs_train.drop_index(\"embedding\")\\ntest_labels = np.array(embs_test[\"label_ids\"])\\ntest_queries = np.array(embs_test[\"embedding\"], dtype=np.float32)\\nfor train_slice in train_slices:\\n    # Create a Faiss index from training slice\\n    embs_train_tmp = embs_train.select(train_slice)\\n    embs_train_tmp.add_faiss_index(\"embedding\")\\n    # Get best k, m values with validation set\\n    perf_micro, _ = find_best_k_m(embs_train_tmp, valid_queries, valid_labels)\\n    k, m = np.unravel_index(perf_micro.argmax(), perf_micro.shape)\\n    # Get predictions on test set\\n    _, samples = embs_train_tmp.get_nearest_examples_batch(\"embedding\",\\n                                                           test_queries,\\n                                                           k=int(k))\\n    y_pred = np.array([get_sample_preds(s, m) for s in samples])\\n    # Evaluate predictions\\n280 | Chapter 9: Dealing with Few to No Labels'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 304, 'page_label': '281'}, page_content='clf_report = classification_report(test_labels, y_pred,\\n        target_names=mlb.classes_, zero_division=0, output_dict=True,)\\n    macro_scores[\"Embedding\"].append(clf_report[\"macro avg\"][\"f1-score\"])\\n    micro_scores[\"Embedding\"].append(clf_report[\"micro avg\"][\"f1-score\"])\\nplot_metrics(micro_scores, macro_scores, train_samples, \"Embedding\")\\nWorking with a Few Labels | 281'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 305, 'page_label': '282'}, page_content='The embedding lookup is competitive on the micro scores with the previous\\napproaches while just having two “learnable” parameters, k and m, but performs\\nslightly worse on the macro scores.\\nTake these results with a grain of salt; which method works best strongly depends on\\nthe domain. The zero-shot pipeline’s training data is quite different from the GitHub\\nissues dataset we’re using it on, which contains a lot of code that the model likely has\\nnot encountered much before. For a more common task such as sentiment analysis of\\nreviews, the pipeline might work much better. Similarly, the embeddings’ quality\\ndepends on the model and the data it was trained on. We tried half a dozen models,\\nsuch as sentence-transformers/stsb-roberta-large, which was trained to give\\nhigh-quality embeddings of sentences, and microsoft/codebert-base and dbern\\nsohn/roberta-python, which were trained on code and documentation. For this spe‐\\ncific use case, GPT-2 trained on Python code worked best.\\nSince you don’t actually need to change anything in your code besides replacing the\\nmodel checkpoint name to test another model, you can quickly try out a few models\\nonce you have the evaluation pipeline set up.\\nLet’s now compare this simple embedding trick against simply fine-tuning a trans‐\\nformer on the limited data we have.\\nEfficient Similarity Search with FAISS\\nWe first encountered FAISS in Chapter 7, where we used it to retrieve documents via\\nthe DPR embeddings. Here we’ll explain briefly how the FAISS library works and why\\nit is a powerful tool in the ML toolbox.\\nWe are used to performing fast text queries on huge datasets such as Wikipedia or the\\nweb with search engines such as Google. When we move from text to embeddings, we\\nwould like to maintain that performance; however, the methods used to speed up text\\nqueries don’t apply to embeddings.\\nTo speed up text search we usually create an inverted index that maps terms to docu‐\\nments. An inverted index works like an index at the end of a book: each word is map‐\\nped to the pages (or in our case, document) it occurs in. When we later run a query\\nwe can quickly look up in which documents the search terms appear. This works well\\nwith discrete objects such as words, but does not work with continuous objects such\\nas vectors. Each document likely has a unique vector, and therefore the index will\\nnever match with a new vector. Instead of looking for exact matches, we need to look\\nfor close or similar matches.\\nWhen we want to find the most similar vectors in a database to a query vector, in\\ntheory we need to compare the query vector to each of the n vectors in the database.\\nFor a small database such as we have in this chapter this is no problem, but if we\\n282 | Chapter 9: Dealing with Few to No Labels'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 306, 'page_label': '283'}, page_content='scaled this up to thousands or even million of entries we would need to wait a while\\nfor each query to be processed.\\nFAISS addresses this issue with several tricks. The main idea is to partition the data‐\\nset. If we only need to compare the query vector to a subset of the database, we can\\nspeed up the process significantly. But if we just randomly partition the dataset, how\\ncan we decide which partition to search, and what guarantees do we get for finding\\nthe most similar entries? Evidently, there must be a better solution: apply k-means\\nclustering to the dataset! This clusters the embeddings into groups by similarity. Fur‐\\nthermore, for each group we get a centroid vector, which is the average of all mem‐\\nbers of the group (Figure 9-4).\\nFigure 9-4. The structure of a FAISS index: the gray points represent data points added\\nto the index, the bold black points are the cluster centers found via k-means clustering,\\nand the colored areas represent the regions belonging to a cluster center\\nGiven such a grouping, searching among n vectors is much easier: we first search\\nacross the k centroids for the one that is most similar to our query ( k comparisons),\\nand then we search within the group ( kn elements to compare). This reduces the num‐\\nber of comparisons from n to k+nk. So the question is, what is the best option for k?\\nIf it is too small, each group still contains many samples we need to compare against\\nin the second step, and if k is too large there are many centroids we need to search\\nthrough. Looking for the minimum of the function f k = k + n\\nk  with respect to k, we\\nfind k=n. In fact, we can visualize this with the following graphic with n= 220.\\nWorking with a Few Labels | 283'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 307, 'page_label': '284'}, page_content='In the plot you can see the number of comparisons as a function of the number of\\nclusters. We are looking for the minimum of this function, where we need to do the\\nleast comparisons. We can see that the minimum is exactly where we expected to see\\nit, at 220 = 210 = 1, 024.\\nIn addition to speeding up queries with partitioning, FAISS also allows you to utilize\\nGPUs for a further speedup. If memory becomes a concern there are also several\\noptions to compress the vectors with advanced quantization schemes. If you want to\\nuse FAISS for your project, the repository has a simple guide for you to choose the\\nright methods for your use case.\\nOne of the largest projects to use FAISS was the creation of the CCMatrix corpus by\\nFacebook. The authors used multilingual embeddings to find parallel sentences in dif‐\\nferent languages. This enormous corpus was subsequently used to train M2M100, a\\nlarge machine translation model that is able to directly translate between any of 100\\nlanguages.\\nFine-Tuning a Vanilla Transformer\\nIf we have access to labeled data, we can also try to do the obvious thing: simply fine-\\ntune a pretrained transformer model. In this section, we’ll use the standard BERT\\ncheckpoint as a starting point. Later, we’ll see the effect that fine-tuning the language\\nmodel has on performance.\\n284 | Chapter 9: Dealing with Few to No Labels'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 308, 'page_label': '285'}, page_content='For many applications, starting with a pretrained BERT-like model\\nis a good idea. However, if the domain of your corpus differs signif‐\\nicantly from the pretraining corpus (which is usually Wikipedia),\\nyou should explore the many models that are available on the Hug‐\\nging Face Hub. Chances are someone has already pretrained a\\nmodel on your domain!\\nLet’s start by loading the pretrained tokenizer, tokenizing our dataset, and getting rid\\nof the columns we don’t need for training and evaluation:\\nimport torch\\nfrom transformers import (AutoTokenizer, AutoConfig,\\n                          AutoModelForSequenceClassification)\\nmodel_ckpt = \"bert-base-uncased\"\\ntokenizer = AutoTokenizer.from_pretrained(model_ckpt)\\ndef tokenize(batch):\\n    return tokenizer(batch[\"text\"], truncation=True, max_length=128)\\nds_enc = ds.map(tokenize, batched=True)\\nds_enc = ds_enc.remove_columns([\\'labels\\', \\'text\\'])\\nThe multilabel loss function expects the labels to be of type float, since it also allows\\nfor class probabilities instead of discrete labels. Therefore, we need to change the type\\nof the column label_ids. Since changing the format of the column element-wise\\ndoes not play well with Arrow’s typed format, we’ll do a little workaround. First, we\\ncreate a new column with the labels. The format of that column is inferred from the\\nfirst element. Then we delete the original column and rename the new one to take the\\nplace of the original one:\\nds_enc.set_format(\"torch\")\\nds_enc = ds_enc.map(lambda x: {\"label_ids_f\": x[\"label_ids\"].to(torch.float)},\\n                    remove_columns=[\"label_ids\"])\\nds_enc = ds_enc.rename_column(\"label_ids_f\", \"label_ids\")\\nSince we are likely to quickly overfit the training data due to its limited size, we set\\nload_best_model_at_end=True and choose the best model based on the micro\\nF1-score:\\nfrom transformers import Trainer, TrainingArguments\\ntraining_args_fine_tune = TrainingArguments(\\n    output_dir=\"./results\", num_train_epochs=20, learning_rate=3e-5,\\n    lr_scheduler_type=\\'constant\\', per_device_train_batch_size=4,\\n    per_device_eval_batch_size=32, weight_decay=0.0,\\n    evaluation_strategy=\"epoch\", save_strategy=\"epoch\",logging_strategy=\"epoch\",\\n    load_best_model_at_end=True, metric_for_best_model=\\'micro f1\\',\\n    save_total_limit=1, log_level=\\'error\\')\\nWorking with a Few Labels | 285'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 309, 'page_label': '286'}, page_content='We need the F1-score to choose the best model, so we need to make sure it is calcula‐\\nted during the evaluation. Because the model returns the logits, we first need to nor‐\\nmalize the predictions with a sigmoid function and can then binarize them with a\\nsimple threshold. Then we return the scores we are interested in from the classifica‐\\ntion report:\\nfrom scipy.special import expit as sigmoid\\ndef compute_metrics(pred):\\n    y_true = pred.label_ids\\n    y_pred = sigmoid(pred.predictions)\\n    y_pred = (y_pred>0.5).astype(float)\\n    clf_dict = classification_report(y_true, y_pred, target_names=all_labels,\\n                                     zero_division=0, output_dict=True)\\n    return {\"micro f1\": clf_dict[\"micro avg\"][\"f1-score\"],\\n            \"macro f1\": clf_dict[\"macro avg\"][\"f1-score\"]}\\nNow we are ready to rumble! For each training set slice we train a classifier from\\nscratch, load the best model at the end of the training loop, and store the results on\\nthe test set:\\nconfig = AutoConfig.from_pretrained(model_ckpt)\\nconfig.num_labels = len(all_labels)\\nconfig.problem_type = \"multi_label_classification\"\\nfor train_slice in train_slices:\\n    model = AutoModelForSequenceClassification.from_pretrained(model_ckpt,\\n                                                               config=config)\\n    trainer = Trainer(\\n        model=model, tokenizer=tokenizer,\\n        args=training_args_fine_tune,\\n        compute_metrics=compute_metrics,\\n        train_dataset=ds_enc[\"train\"].select(train_slice),\\n        eval_dataset=ds_enc[\"valid\"],)\\n    trainer.train()\\n    pred = trainer.predict(ds_enc[\"test\"])\\n    metrics = compute_metrics(pred)\\n    macro_scores[\"Fine-tune (vanilla)\"].append(metrics[\"macro f1\"])\\n    micro_scores[\"Fine-tune (vanilla)\"].append(metrics[\"micro f1\"])\\nplot_metrics(micro_scores, macro_scores, train_samples, \"Fine-tune (vanilla)\")\\n286 | Chapter 9: Dealing with Few to No Labels'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 310, 'page_label': '287'}, page_content='First of all we see that simply fine-tuning a vanilla BERT model on the dataset leads to\\ncompetitive results when we have access to around 64 examples. We also see that\\nbefore this the behavior is a bit erratic, which is again due to training a model on a\\nsmall sample where some labels can be unfavorably unbalanced. Before we make use\\nof the unlabeled part of our dataset, let’s take a quick look at another promising\\napproach for using language models in the few-shot domain.\\nWorking with a Few Labels | 287'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 311, 'page_label': '288'}, page_content='6 T. Brown et al., “Language Models Are Few-Shot Learners”, (2020).\\n7 D. Tam et al., “Improving and Simplifying Pattern Exploiting Training”, (2021).\\n8 T. Le Scao and A.M. Rush, “How Many Data Points Is a Prompt Worth?”, (2021).\\nIn-Context and Few-Shot Learning with Prompts\\nWe saw earlier in this chapter that we can use a language model like BERT or GPT-2\\nand adapt it to a supervised task by using prompts and parsing the model’s token pre‐\\ndictions. This is different from the classic approach of adding a task-specific head and\\ntuning the model parameters for the task. On the plus side, this approach does not\\nrequire any training data, but on the negative side it seems we can’t leverage labeled\\ndata if we have access to it. There is a middle ground that we can sometimes take\\nadvantage of called in-context or few-shot learning.\\nTo illustrate the concept, consider an English to French translation task. In the zero-\\nshot paradigm, we would construct a prompt that might look as follows:\\nprompt = \"\"\"\\\\\\nTranslate English to French:\\nthanks =>\\n\"\"\"\\nThis hopefully prompts the model to predict the tokens of the word “merci” . We\\nalready saw when using GPT-2 for summarization in Chapter 6 that adding “TL;DR”\\nto a text prompted the model to generate a summary without explicitly being trained\\nto do this. An interesting finding of the GPT-3 paper was the ability of large language\\nmodels to effectively learn from examples presented in the prompt—so, the previous\\ntranslation example could be augmented with several English to German examples,\\nwhich would make the model perform much better on this task.6\\nFurthermore, the authors found that the larger the models are scaled, the better they\\nare at using the in-context examples, leading to significant performance boosts.\\nAlthough GPT-3-sized models are challenging to use in production, this is an excit‐\\ning emerging research field and people have built cool applications, such as a natural\\nlanguage shell where commands are entered in natural language and parsed by\\nGPT-3 to shell commands.\\nAn alternative approach to using labeled data is to create examples of the prompts\\nand desired predictions and continue training the language model on these examples.\\nA novel method called ADAPET uses such an approach and beats GPT-3 on a wide\\nvariety of tasks,7 tuning the model with generated prompts. Recent work by Hugging\\nFace researchers suggests that such an approach can be more data-efficient than fine-\\ntuning a custom head.8\\n288 | Chapter 9: Dealing with Few to No Labels'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 312, 'page_label': '289'}, page_content='In this section we briefly looked at various ways to make good use of the few labeled\\nexamples that we have. Very often we also have access to a lot of unlabeled data in\\naddition to the labeled examples; in the next section we’ll discuss how to make good\\nuse of that.\\nLeveraging Unlabeled Data\\nAlthough having access to large volumes of high-quality labeled data is the best-case\\nscenario to train a classifier, this does not mean that unlabeled data is worthless. Just\\nthink about the pretraining of most models we have used: even though they are\\ntrained on mostly unrelated data from the internet, we can leverage the pretrained\\nweights for other tasks on a wide variety of texts. This is the core idea of transfer\\nlearning in NLP . Naturally, if the downstream task has similar textual structure as the\\npretraining texts the transfer works better, so if we can bring the pretraining task\\ncloser to the downstream objective we could potentially improve the transfer.\\nLet’s think about this in terms of our concrete use case: BERT is pretrained on the\\nBookCorpus and English Wikipedia, and texts containing code and GitHub issues are\\ndefinitely a small niche in these datasets. If we pretrained BERT from scratch we\\ncould do it on a crawl of all of the issues on GitHub, for example. However, this\\nwould be expensive, and a lot of aspects about language that BERT has learned are\\nstill valid for GitHub issues. So is there a middle ground between retraining from\\nscratch and just using the model as is for classification? There is, and it is called\\ndomain adaptation (which we also saw for question answering in Chapter 7). Instead\\nof retraining the language model from scratch, we can continue training it on data\\nfrom our domain. In this step we use the classic language model objective of predict‐\\ning masked words, which means we don’t need any labeled data. After that we can\\nload the adapted model as a classifier and fine-tune it, thus leveraging the unlabeled\\ndata.\\nThe beauty of domain adaptation is that compared to labeled data, unlabeled data is\\noften abundantly available. Furthermore, the adapted model can be reused for many\\nuse cases. Imagine you want to build an email classifier and apply domain adaptation\\non all your historic emails. Y ou can later use the same model for named entity recog‐\\nnition or another classification task like sentiment analysis, since the approach is\\nagnostic to the downstream task.\\nLet’s now see the steps we need to take to fine-tune a pretrained language model.\\nFine-Tuning a Language Model\\nIn this section we’ll fine-tune the pretrained BERT model with masked language\\nmodeling on the unlabeled portion of our dataset. To do this we only need two new\\nLeveraging Unlabeled Data | 289'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 313, 'page_label': '290'}, page_content='concepts: an extra step when tokenizing the data and a special data collator. Let’s start\\nwith the tokenization.\\nIn addition to the ordinary tokens from the text the tokenizer also adds special\\ntokens to the sequence, such as the [CLS] and [SEP] tokens that are used for classifi‐\\ncation and next sentence prediction. When we do masked language modeling, we\\nwant to make sure we don’t train the model to also predict these tokens. For this rea‐\\nson we mask them from the loss, and we can get a mask when tokenizing by setting\\nreturn_special_tokens_mask=True. Let’s retokenize the text with that setting:\\ndef tokenize(batch):\\n    return tokenizer(batch[\"text\"], truncation=True,\\n                     max_length=128, return_special_tokens_mask=True)\\nds_mlm = ds.map(tokenize, batched=True)\\nds_mlm = ds_mlm.remove_columns([\"labels\", \"text\", \"label_ids\"])\\nWhat’s missing to start with masked language modeling is the mechanism to mask\\ntokens in the input sequence and have the target tokens in the outputs. One way we\\ncould approach this is by setting up a function that masks random tokens and creates\\nlabels for these sequences. But this would double the size of the dataset, since we\\nwould also store the target sequence in the dataset, and it would mean we would use\\nthe same masking of a sequence every epoch.\\nA much more elegant solution is to use a data collator. Remember that the data colla‐\\ntor is the function that builds the bridge between the dataset and the model calls. A\\nbatch is sampled from the dataset, and the data collator prepares the elements in the\\nbatch to feed them to the model. In the simplest case we have encountered, it simply\\nconcatenates the tensors of each element into a single tensor. In our case we can use it\\nto do the masking and label generation on the fly. That way we don’t need to store the\\nlabels and we get new masks every time we sample. The data collator for this task is\\ncalled DataCollatorForLanguageModeling. We initialize it with the model’s tokenizer\\nand the fraction of tokens we want to mask via the mlm_probability argument. We’ll\\nuse this collator to mask 15% of the tokens, which follows the procedure in the BERT\\npaper:\\nfrom transformers import DataCollatorForLanguageModeling, set_seed\\ndata_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer,\\n                                                mlm_probability=0.15)\\nLet’s have a quick look at the data collator in action to see what it actually does. To\\nquickly show the results in a DataFrame, we switch the return formats of the token‐\\nizer and the data collator to NumPy:\\nset_seed(3)\\ndata_collator.return_tensors = \"np\"\\ninputs = tokenizer(\"Transformers are awesome!\", return_tensors=\"np\")\\n290 | Chapter 9: Dealing with Few to No Labels'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 314, 'page_label': '291'}, page_content='outputs = data_collator([{\"input_ids\": inputs[\"input_ids\"][0]}])\\npd.DataFrame({\\n    \"Original tokens\": tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0]),\\n    \"Masked tokens\": tokenizer.convert_ids_to_tokens(outputs[\"input_ids\"][0]),\\n    \"Original input_ids\": original_input_ids,\\n    \"Masked input_ids\": masked_input_ids,\\n    \"Labels\": outputs[\"labels\"][0]}).T\\n0 1 2 3 4 5\\nOriginal tokens [CLS] transformers are awesome ! [SEP]\\nMasked tokens [CLS] transformers are awesome [MASK] [SEP]\\nOriginal input_ids 101 19081 2024 12476 999 102\\nMasked input_ids 101 19081 2024 12476 103 102\\nLabels -100 -100 -100 -100 999 -100\\nWe see that the token corresponding to the exclamation mark has been replaced with\\na mask token. In addition, the data collator returned a label array, which is –100 for\\nthe original tokens and the token ID for the masked tokens. As we have seen previ‐\\nously, the entries containing –100 are ignored when calculating the loss. Let’s switch\\nthe format of the data collator back to PyTorch:\\ndata_collator.return_tensors = \"pt\"\\nWith the tokenizer and data collator in place, we are ready to fine-tune the masked\\nlanguage model. We set up the TrainingArguments and Trainer as usual:\\nfrom transformers import AutoModelForMaskedLM\\ntraining_args = TrainingArguments(\\n    output_dir = f\"{model_ckpt}-issues-128\", per_device_train_batch_size=32,\\n    logging_strategy=\"epoch\", evaluation_strategy=\"epoch\", save_strategy=\"no\",\\n    num_train_epochs=16, push_to_hub=True, log_level=\"error\", report_to=\"none\")\\ntrainer = Trainer(\\n        model=AutoModelForMaskedLM.from_pretrained(\"bert-base-uncased\"),\\n        tokenizer=tokenizer, args=training_args, data_collator=data_collator,\\n        train_dataset=ds_mlm[\"unsup\"], eval_dataset=ds_mlm[\"train\"])\\ntrainer.train()\\ntrainer.push_to_hub(\"Training complete!\")\\nWe can access the trainer’s log history to look at the training and validation losses of\\nthe model. All logs are stored in trainer.state.log_history as a list of dictionaries\\nthat we can easily load into a Pandas DataFrame. Since the training and validation loss\\nare recorded at different steps, there are missing values in the dataframe. For this rea‐\\nson we drop the missing values before plotting the metrics:\\nLeveraging Unlabeled Data | 291'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 315, 'page_label': '292'}, page_content='df_log = pd.DataFrame(trainer.state.log_history)\\n(df_log.dropna(subset=[\"eval_loss\"]).reset_index()[\"eval_loss\"]\\n .plot(label=\"Validation\"))\\ndf_log.dropna(subset=[\"loss\"]).reset_index()[\"loss\"].plot(label=\"Train\")\\nplt.xlabel(\"Epochs\")\\nplt.ylabel(\"Loss\")\\nplt.legend(loc=\"upper right\")\\nplt.show()\\nIt seems that both the training and validation loss went down considerably. So let’s\\ncheck if we can also see an improvement when we fine-tune a classifier based on this\\nmodel.\\n292 | Chapter 9: Dealing with Few to No Labels'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 316, 'page_label': '293'}, page_content='Fine-Tuning a Classifier\\nNow we’ll repeat the fine-tuning procedure, but with the slight difference that we load\\nour own custom checkpoint:\\nmodel_ckpt = f\\'{model_ckpt}-issues-128\\'\\nconfig = AutoConfig.from_pretrained(model_ckpt)\\nconfig.num_labels = len(all_labels)\\nconfig.problem_type = \"multi_label_classification\"\\nfor train_slice in train_slices:\\n    model = AutoModelForSequenceClassification.from_pretrained(model_ckpt,\\n                                                               config=config)\\n    trainer = Trainer(\\n        model=model,\\n        tokenizer=tokenizer,\\n        args=training_args_fine_tune,\\n        compute_metrics=compute_metrics,\\n        train_dataset=ds_enc[\"train\"].select(train_slice),\\n        eval_dataset=ds_enc[\"valid\"],\\n    )\\n    trainer.train()\\n    pred = trainer.predict(ds_enc[\\'test\\'])\\n    metrics = compute_metrics(pred)\\n    # DA refers to domain adaptation\\n    macro_scores[\\'Fine-tune (DA)\\'].append(metrics[\\'macro f1\\'])\\n    micro_scores[\\'Fine-tune (DA)\\'].append(metrics[\\'micro f1\\'])\\nComparing the results to the fine-tuning based on vanilla BERT, we see that we get an\\nadvantage especially in the low-data domain. We also gain a few percentage points in\\nthe regime where more labeled data is available:\\nplot_metrics(micro_scores, macro_scores, train_samples, \"Fine-tune (DA)\")\\nLeveraging Unlabeled Data | 293'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 317, 'page_label': '294'}, page_content='This highlights that domain adaptation can provide a slight boost to the model’s per‐\\nformance with unlabeled data and little effort. Naturally, the more unlabeled data and\\nthe less labeled data you have, the more impact you will get with this method. Before\\nwe conclude this chapter, we’ll show you a few more tricks for taking advantage of\\nunlabeled data.\\n294 | Chapter 9: Dealing with Few to No Labels'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 318, 'page_label': '295'}, page_content='Advanced Methods\\nFine-tuning the language model before tuning the classification head is a simple yet\\nreliable method to boost performance. However, there are sophisticated methods\\nthan can leverage unlabeled data even further. We summarize a few of these methods\\nhere, which should provide a good starting point if you need more performance.\\nUnsupervised data augmentation\\nThe key idea behind unsupervised data augmentation (UDA) is that a model’s predic‐\\ntions should be consistent for an unlabeled example and a slightly distorted one. Such\\ndistortions are introduced with standard data augmentation strategies such as token\\nreplacement and back translation. Consistency is then enforced by minimizing the\\nKL divergence between the predictions of the original and distorted examples. This\\nprocess is illustrated in Figure 9-5, where the consistency requirement is incorporated\\nby augmenting the cross-entropy loss with an additional term from the unlabeled\\nexamples. This means that one trains a model on the labeled data with the standard\\nsupervised approach, but constrains the model to make consistent predictions on the\\nunlabeled data.\\nFigure 9-5. Training a model M with UDA (courtesy of Qizhe Xie)\\nThe performance of this approach is quite impressive: with a handful of labeled\\nexamples, BERT models trained with UDA get similar performance to models trained\\non thousands of examples. The downside is that you need a data augmentation pipe‐\\nline, and training takes much longer since you need multiple forward passes to gener‐\\nate the predicted distributions on the unlabeled and augmented examples.\\nUncertainty-aware self-training\\nAnother promising method to leverage unlabeled data is uncertainty-aware self-\\ntraining (UST). The idea here is to train a teacher model on the labeled data and then\\nLeveraging Unlabeled Data | 295'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 319, 'page_label': '296'}, page_content='9 S. Mukherjee and A.H. Awadallah, “Uncertainty-Aware Self-Training for Few-Shot Text Classification”,\\n(2020).\\nuse that model to create pseudo-labels on the unlabeled data. Then a student is\\ntrained on the pseudo-labeled data, and after training it becomes the teacher for the\\nnext iteration.\\nOne interesting aspect of this method is how the pseudo-labels are generated: to get\\nan uncertainty measure of the model’s predictions the same input is fed several times\\nthrough the model with dropout turned on. Then the variance in the predictions\\ngives a proxy for the certainty of the model on a specific sample. With that uncer‐\\ntainty measure the pseudo-labels are then sampled using a method called Bayesian\\nActive Learning by Disagreement (BALD). The full training pipeline is illustrated in\\nFigure 9-6.\\nFigure 9-6. The UST method consists of a teacher that generates pseudo-labels and a stu‐\\ndent that is subsequently trained on those labels; after the student is trained it becomes\\nthe teacher and the step is repeated (courtesy of Subhabrata Mukherjee)9\\nWith this iteration scheme the teacher continuously gets better at creating pseudo-\\nlabels, and thus the model’s performance improves. In the end this approach gets\\nwithin a few percent of models trained on the full training data with thousands of\\nsamples and even beats UDA on several datasets.\\nNow that we’ve seen a few advanced methods, let’s take a step back and summarize\\nwhat we’ve learned in this chapter.\\n296 | Chapter 9: Dealing with Few to No Labels'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 320, 'page_label': '297'}, page_content='Conclusion\\nIn this chapter we’ve seen that even if we have only a few or even no labels, not all\\nhope is lost. We can utilize models that have been pretrained on other tasks, such as\\nthe BERT language model or GPT-2 trained on Python code, to make predictions on\\nthe new task of GitHub issue classification. Furthermore, we can use domain adapta‐\\ntion to get an additional boost when training the model with a normal classification\\nhead.\\nWhich of the presented approaches will work best on a specific use case depends on a\\nvariety of aspects: how much labeled data you have, how noisy is it, how close the\\ndata is to the pretraining corpus, and so on. To find out what works best, it is a good\\nidea to set up an evaluation pipeline and then iterate quickly. The flexible API of \\n Transformers allows you to quickly load a handful of models and compare them\\nwithout the need for any code changes. There are over 10,000 models on the Hugging\\nFace Hub, and chances are somebody has worked on a similar problem in the past\\nand you can build on top of this.\\nOne aspect that is beyond the scope of this book is the trade-off between a more com‐\\nplex approach like UDA or UST and getting more data. To evaluate your approach, it\\nmakes sense to at least build a validation and test set early on. At every step of the way\\nyou can also gather more labeled data. Usually annotating a few hundred examples is\\na matter of a couple of hours’ or a few days’ work, and there are many tools that can\\nassist you in doing so. Depending on what you are trying to achieve, it can make\\nsense to invest some time in creating a small, high-quality dataset rather than engi‐\\nneering a very complex method to compensate for the lack thereof. With the methods\\nwe’ve presented in this chapter you can ensure that you get the most value out of your\\nprecious labeled data.\\nHere, we have ventured into the low-data regime and seen that transformer models\\nare still powerful even with just a hundred examples. In the next chapter we’ll look at\\nthe complete opposite case: we’ll see what we can do when we have hundreds of giga‐\\nbytes of data and a lot of compute. We’ll train a large transformer model from scratch\\nto autocomplete code for us.\\nConclusion | 297'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 321, 'page_label': '298'}, page_content=''), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 322, 'page_label': '299'}, page_content='CHAPTER 10\\nTraining Transformers from Scratch\\nIn the opening paragraph of this book, we mentioned a sophisticated application\\ncalled GitHub Copilot that uses GPT-like transformers to perform code autocomple‐\\ntion, a feature that is particularly useful when programming in a new language or\\nframework or learning to code, or for automatically producing boilerplate code.\\nOther products that use AI models for this purpose include TabNine and Kite. Later,\\nin Chapter 5, we had a closer look at how we can use GPT models to generate high-\\nquality text. In this chapter, we’ll close the circle and build our very own GPT-like\\nmodel for generating Python source code! We call the resulting model CodeParrot.\\nSo far we’ve mostly worked on data-constrained applications where the amount of\\nlabeled training data is limited. In these cases, transfer learning helped us build per‐\\nformant models. We took transfer learning to the limit in Chapter 9, where we barely\\nused any training data at all.\\nIn this chapter we’ll move to the other extreme and look at what we can do when we\\nare drowning in all the data we could possibly want. We’ll explore the pretraining step\\nitself and learn how to train a transformer from scratch. In working through this\\nproblem, we’ll look at some aspects of training that we have not considered yet, such\\nas the following:\\n• Gathering and processing a very large dataset\\n• Creating a custom tokenizer for our dataset\\n• Training a model on multiple GPUs at scale\\nTo efficiently train large models with billions of parameters, we’ll need special tools\\nfor distributed training. Although the Trainer from \\n  Transformers supports dis‐\\ntributed training, we’ll take the opportunity to showcase a powerful PyTorch library\\n299'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 323, 'page_label': '300'}, page_content='called \\n  Accelerate. We’ll end up touching on some of the largest NLP models in use\\ntoday—but first, we need to find a sufficiently large dataset.\\nUnlike the code in the others in this book (which can be run with a\\nJupyter notebook on a single GPU), the training code in this chap‐\\nter is designed to be run as a script with multiple GPUs. If you want\\nto train your own version of CodeParrot, we recommend running\\nthe script provided in the \\n  Transformers repository.\\nLarge Datasets and Where to Find Them\\nThere are many domains where you may actually have a large amount of data at\\nhand, ranging from legal documents to biomedical datasets to programming codeba‐\\nses. In most cases, these datasets are unlabeled, and their large size means that they\\ncan usually only be labeled through the use of heuristics, or by using accompanying\\nmetadata that is stored during the gathering process.\\nNevertheless, a very large corpus can be useful even when it is unlabeled or only heu‐\\nristically labeled. We saw an example of this in Chapter 9, where we used the unla‐\\nbeled part of a dataset to fine-tune a language model for domain adaptation. This\\napproach typically yields a performance gain when limited data is available. The deci‐\\nsion to train from scratch rather than fine-tune an existing model is mostly dictated\\nby the size of your fine-tuning corpus and the domain differences between the avail‐\\nable pretrained models and the corpus.\\nUsing a pretrained model forces you to use the model’s corresponding tokenizer, but\\nusing a tokenizer that is trained on a corpus from another domain is typically subop‐\\ntimal. For example, using GPT’s pretrained tokenizer on legal documents, other lan‐\\nguages, or even completely different sequences such as musical notes or DNA\\nsequences will result in poor tokenization (as we will see shortly).\\nAs the amount of training data you have access to gets closer to the amount of data\\nused for pretraining, it thus becomes interesting to consider training the model and\\nthe tokenizer from scratch, provided the necessary computational resources are avail‐\\nable. Before we discuss the different pretraining objectives further, we first need to\\nbuild a large corpus suitable for pretraining. Building such a corpus comes with its\\nown set of challenges, which we’ll explore in the next section.\\nChallenges of Building a Large-Scale Corpus\\nThe quality of a model after pretraining largely reflects the quality of the pretraining\\ncorpus. In particular, the model will inherit any defects in the pretraining corpus.\\nThus, before we attempt to create one of our own it’s good to be aware of some of the\\n300 | Chapter 10: Training Transformers from Scratch'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 324, 'page_label': '301'}, page_content='1 Y . Zhu et al., “ Aligning Books and Movies: Towards Story-Like Visual Explanations by Watching Movies and\\nReading Books”, (2015); J. Dodge et al., “Documenting the English Colossal Clean Crawled Corpus”, (2021).\\n2 J. Bandy and N. Vincent, “ Addressing Documentation Debt in Machine Learning Research: A Retrospective\\nDatasheet for BookCorpus”, (2021).\\ncommon issues and challenges that are associated with building large corpora for\\npretraining.\\nAs the dataset gets larger and larger, the chances that you can fully control—or at\\nleast have a precise idea of—what is inside it diminish. A very large dataset will most\\nlikely not have been assembled by dedicated creators that craft one example at a time,\\nwhile being aware and knowledgeable of the full pipeline and the task that the\\nmachine learning model will be applied to. Instead, it is much more likely that a very\\nlarge dataset will have been created in an automatic or semiautomatic way by collect‐\\ning data that is generated as a side effect of other activities. For instance, it may con‐\\nsist of all the documents (e.g., contracts, purchase orders, etc.) that a company stores,\\nlogs from user activities, or data gathered from the internet.\\nThere are several important consequences that follow from the fact that large-scale\\ndatasets are mostly created with a high degree of automation. An important consider‐\\nation is that there is limited control over both their content and the way they are cre‐\\nated, and thus the risk of training a model on biased and lower-quality data increases.\\nRecent investigations of famous large-scale datasets like BookCorpus and C4, which\\nwere used to train BERT and T5, respectively, have uncovered (among other things)\\nthat:1\\n• A significant proportion of the C4 corpus is machine-translated rather than\\ntranslated by humans.\\n• Disparate erasure of African-American English as a result of stopword filtering\\nin C4 has resulted in an underrepresentation of such content.\\n• It is typically difficult in a large text corpus to find a middle ground between\\nincluding (often too much) sexually or other explicit content and totally erasing\\nall mention of sexuality or gender. As a surprising consequence of this, a rather\\ncommon word like “sex” (which can have both neutral and explicit meanings) is\\ncompletely unknown to a tokenizer that is trained on C4, since this word is fully\\nabsent from the corpus.\\n• There are many occurrences of copyright violation in BookCorpus, and probably\\nin other large-scale datasets as well.2\\n• There is genre skew toward “romance” novels in BookCorpus.\\nThese discoveries might not be incompatible with downstream usage of the models\\ntrained on these corpora. For instance, the strong overrepresentation of romance\\nLarge Datasets and Where to Find Them | 301'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 325, 'page_label': '302'}, page_content='novels in BookCorpus is probably acceptable if the model is intended to be used as a\\nromance novel writing tool or for a building a game.\\nLet’s illustrate the notion of a model being skewed by the data by comparing text gen‐\\nerations from GPT and GPT-2. GPT was mostly trained on BookCorpus, while\\nGPT-2 was trained on web pages, blogs, and news articles linked from Reddit. We’ll\\ncompare similar-sized versions of both models on the same prompt, so that the main\\ndifference is the pretraining dataset, and we’ll use the text-generation pipeline to\\ninvestigate the model outputs:\\nfrom transformers import pipeline, set_seed\\ngeneration_gpt = pipeline(\"text-generation\", model=\"openai-gpt\")\\ngeneration_gpt2 = pipeline(\"text-generation\", model=\"gpt2\")\\nNext, let’s create a simple function to count the number of parameters in each model:\\ndef model_size(model):\\n    return sum(t.numel() for t in model.parameters())\\nprint(f\"GPT  size: {model_size(generation_gpt.model)/1000**2:.1f}M parameters\")\\nprint(f\"GPT2 size: {model_size(generation_gpt2.model)/1000**2:.1f}M parameters\")\\nGPT  size: 116.5M parameters\\nGPT2 size: 124.4M parameters\\nThe original GPT model is about the same size as the smallest GPT-2 model. Now we\\ncan generate three different completions from each model, each with the same input\\nprompt:\\ndef enum_pipeline_ouputs(pipe, prompt, num_return_sequences):\\n    out = pipe(prompt, num_return_sequences=num_return_sequences,\\n               clean_up_tokenization_spaces=True)\\n    return \"\\\\n\".join(f\"{i+1}.\" + s[\"generated_text\"] for i, s in enumerate(out))\\nprompt = \"\\\\nWhen they came back\"\\nprint(\"GPT completions:\\\\n\" + enum_pipeline_ouputs(generation_gpt, prompt, 3))\\nprint(\"\")\\nprint(\"GPT-2 completions:\\\\n\" + enum_pipeline_ouputs(generation_gpt2, prompt, 3))\\nGPT completions:\\n1.\\nWhen they came back.\\n \" we need all we can get, \" jason said once they had settled into the back of\\nthe truck without anyone stopping them. \" after getting out here, it \\'ll be up\\nto us what to find. for now\\n2.\\nWhen they came back.\\n his gaze swept over her body. he \\'d dressed her, too, in the borrowed clothes\\nthat she \\'d worn for the journey.\\n \" i thought it would be easier to just leave you there. \" a woman like\\n3.\\nWhen they came back to the house and she was sitting there with the little boy.\\n302 | Chapter 10: Training Transformers from Scratch'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 326, 'page_label': '303'}, page_content='3 B. Hutchinson et al., “Towards Accountability for Machine Learning Datasets: Practices from Software Engi‐\\nneering and Infrastructure”, (2020).\\n4 By comparison, GitHub Copilot supports over a dozen programming languages.\\n \" don\\'t be afraid, \" he told her. she nodded slowly, her eyes wide. she was so\\nlost in whatever she discovered that tom knew her mistake\\nGPT-2 completions:\\n1.\\nWhen they came back we had a big dinner and the other guys went to see what\\ntheir opinion was on her. I did an hour and they were happy with it.\\n2.\\nWhen they came back to this island there had been another massacre, but he could\\nnot help but feel pity for the helpless victim who had been left to die, and\\nthat they had failed that day. And so was very, very grateful indeed.\\n3.\\nWhen they came back to our house after the morning, I asked if she was sure. She\\nsaid, \"Nope.\" The two kids were gone that morning. I thought they were back to\\nbeing a good friend.\\nWhen Dost\\nBy just sampling a handful of outputs from both models we can already see the dis‐\\ntinctive “romance” skew in GPT generation, which will typically imagine a dialogue\\nwith a romantic interaction between a woman and a man. On the other hand, GPT-2\\nwas trained on webtext linked to and from Reddit articles and mostly adopts a neu‐\\ntral “they” in its generations, which contain “blog-like” or adventure-related elements.\\nIn general, any model trained on a dataset will reflect the language bias and over- or\\nunderrepresentation of populations and events in its training data. These biases in the\\nbehavior of the model are important to take into consideration with regard to the tar‐\\nget audience interacting with the model; for some useful guidelines, we refer you to a\\npaper by Google that provides a framework for dataset development.3\\nThis brief introduction should give you an idea of the difficult challenges you face\\nwhen creating large text corpora. With these in mind, let’s now take a look at creating\\nour own dataset!\\nBuilding a Custom Code Dataset\\nTo simplify the task a bit, we’ll focus on building a code generation model for the\\nPython programming language only.4 The first thing we’ll need is a large pretraining\\ncorpus consisting of Python source code. Fortunately, there is a natural resource that\\nevery software engineer knows: GitHub! The famous code-sharing website hosts\\nterabytes of code repositories that are openly accessible and can be downloaded and\\nused according to their respective licenses. At the time of this book’s writing, GitHub\\nLarge Datasets and Where to Find Them | 303'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 327, 'page_label': '304'}, page_content='5 M.-A. Lachaux et al., “Unsupervised Translation of Programming Languages”, (2020).\\nhosts more than 20 million code repositories. Many of them are small or test reposi‐\\ntories created by users for learning, future side projects, or testing purposes.\\nGitHub repositories can be accessed in two main ways:\\n• Via the GitHub REST API, like we saw in Chapter 9 when we downloaded all the\\nGitHub issues of the \\n  Transformers repository\\n• Via public dataset inventories like Google BigQuery\\nSince the REST API is rate limited and we need a lot data for our pretraining corpus,\\nwe’ll use Google BigQuery to extract all the Python repositories. The bigquery-\\npublic-data.github_repos.contents table contains copies of all ASCII files that are\\nless than 10 MB in size. Projects also need to be open source to be included, as deter‐\\nmined by GitHub’s License API.\\nThe Google BigQuery dataset doesn’t contain star or downstream\\nusage information. For those attributes, we can use the GitHub\\nREST API or a service like Libraries.io that monitors open source\\npackages. Indeed, a team from GitHub recently released a dataset\\ncalled CodeSearchNet that filtered repositories used in at least one\\ndownstream task using information from Libraries.io.\\nLet’s have a look at what it takes to create our code dataset with Google BigQuery.\\nCreating a dataset with Google BigQuery\\nWe’ll begin by extracting all the Python files in GitHub public repositories from the\\nsnapshot on Google BigQuery. For the sake of reproducibility and in case the policy\\naround free usage of BigQuery changes in the future, we will also share this dataset\\non the Hugging Face Hub. The steps to export these files are adapted from the Trans‐\\nCoder implementation and are as follows:5\\n1. Create a Google Cloud account (a free trial should be sufficient).\\n2. Create a Google BigQuery project under your account.\\n3. In this project, create a dataset.\\n4. In this dataset, create a table where the results of the SQL request will be stored.\\n5. Prepare and run the following SQL query on the github_repos (to save the\\nquery results, select More > Query Options, check the “Set a destination table for\\nquery results” box, and specify the table name):\\n304 | Chapter 10: Training Transformers from Scratch'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 328, 'page_label': '305'}, page_content='SELECT\\n  f.repo_name, f.path, c.copies, c.size, c.content, l.license\\nFROM\\n  `bigquery-public-data.github_repos.files` AS f\\nJOIN\\n  `bigquery-public-data.github_repos.contents` AS c\\nON\\n  f.id = c.id\\nJOIN\\n  `bigquery-public-data.github_repos.licenses` AS l\\nON\\n  f.repo_name = l.repo_name\\nWHERE\\n  NOT c.binary\\n  AND ((f.path LIKE \\'%.py\\')\\n    AND (c.size BETWEEN 1024\\n      AND 1048575))\\nThis command processes about 2.6 TB of data to extract 26.8 million files. The result\\nis a dataset of about 50 GB of compressed JSON files, each containing the source code\\nof Python files. We filtered to remove empty files and small files such as __init__.py\\nthat don’t contain much useful information. We also filtered out files larger than 1\\nMB, and we downloaded the licenses for all the files so we can filter the training data\\nbased on licenses if we want later on.\\nNext, we’ll download the results to our local machine. If you try this at home, make\\nsure you have good bandwidth available and at least 50 GB of free disk space. The\\neasiest way to get the resulting table to your local machine is to follow this two-step\\nprocess:\\n1. Export your results to Google Cloud:\\na. Create a bucket and a folder in Google Cloud Storage (GCS).\\nb. Export your table to this bucket by selecting Export > Export to GCS, with an\\nexport format of JSON and gzip compression.\\n2. To download the bucket to your machine, use the gsutil library:\\na. Install gsutil with pip install gsutil.\\nb. Configure gsutil with your Google account: gsutil config.\\nc. Copy your bucket on your machine:\\n$ gsutil -m -o\\n\"GSUtil:parallel_process_count=1\" cp -r gs://<name_of_bucket>\\nAlternatively, you can directly download the dataset from the Hugging Face Hub with\\nthe following command:\\nLarge Datasets and Where to Find Them | 305'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 329, 'page_label': '306'}, page_content='$ git clone https://huggingface.co/datasets/transformersbook/codeparrot\\nTo Filter the Noise or Not?\\nAnybody can create a GitHub repository, so the quality of the projects varies. There\\nare some conscious choices to be made regarding how we want the system to perform\\nin a real-world setting. Having some noise in the training dataset will make our sys‐\\ntem more robust to noisy inputs at inference time, but will also make its predictions\\nmore random. Depending on the intended use and whole system integration, you\\nmay choose more or less noisy data and add pre- and postfiltering operations.\\nFor the educational purposes of the present chapter and to keep the data preparation\\ncode concise, we will not filter according to stars or usage and will just grab all the\\nPython files in the GitHub BigQuery dataset. Data preparation, however, is a crucial\\nstep, and you should make sure you clean up your dataset as much as possible. In our\\ncase a few things to consider are whether to balance the programming languages in\\nthe dataset; filter low-quality data (e.g., via GitHub stars or references from other\\nrepos); remove duplicated code samples; take copyright information into account;\\ninvestigate the language used in documentation, comments, or docstrings; and\\nremove personal identifying information such as passwords or keys.\\nWorking with a 50 GB dataset can be challenging; it requires sufficient disk space,\\nand one must be careful not to run out of RAM. In the following section, we’ll have a\\nlook how \\n  Datasets helps deal with these constraints of working with large datasets\\non small machines.\\nWorking with Large Datasets\\nLoading a very large dataset is often a challenging task, in particular when the data is\\nlarger than your machine’s RAM. For a large-scale pretraining dataset, this is a very\\ncommon situation. In our example, we have 50 GB of compressed data and about 200\\nGB of uncompressed data, which is difficult to extract and load into the RAM mem‐\\nory of a standard-sized laptop or desktop computer.\\nThankfully, \\n  Datasets has been designed from the ground up to overcome this\\nproblem with two specific features that allow you to set yourself free from RAM and\\nhard drive space limitations: memory mapping and streaming.\\nMemory mapping\\nTo overcome RAM limitations, \\n  Datasets uses a mechanism for zero-copy and zero-\\noverhead memory mapping that is activated by default. Basically, each dataset is\\ncached on the drive in a file that is a direct reflection of the content in RAM memory.\\nInstead of loading the dataset in RAM, \\n  Datasets opens a read-only pointer to this\\n306 | Chapter 10: Training Transformers from Scratch'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 330, 'page_label': '307'}, page_content='file and uses it as a substitute for RAM, basically using the hard drive as a direct\\nextension of the RAM memory.\\nUp to now we have mostly used \\n  Datasets to access remote datasets on the Hugging\\nFace Hub. Here, we will directly load our 50 GB of compressed JSON files that we\\nhave stored locally in the codeparrot repository. Since the JSON files are com‐\\npressed, we first need to decompress them, which \\n  Datasets takes care of for us. Be\\ncareful, because this requires about 180 GB of free disk space! However, it will use\\nalmost no RAM. By setting delete_extracted=True in the dataset’s downloading\\nconfiguration, we can make sure that we delete all the files we don’t need anymore as\\nsoon as possible:\\nfrom datasets import load_dataset, DownloadConfig\\ndownload_config = DownloadConfig(delete_extracted=True)\\ndataset = load_dataset(\"./codeparrot\", split=\"train\",\\n                       download_config=download_config)\\nUnder the hood, \\n  Datasets extracted and read all the compressed JSON files by\\nloading them in a single optimized cache file. Let’s see how big this dataset is once\\nloaded:\\nimport psutil\\nprint(f\"Number of python files code in dataset : {len(dataset)}\")\\nds_size = sum(os.stat(f[\"filename\"]).st_size for f in dataset.cache_files)\\n# os.stat.st_size is expressed in bytes, so we convert to GB\\nprint(f\"Dataset size (cache file) : {ds_size / 2**30:.2f} GB\")\\n# Process.memory_info is expressed in bytes, so we convert to MB\\nprint(f\"RAM used: {psutil.Process(os.getpid()).memory_info().rss >> 20} MB\")\\nNumber of python files code in dataset : 18695559\\nDataset size (cache file) : 183.68 GB\\nRAM memory used: 4924 MB\\nAs we can see, the dataset is much larger than our typical RAM memory, but we can\\nstill load and access it, and we’re actually using a very limited amount of memory.\\nY ou may wonder if this will make our training I/O-bound. In practice, NLP data is\\nusually very lightweight to load in comparison to the model processing computa‐\\ntions, so this is rarely an issue. In addition, the zero-copy/zero-overhead format uses\\nApache Arrow under the hood, which makes it very efficient to access any element.\\nDepending on the speed of your hard drive and the batch size, iterating over the\\ndataset can typically be done at a rate of a few tenths of a GB/s to several GB/s. This is\\ngreat, but what if you can’t free enough disk space to store the full dataset locally?\\nEverybody knows the feeling of helplessness when you get a full disk warning and\\nneed to painfully try to reclaim a few GB by looking for hidden files to delete. Luckily,\\nyou don’t need to store the full dataset locally if you use the streaming feature of\\n Datasets!\\nLarge Datasets and Where to Find Them | 307'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 331, 'page_label': '308'}, page_content='Streaming\\nSome datasets (reaching up to 1 TB or more) will be difficult to fit even on a standard\\nhard drive. In this case, an alternative to scaling up the server you are using is to\\nstream the dataset. This is also possible with \\n  Datasets for a number of compressed\\nor uncompressed file formats that can be read line by line, like JSON Lines, CSV , or\\ntext (either raw or zip, gzip, or zstandard compressed). Let’s load our dataset directly\\nfrom the compressed JSON files instead of creating a cache file from them:\\nstreamed_dataset = load_dataset(\\'./codeparrot\\', split=\"train\", streaming=True)\\nAs you’ll see, loading the dataset is instantaneous! In streaming mode, the com‐\\npressed JSON files will be opened and read on the fly. Our dataset is now an Iterable\\nDataset object. This means that we cannot access random elements of it, like\\nstreamed_dataset[1264], but we need to read it in order, for instance with\\nnext(iter(streamed_dataset)). It’s still possible to use methods like shuffle(), but\\nthese will operate by fetching a buffer of examples and shuffling within this buffer\\n(the size of the buffer is adjustable). When several files are provided as raw files (like\\nour 184 files here), shuffle() will also randomize the order of files for the iteration.\\nThe samples of a streamed dataset are identical to the samples of a nonstreamed data‐\\nset, as we can see:\\niterator = iter(streamed_dataset)\\nprint(dataset[0] == next(iterator))\\nprint(dataset[1] == next(iterator))\\nTrue\\nTrue\\nThe main interest of using a streaming dataset is that loading this dataset will not cre‐\\nate a cache file on the drive or require any (significant) RAM memory. The original\\nraw files are extracted and read on the fly when a new batch of examples is requested,\\nand only that batch is loaded in memory. This reduces the memory footprint of our\\ndataset from 180 GB to 50 GB. But we can take this one step further—instead of\\npointing to the local dataset we can reference the dataset on the Hub, and then\\ndirectly download samples without downloading the raw files locally:\\nremote_dataset = load_dataset(\\'transformersbook/codeparrot\\', split=\"train\",\\n                              streaming=True)\\nThis dataset behaves exactly like the previous one, but behind the scenes downloads\\nthe examples on the fly. With such a setup, we can then use arbitrarily large datasets\\non an (almost) arbitrarily small server. Let’s push our dataset with a train and valida‐\\ntion split to the Hugging Face Hub and access it with streaming.\\n308 | Chapter 10: Training Transformers from Scratch'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 332, 'page_label': '309'}, page_content='Adding Datasets to the Hugging Face Hub\\nPushing our dataset to the Hugging Face Hub will allow us to:\\n• Easily access it from our training server.\\n• See how streaming datasets work seamlessly with datasets from the Hub.\\n• Share it with the community, including you, dear reader!\\nTo upload the dataset, we first need to log in to our Hugging Face account by running\\nthe following command in the terminal and providing the relevant credentials:\\n$ huggingface-cli login\\nThis is equivalent to the notebook_login() helper function we used in previous\\nchapters. Once this is done, we can directly create a new dataset on the Hub and\\nupload the compressed JSON files. To simplify things, we will create two repositories:\\none for the train split and one for the validation split. We can do this by running the\\nrepo create command of the CLI as follows:\\n$ huggingface-cli repo create --type dataset --organization transformersbook \\\\\\ncodeparrot-train\\n$ huggingface-cli repo create --type dataset --organization transformersbook \\\\\\ncodeparrot-valid\\nHere we’ve specified that the repository should be a dataset (in contrast to the model\\nrepositories used to store weights), along with the organization we’ d like to store the\\nrepositories under. If you’re running this code under your personal account, you can\\nomit the --organization flag. Next, we need to clone these empty repositories to our\\nlocal machine, copy the JSON files to them, and push the changes to the Hub. We will\\ntake the last compressed JSON file out of the 184 we have as the validation file (i.e.,\\nroughly 0.5 percent of our dataset). Execute these commands to clone the repository\\nfrom the Hub to your local machine:\\n$ git clone https://huggingface.co/datasets/transformersbook/codeparrot-train\\n$ git clone https://huggingface.co/datasets/transformersbook/codeparrot-valid\\nNext, copy all but the last GitHub file as the training set:\\n$ cd codeparrot-train\\n$ cp ../codeparrot/*.json.gz .\\n$ rm ./file-000000000183.json.gz\\nThen commit the files and push them to the Hub:\\n$ git add .\\n$ git commit -m \"Adding dataset files\"\\n$ git push\\nNow, repeat the process for the validation set:\\nLarge Datasets and Where to Find Them | 309'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 333, 'page_label': '310'}, page_content='$ cd ../codeparrot-valid\\n$ cp ../codeparrot/file-000000000183.json.gz .\\n$ mv ./file-000000000183.json.gz ./file-000000000183_validation.json.gz\\n$ git add .\\n$ git commit -m \"Adding dataset files\"\\n$ git push\\nThe git add . step can take a couple of minutes since a hash of all the files is com‐\\nputed. Uploading all the files will also take a little while. Since this will enable us to\\nuse streaming later in the chapter, however, this is not lost time, and this step will\\nallow us to go significantly faster in the rest of our experiments. Note that we added a\\n_validation suffix to the validation filename. This will enable us to load it later as a\\nvalidation split.\\nAnd that’s it! Our two splits of the dataset as well as the full dataset are now live on\\nthe Hugging Face Hub at the following URLs:\\n• https://huggingface.co/datasets/transformersbook/codeparrot\\n• https://huggingface.co/datasets/transformersbook/codeparrot-train\\n• https://huggingface.co/datasets/transformersbook/codeparrot-valid\\nIt’s good practice to add README cards that explain how the data‐\\nsets were created and provide as much useful information about\\nthem as possible. A well-documented dataset is more likely to be\\nuseful to other people, as well as your future self. Y ou can read the\\n Datasets README guide  for a detailed description of how to\\nwrite good dataset documentation. Y ou can also use the web editor\\nto modify your README cards directly on the Hub later.\\nBuilding a Tokenizer\\nNow that we have gathered and loaded our large dataset, let’s see how we can effi‐\\nciently process the data to feed to our model. In the previous chapters we’ve used\\ntokenizers that accompanied the models we used. This made sense since these models\\nwere pretrained using data passed through a specific preprocessing pipeline defined\\nin the tokenizer. When using a pretrained model, it’s important to stick with the same\\npreprocessing design choices selected for pretraining. Otherwise the model may be\\nfed out-of-distribution patterns or unknown tokens.\\nHowever, when we train a new model, using a tokenizer prepared for another dataset\\ncan be suboptimal. Here are a few examples of the kinds of problems we might run\\ninto when using an existing tokenizer:\\n310 | Chapter 10: Training Transformers from Scratch'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 334, 'page_label': '311'}, page_content='• The T5 tokenizer was trained on the C4 corpus that we encountered earlier, but\\nan extensive step of stopword filtering was used to create it. As a result, the T5\\ntokenizer has never seen common English words such as “sex. ”\\n• The CamemBERT tokenizer was also trained on a very large corpus of text, but\\nonly comprising French text (the French subset of the OSCAR corpus). As such,\\nit is unaware of common English words such “being. ”\\nWe can easily test these features of each tokenizer in practice:\\nfrom transformers import AutoTokenizer\\ndef tok_list(tokenizer, string):\\n    input_ids = tokenizer(string, add_special_tokens=False)[\"input_ids\"]\\n    return [tokenizer.decode(tok) for tok in input_ids]\\ntokenizer_T5 = AutoTokenizer.from_pretrained(\"t5-base\")\\ntokenizer_camembert = AutoTokenizer.from_pretrained(\"camembert-base\")\\nprint(f\\'T5 tokens for \"sex\": {tok_list(tokenizer_T5,\"sex\")}\\')\\nprint(f\\'CamemBERT tokens for \"being\": {tok_list(tokenizer_camembert,\"being\")}\\')\\nT5 tokens for \"sex\": [\\'\\', \\'s\\', \\'ex\\']\\nCamemBERT tokens for \"being\": [\\'be\\', \\'ing\\']\\nIn many cases, splitting such short and common words into subparts will be ineffi‐\\ncient, since this will increase the input sequence length of the model (which has limi‐\\nted context). Therefore, it’s important to be aware of the domain and preprocessing of\\nthe dataset that was used to train the tokenizer. The tokenizer and model can encode\\nbias from the dataset that has an impact on the downstream behavior of the model.\\nTo create an optimal tokenizer for our dataset, we thus need to train one ourselves.\\nLet’s see how this can be done.\\nTraining a model involves starting from a given set of weights and\\nusing backpropagation from an error signal on a designed objective\\nto minimize the loss of the model and find an optimal set of\\nweights for the model to perform the task defined by the training\\nobjective. Training a tokenizer, on the other hand, does not involve\\nbackpropagation or weights. It is a way to create an optimal map‐\\nping from a string of text to a list of integers that can be ingested by\\nthe model. In today’s tokenizers, the optimal string-to-integer con‐\\nversion involves a vocabulary consisting of a list of atomic strings\\nand an associated method to convert, normalize, cut, or map a text\\nstring into a list of indices with this vocabulary. This list of indices\\nis then the input for our neural network.\\nBuilding a Tokenizer | 311'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 335, 'page_label': '312'}, page_content='The Tokenizer Model\\nAs you saw in Chapter 4 , the tokenizer is a processing pipeline consisting of four\\nsteps: normalization, pretokenization, the tokenizer model, and postprocessing. The\\npart of the tokenizer pipeline that can be trained on data is the tokenizer model. As\\nwe discussed in Chapter 2, there are several subword tokenization algorithms that can\\nbe used, such as BPE, WordPiece, and Unigram.\\nBPE starts from a list of basic units (single characters) and creates a vocabulary by a\\nprocess of progressively creating new tokens formed by merging the most frequently\\nco-occurring basic units and adding them to the vocabulary. This process is reiterated\\nuntil a predefined vocabulary size is reached.\\nUnigram starts from the other end, by initializing its base vocabulary with all the\\nwords in the corpus, and potential subwords. Then it progressively removes or splits\\nthe less useful tokens to obtain a smaller and smaller vocabulary, until the target\\nvocabulary size is reached. WordPiece is a predecessor of Unigram, and its official\\nimplementation was never open-sourced by Google.\\nThe impact of these various algorithms on downstream performance varies depend‐\\ning on the task, and overall it’s quite difficult to identify if one algorithm is clearly\\nsuperior to the others. Both BPE and Unigram have reasonable performance in most\\ncases, but let’s have a look at some aspects to consider when evaluating.\\nMeasuring Tokenizer Performance\\nThe optimality and performance of a tokenizer are challenging to measure in prac‐\\ntice. Some possible metrics include:\\n• Subword fertility, which calculates the average number of subwords produced per\\ntokenized word\\n• Proportion of continued words, which refers to the proportion of tokenized words\\nin a corpus that are split into at least two subtokens\\n• Coverage metrics like the proportion of unknown words or rarely used tokens in\\na tokenized corpus\\nIn addition, robustness to misspelling or noise is often estimated, as well as model\\nperformance on such out-of-domain examples, as this strongly depends on the toke‐\\nnization process.\\nThese measures give a set of different views on the tokenizer’s performance, but they\\ntend to ignore the interaction of the tokenizer with the model. For example, subword\\nfertility can be minimized by including all the possible words in the vocabulary, but\\nthis will produce a very large vocabulary for the model.\\n312 | Chapter 10: Training Transformers from Scratch'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 336, 'page_label': '313'}, page_content='In the end, the performance of the various tokenization approaches is thus generally\\nbest estimated by using the downstream performance of the model as the ultimate\\nmetric. For instance, the good performance of early BPE approaches was demon‐\\nstrated by showing improved performance on machine translation tasks by models\\ntrained using these tokenizers and vocabularies instead of character- or word-based\\ntokenization.\\nLet’s see how we can build our own tokenizer optimized for Python code.\\nA Tokenizer for Python\\nWe need a custom tokenizer for our use case: tokenizing Python code. The question\\nof pretokenization merits some discussion for programming languages. If we split on\\nwhitespaces and remove them, we will lose all the indentation information, which in\\nPython is important for the semantics of the program (just think about while loops,\\nor if-then-else statements). On the other hand, line breaks are not meaningful and\\ncan be added or removed without impact on the semantics. Similarly, splitting on\\npunctuation, like an underscore, which is used to compose a single variable name\\nfrom several subparts, might not make as much sense as it would in natural language.\\nUsing a natural language pretokenizer for tokenizing code thus seems potentially sub‐\\noptimal.\\nLet’s see if there are any tokenizers in the collection provided on the Hub that might\\nbe useful to us. We want a tokenizer that preserves spaces, so a good candidate could\\nbe a byte-level tokenizer like the one from GPT-2. Let’s load this tokenizer and\\nexplore its tokenization properties:\\nfrom transformers import AutoTokenizer\\npython_code = r\"\"\"def say_hello():\\n    print(\"Hello, World!\")\\n# Print it\\nsay_hello()\\n\"\"\"\\ntokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\\nprint(tokenizer(python_code).tokens())\\n[\\'def\\', \\'Ġsay\\', \\'_\\', \\'hello\\', \\'():\\', \\'Ċ\\', \\'Ġ\\', \\'Ġ\\', \\'Ġ\\', \\'Ġprint\\', \\'(\"\\',\\n\\'Hello\\', \\',\\', \\'ĠWorld\\', \\'!\"\\', \\')\\', \\'Ġ#\\', \\'ĠPrint\\', \\'Ġit\\', \\'Ċ\\', \\'Ċ\\', \\'say\\', \\'_\\',\\n\\'hello\\', \\'()\\', \\'Ċ\\']\\nBuilding a Tokenizer | 313'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 337, 'page_label': '314'}, page_content='Python has a built-in tokenize module that splits Python code\\nstrings into meaningful units (code operation, comments, indent\\nand dedent, etc.). One issue with using this approach is that this\\npretokenizer is Python-based and as such is typically rather slow\\nand limited by the Python global interpreter lock (GIL). On the\\nother hand, most of the tokenizers in the \\n  Transformers library\\nare provided by the \\n  Tokenizers library and are coded in Rust.\\nThe Rust tokenizers are many orders of magnitude faster to train\\nand to use, and we will thus likely want to use them given the size\\nof our corpus.\\nThis is quite a strange output, so let’s try to understand what is happening here by\\nrunning the various submodules of the tokenizer’s pipeline. First let’s see what nor‐\\nmalization is applied in this tokenizer:\\nprint(tokenizer.backend_tokenizer.normalizer)\\nNone\\nAs we can see, the GPT-2 tokenizer uses no normalization. It works directly on\\nthe raw Unicode inputs without any normalization steps. Let’s now take a look at the\\npretokenization:\\nprint(tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(python_code))\\n[(\\'def\\', (0, 3)), (\\'Ġsay\\', (3, 7)), (\\'_\\', (7, 8)), (\\'hello\\', (8, 13)), (\\'():\\',\\n(13, 16)), (\\'ĊĠĠĠ\\', (16, 20)), (\\'Ġprint\\', (20, 26)), (\\'(\"\\', (26, 28)), (\\'Hello\\',\\n(28, 33)), (\\',\\', (33, 34)), (\\'ĠWorld\\', (34, 40)), (\\'!\")\\', (40, 43)), (\\'Ġ#\\', (43,\\n45)), (\\'ĠPrint\\', (45, 51)), (\\'Ġit\\', (51, 54)), (\\'Ċ\\', (54, 55)), (\\'Ċ\\', (55, 56)),\\n(\\'say\\', (56, 59)), (\\'_\\', (59, 60)), (\\'hello\\', (60, 65)), (\\'()\\', (65, 67)), (\\'Ċ\\',\\n(67, 68))]\\nWhat are all these Ġ symbols, and what are the numbers accompanying the tokens?\\nLet’s explain both and see if we can understand better how this tokenizer works.\\nLet’s start with the numbers. \\n  Tokenizers has a very useful feature for switching\\nbetween strings and tokens, called offset tracking. All the operations on the input\\nstring are tracked so that it’s possible to know exactly what part of the input string a\\ntoken after tokenization corresponds to. These numbers simply indicate where in the\\noriginal string each token comes from; for instance, the word \\'hello\\' in the first line\\ncorresponds to the characters 8 to 13 in the original string. If some characters are\\nremoved in a normalization step, we are thus still able to associate each token with\\nthe respective part in the original string.\\nThe other curious feature of the tokenized text is the odd-looking characters, such as\\nĊ and Ġ. Byte-level means that this tokenizer works on bytes instead of Unicode char‐\\nacters. Each Unicode character is composed of between 1 and 4 bytes, depending on\\nthe character. The nice thing about bytes is that while there are 143,859 Unicode\\ncharacters in the Unicode alphabet, there are only 256 elements in the byte alphabet,\\n314 | Chapter 10: Training Transformers from Scratch'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 338, 'page_label': '315'}, page_content='6 L. Xue et al., “ByT5: Towards a Token-Free Future with Pre-Trained Byte-to-Byte Models”, (2021).\\nand you can express each Unicode character as a sequence of these bytes. If we work\\non bytes we can thus express all the strings composed from the UTF-8 world as\\nlonger strings in this alphabet of 256 values. That is, we can have a model using an\\nalphabet of only 256 words and be able to process any Unicode string. Let’s have a\\nlook at what the byte representations of some characters look like:\\na, e = u\"a\", u\"€\"\\nbyte = ord(a.encode(\"utf-8\"))\\nprint(f\\'`{a}` is encoded as `{a.encode(\"utf-8\")}` with a single byte: {byte}\\')\\nbyte = [ord(chr(i)) for i in e.encode(\"utf-8\")]\\nprint(f\\'`{e}` is encoded as `{e.encode(\"utf-8\")}` with three bytes: {byte}\\')\\n`a` is encoded as `b\\'a\\'` with a single byte: 97\\n`€` is encoded as `b\\'\\\\xe2\\\\x82\\\\xac\\'` with three bytes: [226, 130, 172]\\nAt this point you might wonder: why work on a byte level? Think back to our discus‐\\nsion in Chapter 2 about the trade-offs between character and word tokens. We could\\ndecide to build our vocabulary from the 143,859 Unicode characters, but we would\\nalso like to include words—i.e., combinations of Unicode characters—in our vocabu‐\\nlary, so this (already very large) size is only a lower bound for the total size of the\\nvocabulary. This will make our model’s embedding layer very large because it compri‐\\nses one vector for each vocabulary token.\\nOn the other extreme, if we only use the 256 byte values as our vocabulary, the input\\nsequences will be segmented in many small pieces (each byte constituting the Uni‐\\ncode characters), and as such our model will have to work on long inputs and spend\\nsignificant compute power on reconstructing Unicode characters from their separate\\nbytes, and then words from these characters. See the paper accompanying the ByT5\\nmodel release for a detailed study of this overhead.6\\nA middle-ground solution is to construct a medium-sized vocabulary by extending\\nthe 256-word vocabulary with the most common combinations of bytes. This is the\\napproach taken by the BPE algorithm. The idea is to progressively construct a\\nvocabulary of a predefined size by creating new vocabulary tokens through iteratively\\nmerging the most frequently co-occurring pair of tokens in the vocabulary. For\\ninstance, if t and h occur very frequently together, like in English, we’ll add a token th\\nto the vocabulary to model this pair of tokens instead of keeping them separated. The\\nt and h tokens are kept in the vocabulary to tokenize instances where they do not\\noccur together. Starting from a basic vocabulary of elementary units, we can then\\nmodel any string efficiently.\\nBuilding a Tokenizer | 315'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 339, 'page_label': '316'}, page_content=\"7 P . Gage, “ A New Algorithm for Data Compression, ” The C Users Journal 12, no. 2 (1994): 23–38, https://\\ndx.doi.org/10.14569/IJACSA.2012.030803.\\nBe careful not to confuse the “byte” in “Byte-Pair Encoding” with\\nthe “byte” in “byte-level. ” The name Byte-Pair Encoding comes\\nfrom a data compression technique proposed by Philip Gage in\\n1994, originally operating on bytes. 7 Unlike what this name might\\nindicate, standard BPE algorithms in NLP typically operate on Uni‐\\ncode strings rather than bytes (although there is a new type of BPE\\nthat specifically works on bytes, called byte-level BPE). If we read\\nour Unicode strings in bytes we can thus reuse a simple BPE sub‐\\nword splitting algorithm.\\nThere is just one issue when using a typical BPE algorithm in NLP . These algorithms\\nare designed to work with clean Unicode string as inputs, not bytes, and expect regu‐\\nlar ASCII characters in the inputs, without spaces or control characters. But in the\\nUnicode characters corresponding to the 256 first bytes, there are many control char‐\\nacters (newline, tab, escape, line feed, and other nonprintable characters). To over‐\\ncome this problem, the GPT-2 tokenizer first maps all the 256 input bytes to Unicode\\nstrings that can easily be digested by the standard BPE algorithms—that is, we will\\nmap our 256 elementary values to Unicode strings that all correspond to standard\\nprintable Unicode characters.\\nIt’s not very important that these Unicode characters are each encoded with 1 byte or\\nmore; what is important is that we have 256 single values at the end, forming our base\\nvocabulary, and that these 256 values are correctly handled by our BPE algorithm.\\nLet’s see some examples of this mapping with the GPT-2 tokenizer. We can access the\\nentire mapping as follows:\\nfrom transformers.models.gpt2.tokenization_gpt2 import bytes_to_unicode\\nbyte_to_unicode_map = bytes_to_unicode()\\nunicode_to_byte_map = dict((v, k) for k, v in byte_to_unicode_map.items())\\nbase_vocab = list(unicode_to_byte_map.keys())\\nprint(f'Size of our base vocabulary: {len(base_vocab)}')\\nprint(f'First element: `{base_vocab[0]}`, last element: `{base_vocab[-1]}`')\\nSize of our base vocabulary: 256\\nFirst element: `!`, last element: `Ń`\\nAnd we can take a look at some common values of bytes and associated mapped Uni‐\\ncode characters in Table 10-1.\\n316 | Chapter 10: Training Transformers from Scratch\"), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 340, 'page_label': '317'}, page_content='Table 10-1. Examples of character mappings in BPE\\nDescription Character Bytes Mapped bytes\\nRegular characters `a` and `?` 97 and 63 `a` and `?`\\nA nonprintable control character (carriage return) `U+000D` 13 `č`\\nA space ` ` 32 `Ġ`\\nA nonbreakable space `\\\\xa0` 160 `ł`\\nA newline character `\\\\n` 10 `Ċ`\\nWe could have used a more explicit conversion, like mapping newlines to a NEWLINE\\nstring, but BPE algorithms are typically designed to work on characters. For this rea‐\\nson, keeping one Unicode character for each byte character is easier to handle with an\\nout-of-the-box BPE algorithm. Now that we have been introduced to the dark magic\\nof Unicode encodings, we can understand our tokenization conversion a bit better:\\nprint(tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(python_code))\\n[(\\'def\\', (0, 3)), (\\'Ġsay\\', (3, 7)), (\\'_\\', (7, 8)), (\\'hello\\', (8, 13)), (\\'():\\',\\n(13, 16)), (\\'ĊĠĠĠ\\', (16, 20)), (\\'Ġprint\\', (20, 26)), (\\'(\"\\', (26, 28)), (\\'Hello\\',\\n(28, 33)), (\\',\\', (33, 34)), (\\'ĠWorld\\', (34, 40)), (\\'!\")\\', (40, 43)), (\\'Ġ#\\', (43,\\n45)), (\\'ĠPrint\\', (45, 51)), (\\'Ġit\\', (51, 54)), (\\'Ċ\\', (54, 55)), (\\'Ċ\\', (55, 56)),\\n(\\'say\\', (56, 59)), (\\'_\\', (59, 60)), (\\'hello\\', (60, 65)), (\\'()\\', (65, 67)), (\\'Ċ\\',\\n(67, 68))]\\nWe can recognize the newlines, which as we now know are mapped to Ċ, and the\\nspaces, mapped to Ġ. We also see that:\\n• Spaces, and in particular consecutive spaces, are conserved (for instance, the\\nthree spaces in \\'ĊĠĠĠ\\').\\n• Consecutive spaces are considered as a single word.\\n• Each space preceding a word is attached to and considered a part of the subse‐\\nquent word (e.g., in \\'Ġsay\\').\\nLet’s now experiment with the BPE model. As we’ve mentioned, it’s in charge of split‐\\nting the words into subunits until all subunits belong to the predefined vocabulary.\\nThe vocabulary of our GPT-2 tokenizer comprises 50,257 words:\\n• The base vocabulary with the 256 values of the bytes\\n• 50,000 additional tokens created by repeatedly merging the most commonly co-\\noccurring tokens\\n• A special character added to the vocabulary to represent document boundaries\\nWe can easily check that by looking at the length attribute of the tokenizer:\\nprint(f\"Size of the vocabulary: {len(tokenizer)}\")\\nBuilding a Tokenizer | 317'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 341, 'page_label': '318'}, page_content='Size of the vocabulary: 50257\\nRunning the full pipeline on our input code gives us the following output:\\nprint(tokenizer(python_code).tokens())\\n[\\'def\\', \\'Ġsay\\', \\'_\\', \\'hello\\', \\'():\\', \\'Ċ\\', \\'Ġ\\', \\'Ġ\\', \\'Ġ\\', \\'Ġprint\\', \\'(\"\\',\\n\\'Hello\\', \\',\\', \\'ĠWorld\\', \\'!\"\\', \\')\\', \\'Ġ#\\', \\'ĠPrint\\', \\'Ġit\\', \\'Ċ\\', \\'Ċ\\', \\'say\\', \\'_\\',\\n\\'hello\\', \\'()\\', \\'Ċ\\']\\nAs we can see, the BPE tokenizer keeps most of the words but will split the multiple\\nspaces of our indentation into several consecutive spaces. This happens because this\\ntokenizer is not specifically trained on code, but mostly on texts where consecutive\\nspaces are rare. The BPE model thus doesn’t include a specific token in the vocabu‐\\nlary for indentation. This is a case where the tokenizer model is poorly suited for the\\ndataset’s domain. As we discussed earlier, the solution is to retrain the tokenizer on\\nthe target corpus. So let’s get to it!\\nTraining a Tokenizer\\nLet’s retrain our byte-level BPE tokenizer on a slice of our corpus to get a vocabulary\\nbetter adapted to Python code. Retraining a tokenizer provided by \\n  Transformers is\\nsimple. We just need to:\\n• Specify our target vocabulary size.\\n• Prepare an iterator to supply lists of input strings to process to train the tokeniz‐\\ner’s model.\\n• Call the train_new_from_iterator() method.\\nUnlike deep learning models, which are often expected to memorize a lot of specific\\ndetails from the training corpus, tokenizers are really just trained to extract the main\\nstatistics. In a nutshell, the tokenizer is just trained to know which letter combina‐\\ntions are the most frequent in our corpus.\\nTherefore, you don’t necessarily need to train your tokenizer on a very large corpus;\\nthe corpus just needs to be representative of your domain and big enough for the\\ntokenizer to extract statistically significant measures. But depending on the vocabu‐\\nlary size and the exact texts in the corpus, the tokenizer can end up storing\\nunexpected words. We can see this, for instance, when looking at the longest words in\\nthe vocabulary of the GPT-2 tokenizer:\\ntokens = sorted(tokenizer.vocab.items(), key=lambda x: len(x[0]), reverse=True)\\nprint([f\\'{tokenizer.convert_tokens_to_string(t)}\\' for t, _ in tokens[:8]]);\\n[\\'ÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂ\\', \\'\\n=================================================================\\', \\'\\n----------------------------------------------------------------\\n\\',\\n318 | Chapter 10: Training Transformers from Scratch'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 342, 'page_label': '319'}, page_content='\\'................................................................\\',\\n\\'ÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂ\\',\\n\\'\\n----------------------------------------------------------------\\n\\',\\n\\'================================================================\\',\\n\\'________________________________________________________________\\']\\nThese tokens look like separator lines that are likely to be used on forums. This\\nmakes sense since GPT-2 was trained on a corpus centered around Reddit. Now let’s\\nhave a look at the last words that were added to the vocabulary, and thus the least\\nfrequent ones:\\ntokens = sorted(tokenizer.vocab.items(), key=lambda x: x[1], reverse=True)\\nprint([f\\'{tokenizer.convert_tokens_to_string(t)}\\' for t, _ in tokens[:12]]);\\n[\\'<|endoftext|>\\', \\' gazed\\', \\' informants\\', \\' Collider\\', \\' regress\\', \\'ominated\\',\\n\\' amplification\\', \\'Compar\\', \\'...\"\\', \\' (/\\', \\'Commission\\', \\' Hitman\\']\\nThe first token, <|endoftext|>, is the special token used to specify the end of a text\\nsequence and was added after the BPE vocabulary was built. For each of these tokens\\nour model will have to learn an associated word embedding, and we probably don’t\\nwant the embedding matrix to contain too many noisy words. Also note how some\\nvery time- and space-specific knowledge of the world (e.g., proper nouns like Hitman\\nand Commission) is embedded at a very low level in our modeling approach by these\\nwords being granted separate tokens with associated vectors in the vocabulary. The\\ncreation of such specific tokens by a BPE tokenizer can also be an indication that the\\ntarget vocabulary size is too large or that the corpus contains idiosyncratic tokens.\\nLet’s train a fresh tokenizer on our corpus and examine its learned vocabulary. Since\\nwe just need a corpus reasonably representative of our dataset statistics, let’s select\\nabout 1–2 GB of data, or about 100,000 documents from our corpus:\\nfrom tqdm.auto import tqdm\\nlength = 100000\\ndataset_name = \\'transformersbook/codeparrot-train\\'\\ndataset = load_dataset(dataset_name, split=\"train\", streaming=True)\\niter_dataset = iter(dataset)\\ndef batch_iterator(batch_size=10):\\n    for _ in tqdm(range(0, length, batch_size)):\\n        yield [next(iter_dataset)[\\'content\\'] for _ in range(batch_size)]\\nnew_tokenizer = tokenizer.train_new_from_iterator(batch_iterator(),\\n                                                  vocab_size=12500,\\n                                                  initial_alphabet=base_vocab)\\nLet’s investigate the first and last words created by our BPE algorithm to see how rele‐\\nvant our vocabulary is. We skip the 256 byte tokens and look at the first tokens added\\nthereafter:\\nBuilding a Tokenizer | 319'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 343, 'page_label': '320'}, page_content='tokens = sorted(new_tokenizer.vocab.items(), key=lambda x: x[1], reverse=False)\\nprint([f\\'{tokenizer.convert_tokens_to_string(t)}\\' for t, _ in tokens[257:280]]);\\n[\\'  \\', \\'    \\', \\'   \\', \\'        \\', \\'se\\', \\'in\\', \\'       \\', \\'re\\', \\'on\\', \\'te\\', \\'\\\\n\\n\\', \\'\\\\n        \\', \\'or\\', \\'st\\', \\'de\\', \\'\\\\n   \\', \\'th\\', \\'le\\', \\' =\\', \\'lf\\', \\'self\\',\\n\\'me\\', \\'al\\']\\nHere we can see various standard levels of indentation and whitespace tokens, as well\\nas short common Python keywords like self, or, and in. This is a good sign that our\\nBPE algorithm is working as intended. Now let’s check out the last words:\\nprint([f\\'{new_tokenizer.convert_tokens_to_string(t)}\\' for t,_ in tokens[-12:]]);\\n[\\' capt\\', \\' embedded\\', \\' regarding\\', \\'Bundle\\', \\'355\\', \\' recv\\', \\' dmp\\', \\' vault\\',\\n\\' Mongo\\', \\' possibly\\', \\'implementation\\', \\'Matches\\']\\nHere there are still some relatively common words, like recv, as well as some more\\nnoisy words probably coming from the comments.\\nWe can also tokenize our simple example of Python code to see how our tokenizer is\\nbehaving on a simple example:\\nprint(new_tokenizer(python_code).tokens())\\n[\\'def\\', \\'Ġs\\', \\'ay\\', \\'_\\', \\'hello\\', \\'():\\', \\'ĊĠĠĠ\\', \\'Ġprint\\', \\'(\"\\', \\'Hello\\', \\',\\',\\n\\'ĠWor\\', \\'ld\\', \\'!\")\\', \\'Ġ#\\', \\'ĠPrint\\', \\'Ġit\\', \\'Ċ\\', \\'Ċ\\', \\'s\\', \\'ay\\', \\'_\\', \\'hello\\',\\n\\'()\\', \\'Ċ\\']\\nEven though they are not code keywords, it’s a little annoying to see common English\\nwords like World or say being split by our tokenizer, since we’ d expect them to occur\\nrather frequently in the corpus. Let’s check if all the Python reserved keywords are in\\nthe vocabulary:\\nimport keyword\\nprint(f\\'There are in total {len(keyword.kwlist)} Python keywords.\\')\\nfor keyw in keyword.kwlist:\\n    if keyw not in new_tokenizer.vocab:\\n        print(f\\'No, keyword `{keyw}` is not in the vocabulary\\')\\nThere are in total 35 Python keywords.\\nNo, keyword `await` is not in the vocabulary\\nNo, keyword `finally` is not in the vocabulary\\nNo, keyword `nonlocal` is not in the vocabulary\\nIt appears that several quite frequent keywords, like finally, are not in the vocabu‐\\nlary either. Let’s try building a larger vocabulary using a larger sample of our dataset.\\nFor instance, we can build a vocabulary of 32,768 words (multiples of 8 are better for\\nsome efficient GPU/TPU computations) and train the tokenizer on a twice as large\\nslice of our corpus:\\nlength = 200000\\nnew_tokenizer_larger = tokenizer.train_new_from_iterator(batch_iterator(),\\n    vocab_size=32768, initial_alphabet=base_vocab)\\n320 | Chapter 10: Training Transformers from Scratch'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 344, 'page_label': '321'}, page_content='We don’t expect the most frequent tokens to change much when adding more docu‐\\nments, but let’s look at the last tokens:\\ntokens = sorted(new_tokenizer_larger.vocab.items(), key=lambda x: x[1],\\n                reverse=False)\\nprint([f\\'{tokenizer.convert_tokens_to_string(t)}\\' for t, _ in tokens[-12:]]);\\n[\\'lineEdit\\', \\'spik\\', \\' BC\\', \\'pective\\', \\'OTA\\', \\'theus\\', \\'FLUSH\\', \\' excutils\\',\\n\\'00000002\\', \\' DIVISION\\', \\'CursorPosition\\', \\' InfoBar\\']\\nA brief inspection doesn’t show any regular programming keywords here, which is\\npromising. Let’s try tokenizing our sample code example with the new larger\\ntokenizer:\\nprint(new_tokenizer_larger(python_code).tokens())\\n[\\'def\\', \\'Ġsay\\', \\'_\\', \\'hello\\', \\'():\\', \\'ĊĠĠĠ\\', \\'Ġprint\\', \\'(\"\\', \\'Hello\\', \\',\\',\\n\\'ĠWorld\\', \\'!\")\\', \\'Ġ#\\', \\'ĠPrint\\', \\'Ġit\\', \\'Ċ\\', \\'Ċ\\', \\'say\\', \\'_\\', \\'hello\\', \\'()\\',\\n\\'Ċ\\']\\nHere also the indents are conveniently kept in the vocabulary, and we see that com‐\\nmon English words like Hello, World, and say are also included as single tokens. This\\nseems more in line with our expectations of the data the model may see in the down‐\\nstream task. Let’s investigate the common Python keywords, as we did before:\\nfor keyw in keyword.kwlist:\\n    if keyw not in new_tokenizer_larger.vocab:\\n        print(f\\'No, keyword `{keyw}` is not in the vocabulary\\')\\nNo, keyword `nonlocal` is not in the vocabulary\\nWe are still missing the nonlocal keyword, but it’s also rarely used in practice as it\\nmakes the syntax more complex. Keeping it out of the vocabulary seems reasonable.\\nAfter this manual inspection, our larger tokenizer seems well adapted for our task—\\nbut as we mentioned earlier, objectively evaluating the performance of a tokenizer is a\\nchallenging task without measuring the model’s performance. We will proceed with\\nthis one and train a model to see how well it works in practice.\\nY ou can easily verify that the new tokenizer is about twice as effi‐\\ncient than the standard GPT-2 tokenizer by comparing the\\nsequence lengths of tokenized code examples. Our tokenizer uses\\napproximately half as many tokens as the existing one to encode a\\ntext, which gives us twice the effective model context for free.\\nWhen we train a new model with the new tokenizer on a context\\nwindow of size 1,024 it is equivalent to training the same model\\nwith the old tokenizer on a context window of size 2,048, with the\\nadvantage of being much faster and more memory efficient.\\nBuilding a Tokenizer | 321'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 345, 'page_label': '322'}, page_content='Saving a Custom Tokenizer on the Hub\\nNow that our tokenizer is trained, we should save it. The simplest way to save it and\\nbe able to access it from anywhere later is to push it to the Hugging Face Hub. This\\nwill be especially useful later, when we use a separate training server.\\nTo create a private model repository and save our tokenizer in it as a first file, we can\\ndirectly use the push_to_hub() method of the tokenizer. Since we already authentica‐\\nted our account with huggingface-cli login, we can simply push the tokenizer as\\nfollows:\\nmodel_ckpt = \"codeparrot\"\\norg = \"transformersbook\"\\nnew_tokenizer_larger.push_to_hub(model_ckpt, organization=org)\\nIf you don’t want to push to an organization, you can simply omit the organization\\nargument. This will create a repository in your namespace named codeparrot, which\\nanyone can then load by running:\\nreloaded_tokenizer = AutoTokenizer.from_pretrained(org + \"/\" + model_ckpt)\\nprint(reloaded_tokenizer(python_code).tokens())\\n[\\'def\\', \\'Ġsay\\', \\'_\\', \\'hello\\', \\'():\\', \\'ĊĠĠĠ\\', \\'Ġprint\\', \\'(\"\\', \\'Hello\\', \\',\\',\\n\\'ĠWorld\\', \\'!\")\\', \\'Ġ#\\', \\'ĠPrint\\', \\'Ġit\\', \\'Ċ\\', \\'Ċ\\', \\'say\\', \\'_\\', \\'hello\\', \\'()\\',\\n\\'Ċ\\']\\nThe tokenizer loaded from the Hub behaves exactly as we just saw. We can also inves‐\\ntigate its files and saved vocabulary on the Hub. For reproducibility, let’s save our\\nsmaller tokenizer as well:\\nnew_tokenizer.push_to_hub(model_ckpt+ \"-small-vocabulary\", organization=org)\\nThis was a deep dive into building a tokenizer for a specific use case. Next, we will\\nfinally create a new model and train it from scratch.\\n322 | Chapter 10: Training Transformers from Scratch'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 346, 'page_label': '323'}, page_content='Training a Model from Scratch\\nHere’s the part you’ve probably been waiting for: the model training. In this section\\nwe’ll decide which architecture works best for the task, initialize a fresh model\\nwithout pretrained weights, set up a custom data loading class, and create a scalable\\ntraining loop. In the grand finale we will train small and large GPT-2 models with 111\\nmillion and 1.5 billion parameters, respectively! But let’s not get ahead ourselves.\\nFirst, we need to decide which architecture is best suited for code autocompletion.\\nIn this section we will implement a longer than usual script to train\\na model on a distributed infrastructure. Therefore, you should not\\nrun each code snippet independently, but instead download the\\nscript provided in the \\n  Transformers repository . Follow the\\naccompanying instructions to execute the script with \\n  Accelerate\\non your hardware.\\nA Tale of Pretraining Objectives\\nNow that we have access to a large-scale pretraining corpus and an efficient tokenizer,\\nwe can start thinking about how to pretrain a transformer model. With such a large\\ncodebase consisting of code snippets like the one shown in Figure 10-1, we can tackle\\nseveral tasks. Which one we choose will influence our choice of pretraining objec‐\\ntives. Let’s have a look at three common tasks.\\nFigure 10-1. An example of a Python function that could be found in our dataset\\nCausal language modeling\\nA natural task with textual data is to provide a model with the beginning of a code\\nsample and ask it to generate possible completions. This is a self-supervised training\\nobjective in which we can use the dataset without annotations. This should ring a\\nbell: it’s the causal language modeling task we encountered in Chapter 5. A directly\\nrelated downstream task is code autocompletion, so we’ll definitely put this model on\\nthe shortlist. A decoder-only architecture such as the GPT family of models is usually\\nbest suited for this task, as shown in Figure 10-2.\\nTraining a Model from Scratch | 323'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 347, 'page_label': '324'}, page_content='Figure 10-2. In causal language modeling, the future tokens are masked and the model\\nhas to predict them; typically a decoder model such as GPT is used for such a task\\nMasked language modeling\\nA related but slightly different task is to provide a model with a noisy code sample,\\nfor instance with a code instruction replaced by a random or masked word, and ask it\\nto reconstruct the original clean sample, as illustrated in Figure 10-3. This is also a\\nself-supervised training objective and is commonly called masked language modeling\\nor the denoising objective. It’s harder to think about a downstream task directly related\\nto denoising, but denoising is generally a good pretraining task to learn general rep‐\\nresentations for later downstream tasks. Many of the models that we have used in the\\nprevious chapters (like BERT and XLM-RoBERTa) are pretrained in that way. Train‐\\ning a masked language model on a large corpus can thus be combined with fine-\\ntuning the model on a downstream task with a limited number of labeled examples.\\nFigure 10-3. In masked language modeling some of the input tokens are either masked or\\nreplaced, and the model’s task is to predict the original tokens; this is the architecture\\nunderlying the encoder branch of transformer models\\nSequence-to-sequence training\\nAn alternative task is to use a heuristic like regular expressions to separate comments\\nor docstrings from code and build a large-scale dataset of (code, comments) pairs that\\ncan be used as an annotated dataset. The training task is then a supervised training\\nobjective in which one category (code or comment) is used as input for the model\\nand the other category (comment or code) is used as labels. This is a case of super‐\\nvised learning with (input, labels) pairs, as highlighted in Figure 10-4. With a large,\\nclean, and diverse dataset as well as a model with sufficient capacity, we can try to\\ntrain a model that learns to transcript comments in code or vice versa. A downstream\\ntask directly related to this supervised training task is then documentation generation\\nfrom code or code generation from documentation, depending on how we set our\\ninput/outputs. In this setting a sequence is translated into another sequence, which is\\nwhere encoder-decoder architectures such as T5, BART, and PEGASUS shine.\\n324 | Chapter 10: Training Transformers from Scratch'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 348, 'page_label': '325'}, page_content='Figure 10-4. Using an encoder-decoder architecture for a sequence-to-sequence task\\nwhere the inputs are split into comment/code pairs using heuristics: the model gets one\\nelement as input and needs to generate the other one\\nSince we want to build a code autocompletion model, we’ll select the first objective\\nand choose a GPT architecture for the task. So let’s initialize a fresh GPT-2 model!\\nInitializing the Model\\nThis is the first time in this book that we won’t use the from_pretrained() method to\\nload a model but initialize the new model. We will, however, load the configuration of\\ngpt2-xl so that we use the same hyperparameters and only adapt the vocabulary size\\nfor the new tokenizer. We then initialize a new model with this configuration with the\\nfrom_config() method:\\nfrom transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer\\ntokenizer = AutoTokenizer.from_pretrained(model_ckpt)\\nconfig = AutoConfig.from_pretrained(\"gpt2-xl\", vocab_size=len(tokenizer))\\nmodel = AutoModelForCausalLM.from_config(config)\\nLet’s check how large the model actually is:\\nprint(f\\'GPT-2 (xl) size: {model_size(model)/1000**2:.1f}M parameters\\')\\nGPT-2 (xl) size: 1529.6M parameters\\nThis is a 1.5B parameter model! This is a lot of capacity, but we also have a large data‐\\nset. In general, large language models are more efficient to train as long as the dataset\\nis reasonably large. Let’s save the newly initialized model in a models/ folder and push\\nit to the Hub:\\nmodel.save_pretrained(\"models/\" + model_ckpt, push_to_hub=True,\\n                      organization=org)\\nTraining a Model from Scratch | 325'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 349, 'page_label': '326'}, page_content='Pushing the model to the Hub may take a few minutes given the size of the check‐\\npoint (> 5 GB). Since this model is quite large, we’ll also create a smaller version that\\nwe can train to make sure everything works before scaling up. We will take the stan‐\\ndard GPT-2 size as a base:\\ntokenizer = AutoTokenizer.from_pretrained(model_ckpt)\\nconfig_small = AutoConfig.from_pretrained(\"gpt2\", vocab_size=len(tokenizer))\\nmodel_small = AutoModelForCausalLM.from_config(config_small)\\nprint(f\\'GPT-2 size: {model_size(model_small)/1000**2:.1f}M parameters\\')\\nGPT-2 size: 111.0M parameters\\nAnd let’s save it to the Hub as well for easy sharing and reuse:\\nmodel_small.save_pretrained(\"models/\" + model_ckpt + \"-small\", push_to_hub=True,\\n                            organization=org)\\nNow that we have two models we can train, we need to make sure we can feed them\\nthe input data efficiently during training.\\nImplementing the Dataloader\\nTo be able to train with maximal efficiency, we will want to supply our model with\\nsequences filling its context. For example, if the context length of our model is 1,024\\ntokens, we always want to provide 1,024-token sequences during training. But some\\nof our code examples might be shorter or longer than 1,024 tokens. To feed batches\\nwith full sequences of sequence_length to our model, we should thus either drop the\\nlast incomplete sequence or pad it. However, this will render our training slightly less\\nefficient and force us to take care of padding and masking padded token labels. We\\nare much more compute- than data-constrained, so we’ll take the easy and efficient\\nway here. We can use a little trick to make sure we don’t lose too many trailing seg‐\\nments: we can tokenize several examples and then concatenate them, separated by the\\nspecial end-of-sequence token, to get a very long sequence. Finally, we split this\\nsequence into equally sized chunks as shown in Figure 10-5. With this approach, we\\nlose at most a small fraction of the data at the end.\\n326 | Chapter 10: Training Transformers from Scratch'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 350, 'page_label': '327'}, page_content='Figure 10-5. Preparing sequences of varying length for causal language modeling by con‐\\ncatenating several tokenized examples with an EOS token before chunking them\\nWe can, for instance, make sure we have roughly one hundred full sequences in our\\ntokenized examples by defining our input string character length as:\\ninput_characters = number_of_sequences * sequence_length * characters_per_token\\nwhere:\\n• input_characters is the number of characters in the string input to our\\ntokenizer.\\n• number_of_sequences is the number of (truncated) sequences we would like\\nfrom our tokenizer, (e.g., 100).\\n• sequence_length is the number of tokens per sequence returned by the token‐\\nizer, (e.g., 1,024).\\n• characters_per_token is the average number of characters per output token\\nthat we first need to estimate.\\nIf we input a string with input_characters characters we will thus get on average\\nnumber_of_sequences output sequences, and we can easily calculate how much input\\ndata we are losing by dropping the last sequence. If number_of_sequences=100 it\\nmeans that we stack roughly 100 sequences and at most lose the last element, which\\nmight be too short or too long. This corresponds to at most losing 1% of our dataset.\\nAt the same time, this approach ensures that we don’t introduce a bias by cutting off\\nthe majority of file endings.\\nTraining a Model from Scratch | 327'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 351, 'page_label': '328'}, page_content='Let’s first estimate the average character length per token in our dataset:\\nexamples, total_characters, total_tokens = 500, 0, 0\\ndataset = load_dataset(\\'transformersbook/codeparrot-train\\', split=\\'train\\',\\n                       streaming=True)\\nfor _, example in tqdm(zip(range(examples), iter(dataset)), total=examples):\\n    total_characters += len(example[\\'content\\'])\\n    total_tokens += len(tokenizer(example[\\'content\\']).tokens())\\ncharacters_per_token = total_characters / total_tokens\\nprint(characters_per_token)\\n3.6233025034779565\\nWith that we have all that’s needed to create our own IterableDataset (which is a\\nhelper class provided by PyTorch) for preparing constant-length inputs for the\\nmodel. We just need to inherit from IterableDataset and set up the __iter__()\\nfunction that yields the next element with the logic we just walked through:\\nimport torch\\nfrom torch.utils.data import IterableDataset\\nclass ConstantLengthDataset(IterableDataset):\\n    def __init__(self, tokenizer, dataset, seq_length=1024,\\n                 num_of_sequences=1024, chars_per_token=3.6):\\n        self.tokenizer = tokenizer\\n        self.concat_token_id = tokenizer.eos_token_id\\n        self.dataset = dataset\\n        self.seq_length = seq_length\\n        self.input_characters = seq_length * chars_per_token * num_of_sequences\\n    def __iter__(self):\\n        iterator = iter(self.dataset)\\n        more_examples = True\\n        while more_examples:\\n            buffer, buffer_len = [], 0\\n            while True:\\n                if buffer_len >= self.input_characters:\\n                    m=f\"Buffer full: {buffer_len}>={self.input_characters:.0f}\"\\n                    print(m)\\n                    break\\n                try:\\n                    m=f\"Fill buffer: {buffer_len}<{self.input_characters:.0f}\"\\n                    print(m)\\n                    buffer.append(next(iterator)[\"content\"])\\n                    buffer_len += len(buffer[-1])\\n                except StopIteration:\\n                    iterator = iter(self.dataset)\\n            all_token_ids = []\\n328 | Chapter 10: Training Transformers from Scratch'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 352, 'page_label': '329'}, page_content='tokenized_inputs = self.tokenizer(buffer, truncation=False)\\n            for tokenized_input in tokenized_inputs[\"input_ids\\'\"]:\\n            for tokenized_input in tokenized_inputs:\\n                all_token_ids.extend(tokenized_input + [self.concat_token_id])\\n            for i in range(0, len(all_token_ids), self.seq_length):\\n                input_ids = all_token_ids[i : i + self.seq_length]\\n                if len(input_ids) == self.seq_length:\\n                    yield torch.tensor(input_ids)\\nThe __iter__() function builds up a buffer of strings until it contains enough char‐\\nacters. All the elements in the buffer are tokenized and concatenated with the EOS\\ntoken, then the long sequence in all_token_ids is chunked in seq_length-sized sli‐\\nces. Normally, we need attention masks to stack padded sequences of varying length\\nand make sure the padding is ignored during training. We have taken care of this by\\nonly providing sequences of the same (maximal) length, so we don’t need the masks\\nhere and only return the input_ids. Let’s test our iterable dataset:\\nshuffled_dataset = dataset.shuffle(buffer_size=100)\\nconstant_length_dataset = ConstantLengthDataset(tokenizer, shuffled_dataset,\\n                                                num_of_sequences=10)\\ndataset_iterator = iter(constant_length_dataset)\\nlengths = [len(b) for _, b in zip(range(5), dataset_iterator)]\\nprint(f\"Lengths of the sequences: {lengths}\")\\nFill buffer: 0<36864\\nFill buffer: 3311<36864\\nFill buffer: 9590<36864\\nFill buffer: 22177<36864\\nFill buffer: 25530<36864\\nFill buffer: 31098<36864\\nFill buffer: 32232<36864\\nFill buffer: 33867<36864\\nBuffer full: 41172>=36864\\nLengths of the sequences: [1024, 1024, 1024, 1024, 1024]\\nNice, this works as intended and we get constant-length inputs for the model. Now\\nthat we have a reliable data source for the model, it’s time to build the actual training\\nloop.\\nNotice that we shuffled the raw dataset before creating a Constant\\nLengthDataset. Since this is an iterable dataset, we can’t just shuffle\\nthe whole dataset at the beginning. Instead, we set up a buffer with\\nsize buffer_size and shuffle the elements in this buffer before we\\nget elements from the dataset.\\nTraining a Model from Scratch | 329'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 353, 'page_label': '330'}, page_content=\"Defining the Training Loop\\nWe now have all the elements to write our training loop. One obvious limitation of\\ntraining our own language model is the memory limits on the GPUs we will use. Even\\non a modern graphics card you can’t train a model at GPT-2 scale in reasonable time.\\nIn this tutorial we will implement data parallelism, which will help us utilize several\\nGPUs for training. Fortunately, we can use \\n  Accelerate to make our code scalable.\\nThe \\n  Accelerate library is designed to make distributed training—and changing the\\nunderlying hardware for training—easy. We can also use the Trainer for distributed\\ntraining but \\n  Accelerate gives us full control over the training loop, which is what\\nwe want to explore here.\\n Accelerate provides an easy API to make training scripts run with mixed precision\\nand in any kind of distributed setting (single GPU, multiple GPUs, and TPUs). The\\nsame code can then run seamlessly on your local machine for debugging purposes or\\nyour beefy training cluster for the final training run. Y ou only need to make a handful\\nof changes to your native PyTorch training loop:\\n  import torch\\n  import torch.nn.functional as F\\n  from datasets import load_dataset\\n+ from accelerate import Accelerator\\n- device = 'cpu'\\n+ accelerator = Accelerator()\\n- model = torch.nn.Transformer().to(device)\\n+ model = torch.nn.Transformer()\\n  optimizer = torch.optim.Adam(model.parameters())\\n  dataset = load_dataset('my_dataset')\\n  data = torch.utils.data.DataLoader(dataset, shuffle=True)\\n+ model, optimizer, data = accelerator.prepare(model, optimizer, data)\\n  model.train()\\n  for epoch in range(10):\\n      for source, targets in data:\\n-         source = source.to(device)\\n-         targets = targets.to(device)\\n          optimizer.zero_grad()\\n          output = model(source)\\n          loss = F.cross_entropy(output, targets)\\n-         loss.backward()\\n+         accelerator.backward(loss)\\n          optimizer.step()\\nThe core part of the changes is the call to prepare(), which makes sure the model,\\noptimizers, and dataloaders are all prepared and distributed on the infrastructure.\\nThese minor changes to the PyTorch training loop enable you to easily scale training\\nacross different infrastructures. With that in mind, let’s start building up our training\\n330 | Chapter 10: Training Transformers from Scratch\"), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 354, 'page_label': '331'}, page_content='script and define a few helper functions. First we set up the hyperparameters for\\ntraining and wrap them in a Namespace for easy access:\\nfrom argparse import Namespace\\n# Commented parameters correspond to the small model\\nconfig = {\"train_batch_size\": 2, # 12\\n          \"valid_batch_size\": 2, # 12\\n          \"weight_decay\": 0.1,\\n          \"shuffle_buffer\": 1000,\\n          \"learning_rate\": 2e-4, # 5e-4\\n          \"lr_scheduler_type\": \"cosine\",\\n          \"num_warmup_steps\": 750, # 2000\\n          \"gradient_accumulation_steps\": 16, # 1\\n          \"max_train_steps\": 50000, # 150000\\n          \"max_eval_steps\": -1,\\n          \"seq_length\": 1024,\\n          \"seed\": 1,\\n          \"save_checkpoint_steps\": 50000} # 15000\\nargs = Namespace(**config)\\nNext, we set up logging for training. Since we are training a model from scratch, the\\ntraining run will take a while and require expensive infrastructure. Therefore, we\\nwant to make sure that all the relevant information is stored and easily accessible. The\\nsetup_logging() method sets up three levels of logging: using a standard Python\\nLogger, TensorBoard, and Weights & Biases. Depending on your preferences and use\\ncase, you can add or remove logging frameworks here:\\nfrom torch.utils.tensorboard import SummaryWriter\\nimport logging\\nimport wandb\\ndef setup_logging(project_name):\\n    logger = logging.getLogger(__name__)\\n    logging.basicConfig(\\n        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\\n        datefmt=\"%m/%d/%Y %H:%M:%S\", level=logging.INFO, handlers=[\\n        logging.FileHandler(f\"log/debug_{accelerator.process_index}.log\"),\\n        logging.StreamHandler()])\\n    if accelerator.is_main_process: # We only want to set up logging once\\n        wandb.init(project=project_name, config=args)\\n        run_name = wandb.run.name\\n        tb_writer = SummaryWriter()\\n        tb_writer.add_hparams(vars(args), {\\'0\\': 0})\\n        logger.setLevel(logging.INFO)\\n        datasets.utils.logging.set_verbosity_debug()\\n        transformers.utils.logging.set_verbosity_info()\\n    else:\\n        tb_writer = None\\n        run_name = \\'\\'\\n        logger.setLevel(logging.ERROR)\\nTraining a Model from Scratch | 331'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 355, 'page_label': '332'}, page_content='datasets.utils.logging.set_verbosity_error()\\n        transformers.utils.logging.set_verbosity_error()\\n    return logger, tb_writer, run_name\\nEach worker gets a unique accelerator.process_index, which we use with the File\\nHandler to write the logs of each worker to an individual file. We also use the\\naccelerator.is_main_process attribute, which is only true for the main worker.\\nWe make sure we don’t initialize the TensorBoard and Weights & Biases loggers sev‐\\neral times, and we decrease the logging levels for the other workers. We return the\\nautogenerated, unique wandb.run.name, which we use later to name our experiment\\nbranch on the Hub.\\nWe’ll also define a function to log the metrics with TensorBoard and Weights & Bia‐\\nses. We again use the accelerator.is_main_process here to ensure that we only log\\nthe metrics once and not for each worker:\\ndef log_metrics(step, metrics):\\n    logger.info(f\"Step {step}: {metrics}\")\\n    if accelerator.is_main_process:\\n        wandb.log(metrics)\\n        [tb_writer.add_scalar(k, v, step) for k, v in metrics.items()]\\nNext, let’s write a function that creates the dataloaders for the training and validation\\nsets with our brand new ConstantLengthDataset class:\\nfrom torch.utils.data.dataloader import DataLoader\\ndef create_dataloaders(dataset_name):\\n    train_data = load_dataset(dataset_name+\\'-train\\', split=\"train\",\\n                              streaming=True)\\n    train_data = train_data.shuffle(buffer_size=args.shuffle_buffer,\\n                                    seed=args.seed)\\n    valid_data = load_dataset(dataset_name+\\'-valid\\', split=\"validation\",\\n                              streaming=True)\\n    train_dataset = ConstantLengthDataset(tokenizer, train_data,\\n                                          seq_length=args.seq_length)\\n    valid_dataset = ConstantLengthDataset(tokenizer, valid_data,\\n                                          seq_length=args.seq_length)\\n    train_dataloader=DataLoader(train_dataset, batch_size=args.train_batch_size)\\n    eval_dataloader=DataLoader(valid_dataset, batch_size=args.valid_batch_size)\\n    return train_dataloader, eval_dataloader\\nAt the end we wrap the dataset in a DataLoader, which also handles the batching. \\n Accelerate will take care of distributing the batches to each worker.\\nAnother aspect we need to implement is optimization. We will set up the optimizer\\nand learning rate schedule in the main loop, but we define a helper function here to\\ndifferentiate the parameters that should receive weight decay. In general, biases and\\nLayerNorm weights are not subject to weight decay:\\n332 | Chapter 10: Training Transformers from Scratch'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 356, 'page_label': '333'}, page_content='def get_grouped_params(model, no_decay=[\"bias\", \"LayerNorm.weight\"]):\\n    params_with_wd, params_without_wd = [], []\\n    for n, p in model.named_parameters():\\n        if any(nd in n for nd in no_decay):\\n            params_without_wd.append(p)\\n        else:\\n            params_with_wd.append(p)\\n    return [{\\'params\\': params_with_wd, \\'weight_decay\\': args.weight_decay},\\n            {\\'params\\': params_without_wd, \\'weight_decay\\': 0.0}]\\nFinally, we want to evaluate the model on the validation set from time to time, so let’s\\nadd an evaluation function we can call that calculates the loss and perplexity on the\\nevaluation set:\\ndef evaluate():\\n    model.eval()\\n    losses = []\\n    for step, batch in enumerate(eval_dataloader):\\n        with torch.no_grad():\\n            outputs = model(batch, labels=batch)\\n        loss = outputs.loss.repeat(args.valid_batch_size)\\n        losses.append(accelerator.gather(loss))\\n        if args.max_eval_steps > 0 and step >= args.max_eval_steps: break\\n    loss = torch.mean(torch.cat(losses))\\n    try:\\n perplexity = torch.exp(loss)\\n    except OverflowError:\\n perplexity = torch.tensor(float(\"inf\"))\\n    return loss.item(), perplexity.item()\\nThe perplexity measures how well the model’s output probability distributions pre‐\\ndict the targeted tokens. So a lower perplexity corresponds to a better performance.\\nNote that we can compute the perplexity by exponentiating the cross-entropy loss\\nwhich we get from the model’s output. Especially at the start of training when the loss\\nis still high, it is possible to get a numerical overflow when calculating the perplexity.\\nWe catch this error and set the perplexity to infinity in these instances.\\nBefore we put it all together in the training script, there is one more additional func‐\\ntion that we’ll use. As you know by now, the Hugging Face Hub uses Git under the\\nhood to store and version models and datasets. With the Repository class from the\\nhuggingface_hub library you can programmatically access the repository and pull,\\nbranch, commit, or push. We’ll use this in our script to continuously push model\\ncheckpoints to the Hub during training.\\nNow that we have all these helper functions in place, we are ready to write the heart\\nof the training script:\\nset_seed(args.seed)\\n# Accelerator\\naccelerator = Accelerator()\\nTraining a Model from Scratch | 333'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 357, 'page_label': '334'}, page_content='samples_per_step = accelerator.state.num_processes * args.train_batch_size\\n# Logging\\nlogger, tb_writer, run_name = setup_logging(project_name.split(\"/\")[1])\\nlogger.info(accelerator.state)\\n# Load model and tokenizer\\nif accelerator.is_main_process:\\n    hf_repo = Repository(\"./\", clone_from=project_name, revision=run_name)\\nmodel = AutoModelForCausalLM.from_pretrained(\"./\", gradient_checkpointing=True)\\ntokenizer = AutoTokenizer.from_pretrained(\"./\")\\n# Load dataset and dataloader\\ntrain_dataloader, eval_dataloader = create_dataloaders(dataset_name)\\n# Prepare the optimizer and learning rate scheduler\\noptimizer = AdamW(get_grouped_params(model), lr=args.learning_rate)\\nlr_scheduler = get_scheduler(name=args.lr_scheduler_type, optimizer=optimizer,\\n                             num_warmup_steps=args.num_warmup_steps,\\n                             num_training_steps=args.max_train_steps,)\\ndef get_lr():\\n    return optimizer.param_groups[0][\\'lr\\']\\n# Prepare everything with our `accelerator` (order of args is not important)\\nmodel, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\\n    model, optimizer, train_dataloader, eval_dataloader)\\n# Train model\\nmodel.train()\\ncompleted_steps = 0\\nfor step, batch in enumerate(train_dataloader, start=1):\\n    loss = model(batch, labels=batch).loss\\n    log_metrics(step, {\\'lr\\': get_lr(), \\'samples\\': step*samples_per_step,\\n                       \\'steps\\': completed_steps, \\'loss/train\\': loss.item()})\\n    loss = loss / args.gradient_accumulation_steps\\n    accelerator.backward(loss)\\n    if step % args.gradient_accumulation_steps == 0:\\n        optimizer.step()\\n        lr_scheduler.step()\\n        optimizer.zero_grad()\\n        completed_steps += 1\\n    if step % args.save_checkpoint_steps == 0:\\n        logger.info(\\'Evaluating and saving model checkpoint\\')\\n        eval_loss, perplexity = evaluate()\\n        log_metrics(step, {\\'loss/eval\\': eval_loss, \\'perplexity\\': perplexity})\\n        accelerator.wait_for_everyone()\\n        unwrapped_model = accelerator.unwrap_model(model)\\n        if accelerator.is_main_process:\\n            unwrapped_model.save_pretrained(\"./\")\\n            hf_repo.push_to_hub(commit_message=f\\'step {step}\\')\\n        model.train()\\n    if completed_steps >= args.max_train_steps:\\n334 | Chapter 10: Training Transformers from Scratch'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 358, 'page_label': '335'}, page_content='8 T. Brown et al., “Language Models Are Few-Shot Learners”, (2020).\\n        break\\n# Evaluate and save the last checkpoint\\nlogger.info(\\'Evaluating and saving model after training\\')\\neval_loss, perplexity = evaluate()\\nlog_metrics(step, {\\'loss/eval\\': eval_loss, \\'perplexity\\': perplexity})\\naccelerator.wait_for_everyone()\\nunwrapped_model = accelerator.unwrap_model(model)\\nif accelerator.is_main_process:\\n    unwrapped_model.save_pretrained(\"./\")\\n    hf_repo.push_to_hub(commit_message=f\\'final model\\')\\nThis is quite a code block, but remember that this is all the code you need to train a\\nfancy, large language model on a distributed infrastructure. Let’s deconstruct the\\nscript a little bit and highlight the most important parts:\\nModel saving\\nWe run the script from within the model repository, and at the start we check out\\na new branch named after the run_name we get from Weights & Biases. Later, we\\ncommit the model at each checkpoint and push it to the Hub. With that setup\\neach experiment is on a new branch and each commit represents a model check‐\\npoint. Note that we need to call wait_for_everyone() and unwrap_model() to\\nmake sure the model is properly synchronized when we store it.\\nOptimization\\nFor the model optimization we use AdamW with a cosine learning rate schedule\\nafter a linear warming-up period. For the hyperparameters, we closely follow the\\nparameters described in the GPT-3 paper for similar-sized models.8\\nEvaluation\\nWe evaluate the model on the evaluation set every time we save—that is, every\\nsave_checkpoint_steps and after training. Along with the validation loss we\\nalso log the validation perplexity.\\nGradient accumulation and checkpointing\\nThe required batch sizes don’t fit in a GPU’s memory, even when we run on the\\nlatest GPUs. Therefore, we implement gradient accumulation, which gathers gra‐\\ndients over several backward passes and optimizes once enough gradients are\\naccumulated. In Chapter 6, we saw how we can do this with the Trainer. For the\\nlarge model, even a single batch does not quite fit on a single GPU. Using a\\nmethod called gradient checkpointing we can trade some of the memory footprint\\nTraining a Model from Scratch | 335'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 359, 'page_label': '336'}, page_content='9 Y ou can read more about gradient checkpointing on OpenAI’s release post.\\nfor an approximately 20% training slowdown.9 This allows us to fit even the large\\nmodel in a single GPU.\\nOne aspect that might still be a bit obscure is what it means to train a model on mul‐\\ntiple GPUs. There are several approaches to train models in a distributed fashion\\ndepending on the size of your model and volume of data. The approach utilized by \\nAccelerate is called DataDistributedParallelism (DDP). The main advantage of\\nthis approach is that it allows you to train models faster with larger batch sizes that\\nwouldn’t fit into any single GPU. The process is illustrated in Figure 10-6.\\nFigure 10-6. Illustration of the processing steps in DDP with four GPUs\\nLet’s go through the pipeline step by step:\\n1. Each worker consists of a GPU. In \\n  Accelerate, there is a dataloader running on\\nthe main process that prepares the batches of data and sends them to all the\\nworkers.\\n2. Each GPU receives a batch of data and calculates the loss and respective accumu‐\\nlated gradients from forward and backward passes with a local copy of the model.\\n3. The gradients from each node are averaged with a reduce pattern, and the aver‐\\naged gradients are sent back to each worker.\\n336 | Chapter 10: Training Transformers from Scratch'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 360, 'page_label': '337'}, page_content='4. The gradients are applied using the optimizer on each node individually.\\nAlthough this might seem like redundant work, it avoids transferring copies of\\nthe large models between nodes. We’ll need to update the model at least once,\\nand without this approach the other nodes would each need to wait until they’ d\\nreceived the updated version.\\n5. Once all models are updated we start all over again, with the main worker pre‐\\nparing new batches.\\nThis simple pattern allows us to train large models extremely fast by scaling up to the\\nnumber of available GPUs without much additional logic. Sometimes, however, this is\\nnot enough. For example, if the model does not fit on a single GPU you might need\\nmore sophisticated parallelism strategies. Now that we have all the pieces needed for\\ntraining, it’s time to launch a job! As you’ll see in the next section, this is quite simple\\nto do.\\nThe Training Run\\nWe’ll save the training script in a file called codeparrot_training.py so that we can exe‐\\ncute it on our training server. To make life even easier, we’ll add it along with a\\nrequirements.txt file containing all the required Python dependencies to the model\\nrepository on the Hub. Remember that the models on the Hub are essentially Git\\nrepositories so we can just clone the repository, add any files we want, and then push\\nthem back to the Hub. On the training server, we can then spin up training with the\\nfollowing handful of commands:\\n$ git clone https://huggingface.co/transformersbook/codeparrot\\n$ cd codeparrot\\n$ pip install -r requirements.txt\\n$ wandb login\\n$ accelerate config\\n$ accelerate launch codeparrot_training.py\\nAnd that’s it—our model is now training! Note that wandb login will prompt you to\\nauthenticate with Weights & Biases for logging. The accelerate config command\\nwill guide you through setting up the infrastructure; you can see the settings used for\\nthis experiment in Table 10-2. We use an a2-megagpu-16g instance for all experi‐\\nments, which is a workstation with 16 A100 GPUs with 40 GB of memory each.\\nTraining a Model from Scratch | 337'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 361, 'page_label': '338'}, page_content='Table 10-2. Configuration used to train the CodeParrot models\\nSetting Value\\nCompute environment? multi-GPU\\nHow many machines? 1\\nDeepSpeed? No\\nHow many processes? 16\\nUse FP16? Yes\\nRunning the training script with these settings on that infrastructure takes about 24\\nhours and 7 days for the small and large models, respectively. If you train your own\\ncustom model, make sure your code runs smoothly on smaller infrastructure in order\\nto make sure that expensive long run goes smoothly as well. After the full training\\nrun completes successfully, you can merge the experiment branch on the Hub back\\ninto the main branch with the following commands:\\n$ git checkout main\\n$ git merge <RUN_NAME>\\n$ git push\\nNaturally, RUN_NAME should be the name of the experiment branch on the Hub you\\nwould like to merge. Now that we have a trained model, let’s have a look at how we\\ncan investigate its performance.\\nResults and Analysis\\nAfter anxiously monitoring the logs for a week, you will probably see loss and per‐\\nplexity curves that look like those shown in Figure 10-7. The training loss and valida‐\\ntion perplexity go down continuously, and the loss curve looks almost linear on the\\nlog-log scale. We also see that the large model converges faster in terms of processed\\ntokens, although the overall training takes longer.\\n338 | Chapter 10: Training Transformers from Scratch'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 362, 'page_label': '339'}, page_content='Figure 10-7. Training loss and validation perplexity as a function of processed tokens for\\nthe small and large CodeParrot models\\nSo what can we do with our freshly baked language model, straight out of the GPU\\noven? Well, we can use it to write some code for us. There are two types of analyses\\nwe can conduct: qualitative and quantitative. In the former, we look at concrete\\nexamples and try to better understand in which cases the model succeeds and where\\nit fails. In the latter case, we evaluate the model’s performance statistically on a large\\nset of test cases. In this section we’ll explore how we can use our model. First we’ll\\nhave a look at a few examples, and then we’ll briefly discuss how we could evaluate\\nthe model systematically and more robustly. First, let’s wrap the small model in a\\npipeline and use it to continue some code inputs:\\nfrom transformers import pipeline, set_seed\\nResults and Analysis | 339'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 363, 'page_label': '340'}, page_content='model_ckpt = \\'transformersbook/codeparrot-small\\'\\ngeneration = pipeline(\\'text-generation\\', model=model_ckpt, device=0)\\nNow we can use the generation pipeline to generate candidate completions from a\\ngiven prompt. By default, the pipeline will generate code until a predefined maximum\\nlength, and the output could contain multiple functions or classes. So, to keep the\\noutputs concise, we’ll implement a first_block() function that uses regular expres‐\\nsions to extract the first occurrence of a function or class. The complete_code()\\nfunction below applies this logic to print out the completions generated by\\nCodeParrot:\\nimport re\\nfrom transformers import set_seed\\ndef first_block(string):\\n    return re.split(\\'\\\\nclass|\\\\ndef|\\\\n#|\\\\n@|\\\\nprint|\\\\nif\\', string)[0].rstrip()\\ndef complete_code(pipe, prompt, max_length=64, num_completions=4, seed=1):\\n    set_seed(seed)\\n    gen_kwargs = {\"temperature\":0.4, \"top_p\":0.95, \"top_k\":0, \"num_beams\":1,\\n                  \"do_sample\":True,}\\n    code_gens = generation(prompt, num_return_sequences=num_completions,\\n                            max_length=max_length, **gen_kwargs)\\n    code_strings = []\\n    for code_gen in code_gens:\\n        generated_code = first_block(code_gen[\\'generated_text\\'][len(prompt):])\\n        code_strings.append(generated_code)\\n    print((\\'\\\\n\\'+\\'=\\'*80 + \\'\\\\n\\').join(code_strings))\\nLet’s start with a simple example and have the model write a function for us that cal‐\\nculates the area of a rectangle:\\nprompt = \\'\\'\\'def area_of_rectangle(a: float, b: float):\\n    \"\"\"Return the area of the rectangle.\"\"\"\\'\\'\\'\\ncomplete_code(generation, prompt)\\n    return math.sqrt(a * b)\\n================================================================================\\n    return a * b / 2.0\\n================================================================================\\n    return a * b\\n================================================================================\\n    return a * b / a\\nThat looks pretty good! Although not all the generations are correct, the right solu‐\\ntion is in there. Now, can the model also solve a more complex task of extracting\\nURLs from an HTML string? Let’s see:\\n340 | Chapter 10: Training Transformers from Scratch'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 364, 'page_label': '341'}, page_content='prompt = \\'\\'\\'def get_urls_from_html(html):\\n    \"\"\"Get all embedded URLs in a HTML string.\"\"\"\\'\\'\\'\\ncomplete_code(generation, prompt)\\n    if not html:\\n        return []\\n    return [url for url in re.findall(r\\'<a href=\"(/[^/]+/[^\"]+?)\">\\', html)]\\n================================================================================\\n    return [url for url in re.findall(r\\'<a href=\"(.*?)\"\\', html)\\n            if url]\\n================================================================================\\n    return [url for url in re.findall(r\\'<a href=\"(/.*)\",\\', html)]\\n================================================================================\\n    return re.findall(r\\'<a href=\"(.*?)\" class=\"url\"[^>]*>\\', html)\\nAlthough it didn’t quite get it right in the second attempt, the other three generations\\nare correct. We can test the function on the Hugging Face home page:\\nimport requests\\ndef get_urls_from_html(html):\\n    return [url for url in re.findall(r\\'<a href=\"(.*?)\"\\', html) if url]\\nprint(\" | \".join(get_urls_from_html(requests.get(\\'https://hf.co/\\').text)))\\nhttps://github.com/huggingface/transformers | /allenai | /facebook |\\n/asteroid-team | /google | /amazon | /speechbrain | /microsoft | /grammarly |\\n/models | /inference-api | /distilbert-base-uncased |\\n/dbmdz/bert-large-cased-finetuned-conll03-english |\\nhttps://huggingface.co/transformers | https://arxiv.org/abs/1811.06031 |\\nhttps://arxiv.org/abs/1803.10631 | https://transformer.huggingface.co/ | /coref\\n| https://medium.com/huggingface/distilbert-8cf3380435b5\\nWe can see that all the URLs starting with https are external pages, whereas the oth‐\\ners are subpages of the main website. That’s exactly what we wanted. Finally, let’s load\\nthe large model and see if we can use it to translate a function from pure Python to\\nNumPy:\\nmodel_ckpt = \\'transformersbook/codeparrot\\'\\ngeneration = pipeline(\\'text-generation\\', model=model_ckpt, device=0)\\nprompt = \\'\\'\\'# a function in native python:\\ndef mean(a):\\n    return sum(a)/len(a)\\n# the same function using numpy:\\nimport numpy as np\\ndef mean(a):\\'\\'\\'\\ncomplete_code(generation, prompt, max_length=64)\\nResults and Analysis | 341'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 365, 'page_label': '342'}, page_content=\"Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\\n    return np.mean(a)\\n================================================================================\\n    return np.mean(a)\\n================================================================================\\n    return np.mean(a)\\n================================================================================\\n    return np.mean(a)\\nThat worked! Let’s see if we can also use the CodeParrot model to help us build a\\nScikit-learn model:\\nprompt = '''X = np.random.randn(100, 100)\\ny = np.random.randint(0, 1, 100)\\n# fit random forest classifier with 20 estimators'''\\ncomplete_code(generation, prompt, max_length=96)\\nSetting `pad_token_id` to `eos_token_id`:0 for open-end generation.\\nreg = DummyRegressor()\\nforest = RandomForestClassifier(n_estimators=20)\\nforest.fit(X, y)\\n================================================================================\\nclf = ExtraTreesClassifier(n_estimators=100, max_features='sqrt')\\nclf.fit(X, y)\\n================================================================================\\nclf = RandomForestClassifier(n_estimators=20, n_jobs=n_jobs, random_state=1)\\nclf.fit(X, y)\\n================================================================================\\nclf = RandomForestClassifier(n_estimators=20)\\nclf.fit(X, y)\\nAlthough in the second attempt it tried to train an extra-trees classifier, it generated\\nwhat we asked in the other cases.\\nIn Chapter 5  we explored a few metrics to measure the quality of generated text.\\nAmong these was the BLEU score, which is frequently used for that purpose. While\\nthis metric has limitations in general, it is particularly badly suited for our use case.\\nThe BLEU score measures the overlap of n-grams between the reference texts and the\\ngenerated texts. When writing code we have a lot of freedom in terms of variables\\n342 | Chapter 10: Training Transformers from Scratch\"), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 366, 'page_label': '343'}, page_content='10 M. Chen et al., “Evaluating Large Language Models Trained on Code”, (2021).\\nand classes, and the success of a program does not depend on the naming scheme as\\nlong as it is consistent. However, the BLEU score would punish a generation that\\ndeviates from the reference naming, which might in fact be almost impossible to pre‐\\ndict (even for a human coder).\\nIn software development there are much better and more reliable ways to measure\\nthe quality of code, such as unit tests. This is how all the OpenAI Codex models were\\nevaluated: by running several code generations for coding tasks through a set of unit\\ntests and calculating the fraction of generations that pass the tests. 10 For a proper per‐\\nformance measure we should apply the same evaluation regimen to our models but\\nthis is beyond the scope of this chapter. Y ou can find details on how CodeParrot per‐\\nforms on the HumanEval benchmark in the model’s accompanying blog post.\\nConclusion\\nLet’s take a step back for a moment and contemplate what we have achieved in this\\nchapter. We set out to create a code autocomplete function for Python. First we built a\\ncustom, large-scale dataset suitable for pretraining a large language model. Then we\\ncreated a custom tokenizer that is able to efficiently encode Python code with that\\ndataset. Finally, with the help of \\n  Accelerate we put everything together and wrote a\\ntraining script to train small and large versions of a GPT-2 model from scratch on a\\nmulti-GPU infrastructure, in under two hundred lines of code. Investigating the\\nmodel outputs, we saw that it can generate reasonable code continuations, and we\\ndiscussed how the model could be systematically evaluated.\\nY ou now not only know how to fine-tune any of the many pretrained models on the\\nHub, but also how to pretrain a custom model from scratch when you have enough\\ndata and compute resources available. Y ou are now prepared to tackle almost any\\nNLP use case with transformers. So the question is: where to next? In the next and\\nlast chapter, we’ll have a look at where the field is currently moving and what new\\nexciting applications and domains beyond NLP transformer models can tackle.\\nConclusion | 343'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 367, 'page_label': '344'}, page_content=''), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 368, 'page_label': '345'}, page_content='CHAPTER 11\\nFuture Directions\\nThroughout this book we’ve explored the powerful capabilities of transformers across\\na wide range of NLP tasks. In this final chapter, we’ll shift our perspective and look at\\nsome of the current challenges with these models and the research trends that are try‐\\ning to overcome them. In the first part we explore the topic of scaling up transform‐\\ners, both in terms of model and corpus size. Then we turn our attention toward\\nvarious techniques that have been proposed to make the self-attention mechanism\\nmore efficient. Finally, we explore the emerging and exciting field of multimodal\\ntransformers, which can model inputs across multiple domains like text, images, and\\naudio.\\nScaling Transformers\\nIn 2019, the researcher Richard Sutton wrote a provocative essay entitled “The Bitter\\nLesson” in which he argued that:\\nThe biggest lesson that can be read from 70 years of AI research is that general meth‐\\nods that leverage computation are ultimately the most effective, and by a large mar‐\\ngin…. Seeking an improvement that makes a difference in the shorter term,\\nresearchers seek to leverage their human knowledge of the domain, but the only thing\\nthat matters in the long run is the leveraging of computation. These two need not run\\ncounter to each other, but in practice they tend to…. And the human-knowledge\\napproach tends to complicate methods in ways that make them less suited to taking\\nadvantage of general methods leveraging computation.\\nThe essay provides several historical examples, such as playing chess or Go, where the\\napproach of encoding human knowledge within AI systems was ultimately outdone\\nby increased computation. Sutton calls this the “bitter lesson” for the AI research\\nfield:\\n345'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 369, 'page_label': '346'}, page_content='We have to learn the bitter lesson that building in how we think we think does not\\nwork in the long run…. One thing that should be learned from the bitter lesson is the\\ngreat power of general purpose methods, of methods that continue to scale with\\nincreased computation even as the available computation becomes very great. The two\\nmethods that seem to scale arbitrarily in this way are search and learning.\\nThere are now signs that a similar lesson is at play with transformers; while many of\\nthe early BERT and GPT descendants focused on tweaking the architecture or pre‐\\ntraining objectives, the best-performing models in mid-2021, like GPT-3, are essen‐\\ntially basic scaled-up versions of the original models without many architectural\\nmodifications. In Figure 11-1 you can see a timeline of the development of the largest\\nmodels since the release of the original Transformer architecture in 2017, which\\nshows that model size has increased by over four orders of magnitude in just a few\\nyears!\\nFigure 11-1. Parameter counts over time for prominent Transformer architectures\\nThis dramatic growth is motivated by empirical evidence that large language models\\nperform better on downstream tasks and that interesting capabilities such as zero-\\nshot and few-shot learning emerge in the 10- to 100-billion parameter range. How‐\\never, the number of parameters is not the only factor that affects model performance;\\nthe amount of compute and training data must also be scaled in tandem to train these\\nmonsters. Given that large language models like GPT-3 are estimated to cost $4.6\\nmillion to train, it is clearly desirable to be able to estimate the model’s performance\\nin advance. Somewhat surprisingly, the performance of language models appears to\\n346 | Chapter 11: Future Directions'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 370, 'page_label': '347'}, page_content='1 J. Kaplan et al., “Scaling Laws for Neural Language Models”, (2020).\\n2 The dataset size is measured in the number of tokens, while the model size excludes parameters from the\\nembedding layers.\\nobey a power law relationship with model size and other factors that is codified in a set\\nof scaling laws.1 Let’s take a look at this exciting area of research.\\nScaling Laws\\nScaling laws allow one to empirically quantify the “bigger is better” paradigm for lan‐\\nguage models by studying their behavior with varying compute budget C, dataset size\\nD, and model size N.2 The basic idea is to chart the dependence of the cross-entropy\\nloss L on these three factors and determine if a relationship emerges. For autoregres‐\\nsive models like those in the GPT family, the resulting loss curves are shown in\\nFigure 11-2, where each blue curve represents the training run of a single model.\\nFigure 11-2. Power-law scaling of test loss versus compute budget (left), dataset size\\n(middle), and model size (right) (courtesy of Jared Kaplan)\\nFrom these loss curves we can draw a few conclusions about:\\nThe relationship of performance and scale\\nAlthough many NLP researchers focus on architectural tweaks or hyperparame‐\\nter optimization (like tuning the number of layers or attention heads) to improve\\nperformance on a fixed set of datasets, the implication of scaling laws is that a\\nmore productive path toward better models is to focus on increasing N, C, and D\\nin tandem.\\nSmooth power laws\\nThe test loss L has a power law relationship with each of N, C, and D across sev‐\\neral orders of magnitude (power law relationships are linear on a log-log scale).\\nFor X = N, C, D we can express these power law relationships as L X ∼ 1/ Xα,\\nwhere α is a scaling exponent that is determined by a fit to the loss curves shown\\nScaling Transformers | 347'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 371, 'page_label': '348'}, page_content='3 T. Henighan et al., “Scaling Laws for Autoregressive Generative Modeling”, (2020).\\nin Figure 11-2.3 Typical values for αX lie in the 0.05–0.095 range, and one attrac‐\\ntive feature of these power laws is that the early part of a loss curve can be\\nextrapolated to predict what the approximate loss would be if training was con‐\\nducted for much longer.\\nSample efficiency\\nLarge models are able to reach the same performance as smaller models with a\\nsmaller number of training steps. This can be seen by comparing the regions\\nwhere a loss curve plateaus over some number of training steps, which indicates\\none gets diminishing returns in performance compared to simply scaling up the\\nmodel.\\nSomewhat surprisingly, scaling laws have also been observed for other modalities, like\\nimages, videos, and mathematical problem solving, as illustrated in Figure 11-3.\\nFigure 11-3. Power-law scaling of test loss versus compute budget across a wide range of\\nmodalities (courtesy of Tom Henighan)\\nWhether power-law scaling is a universal property of transformer language models is\\ncurrently unknown. For now, we can use scaling laws as a tool to extrapolate large,\\nexpensive models without having to explicitly train them. However, scaling isn’t quite\\nas easy as it sounds. Let’s now look at a few challenges that crop up when charting this\\nfrontier.\\n348 | Chapter 11: Future Directions'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 372, 'page_label': '349'}, page_content='4 However, recently a distributed deep learning framework has been proposed that enables smaller groups to\\npool their computational resources and pretrain models in a collaborative fashion. See M. Diskin et al., “Dis‐\\ntributed Deep Learning in Open Collaborations”, (2021).\\nChallenges with Scaling\\nWhile scaling up sounds simple in theory (“just add more layers!”), in practice there\\nare many difficulties. Here are a few of the biggest challenges you’re likely to\\nencounter when scaling language models:\\nInfrastructure\\nProvisioning and managing infrastructure that potentially spans hundreds or\\nthousands of nodes with as many GPUs is not for the faint-hearted. Are the\\nrequired number of nodes available? Is communication between nodes a bottle‐\\nneck? Tackling these issues requires a very different skill set than that found in\\nmost data science teams, and typically involves specialized engineers familiar\\nwith running large-scale, distributed experiments.\\nCost\\nMost ML practitioners have experienced the feeling of waking up in the middle\\nof the night in a cold sweat, remembering they forgot to shut down that fancy\\nGPU on the cloud. This feeling intensifies when running large-scale experiments,\\nand most companies cannot afford the teams and resources necessary to train\\nmodels at the largest scales. Training a single GPT-3-sized model can cost several\\nmillion dollars, which is not the kind of pocket change that many companies\\nhave lying around.4\\nDataset curation\\nA model is only as good as the data it is trained on. Training large models\\nrequires large, high-quality datasets. When using terabytes of text data it\\nbecomes harder to make sure the dataset contains high-quality text, and even\\npreprocessing becomes challenging. Furthermore, one needs to ensure that there\\nis a way to control biases like sexism and racism that these language models can\\nacquire when trained on large-scale webtext corpora. Another type of considera‐\\ntion revolves around licensing issues with the training data and personal infor‐\\nmation that can be embedded in large text datasets.\\nModel evaluation\\nOnce the model is trained, the challenges don’t stop. Evaluating the model on\\ndownstream tasks again requires time and resources. In addition, you’ll want to\\nprobe the model for biased and toxic generations, even if you are confident that\\nyou created a clean dataset. These steps take time and need to be carried out\\nthoroughly to minimize the risks of adverse effects later on.\\nScaling Transformers | 349'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 373, 'page_label': '350'}, page_content='Deployment\\nFinally, serving large language models also poses a significant challenge. In Chap‐\\nter 8 we looked at a few approaches, such as distillation, pruning, and quantiza‐\\ntion, to help with these issues. However, this may not be enough if you are\\nstarting with a model that is hundreds of gigabytes in size. Hosted services such\\nas the OpenAI API or Hugging Face’s Accelerated Inference API are designed to\\nhelp companies that cannot or do not want to deal with these deployment\\nchallenges.\\nThis is by no means an exhaustive list, but it should give you an idea of the kinds of\\nconsiderations and challenges that go hand in hand with scaling language models to\\never larger sizes. While most of these efforts are centralized around a few institutions\\nthat have the resources and know-how to push the boundaries, there are currently\\ntwo community-led projects that aim to produce and probe large language models in\\nthe open:\\nBigScience\\nThis is a one-year-long research workshop that runs from 2021 to 2022 and is\\nfocused on large language models. The workshop aims to foster discussions and\\nreflections around the research questions surrounding these models (capabilities,\\nlimitations, potential improvements, bias, ethics, environmental impact, role in\\nthe general AI/cognitive research landscape) as well as the challenges around cre‐\\nating and sharing such models and datasets for research purposes and among the\\nresearch community. The collaborative tasks involve creating, sharing, and evalu‐\\nating a large multilingual dataset and a large language model. An unusually large\\ncompute budget was allocated for these collaborative tasks (several million GPU\\nhours on several thousands GPUs). If successful, this workshop will run again in\\nthe future, focusing on involving an updated or different set of collaborative\\ntasks. If you want to join the effort, you can find more information at the proj‐\\nect’s website.\\nEleutherAI\\nThis is a decentralized collective of volunteer researchers, engineers, and devel‐\\nopers focused on AI alignment, scaling, and open source AI research. One of its\\naims is to train and open-source a GPT-3-sized model, and the group has already\\nreleased some impressive models like GPT-Neo and GPT-J, which is a 6-billion-\\nparameter model and currently the best-performing publicly available trans‐\\nformer in terms of zero-shot performance. Y ou can find more information at\\nEleutherAI’s website.\\nNow that we’ve explored how to scale transformers across compute, model size, and\\ndataset size, let’s examine another active area of research: making self-attention more\\nefficient.\\n350 | Chapter 11: Future Directions'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 374, 'page_label': '351'}, page_content='5 Although standard implementations of self-attention have On2 time and memory complexity, a recent paper\\nby Google researchers shows that the memory complexity can be reduced to Ologn via a simple reordering\\nof the operations.\\n6 Yi Tay et al., “Efficient Transformers: A Survey”, (2020).\\nAttention Please!\\nWe’ve seen throughout this book that the self-attention mechanism plays a central\\nrole in the architecture of transformers; after all, the original Transformer paper is\\ncalled “ Attention Is All Y ou Need”! However, there is a key challenge associated with\\nself-attention: since the weights are generated from pairwise comparisons of all the\\ntokens in a sequence, this layer becomes a computational bottleneck when trying to\\nprocess long documents or apply transformers to domains like speech processing or\\ncomputer vision. In terms of time and memory complexity, the self-attention layer of\\nthe Transformer architecture naively scales like \\ud835n2, where n is the length of the\\nsequence.5\\nAs a result, much of the recent research on transformers has focused on making self-\\nattention more efficient. The research directions are broadly clustered in Figure 11-4.\\nFigure 11-4. A summarization of research directions to make attention more efficient\\n(courtesy of Yi Tay et al.)6\\nScaling Transformers | 351'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 375, 'page_label': '352'}, page_content='7 T. Lin et al., “ A Survey of Transformers”, (2021).\\nA common pattern is to make attention more efficient by introducing sparsity into\\nthe attention mechanism or by applying kernels to the attention matrix. Let’s take a\\nquick look at some of the most popular approaches to make self-attention more effi‐\\ncient, starting with sparsity.\\nSparse Attention\\nOne way to reduce the number of computations that are performed in the self-\\nattention layer is to simply limit the number of query-key pairs that are generated\\naccording to some predefined pattern. There have been many sparsity patterns\\nexplored in the literature, but most of them can be decomposed into a handful of\\n“atomic” patterns illustrated in Figure 11-5.\\nFigure 11-5. Common atomic sparse attention patterns for self-attention: a colored\\nsquare means the attention score is calculated, while a blank square means the score is\\ndiscarded (courtesy of Tianyang Lin)\\nWe can describe these patterns as follows:7\\nGlobal attention\\nDefines a few special tokens in the sequence that are allowed to attend to all other\\ntokens\\nBand attention\\nComputes attention over a diagonal band\\nDilated attention\\nSkips some query-key pairs by using a dilated window with gaps\\nRandom attention\\nRandomly samples a few keys for each query to compute attention scores\\n352 | Chapter 11: Future Directions'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 376, 'page_label': '353'}, page_content='Block local attention\\nDivides the sequence into blocks and restricts attention within these blocks\\nIn practice, most transformer models with sparse attention use a mix of the atomic\\nsparsity patterns shown in Figure 11-5 to generate the final attention matrix. As illus‐\\ntrated in Figure 11-6, models like Longformer use a mix of global and band attention,\\nwhile BigBird adds random attention to the mix. Introducing sparsity into the atten‐\\ntion matrix enables these models to process much longer sequences; in the case of\\nLongformer and BigBird the maximum sequence length is 4,096 tokens, which is 8\\ntimes larger than BERT!\\nFigure 11-6. Sparse attention patterns for recent transformer models (courtesy of\\nTianyang Lin)\\nIt is also possible to learn the sparsity pattern in a data-driven man‐\\nner. The basic idea behind such approaches is to cluster the tokens\\ninto chunks. For example, Reformer uses a hash function to cluster\\nsimilar tokens together.\\nNow that we’ve seen how sparsity can reduce the complexity of self-attention, let’s\\ntake a look at another popular approach based on changing the operations directly.\\nLinearized Attention\\nAn alternative way to make self-attention more efficient is to change the order of\\noperations that are involved in computing the attention scores. Recall that to compute\\nthe self-attention scores of the queries and keys we need a similarity function, which\\nfor the transformer is just a simple dot product. However, for a general similarity\\nfunction sim qi, kj  we can express the attention outputs as the following equation:\\nyi = ∑\\nj\\nsim Qi, K j\\n∑k sim Qi, Kk\\nV j\\nScaling Transformers | 353'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 377, 'page_label': '354'}, page_content='8 A. Katharopoulos et al., “Transformers Are RNNs: Fast Autoregressive Transformers with Linear Attention”,\\n(2020); K. Choromanski et al., “Rethinking Attention with Performers”, (2020).\\nThe trick behind linearized attention mechanisms is to express the similarity function\\nas a kernel function that decomposes the operation into two pieces:\\nsim Qj, K j = φ Qi\\nTφ K j\\nwhere φ is typically a high-dimensional feature map. Since φQi is independent of j\\nand k, we can pull it under the sums to write the attention outputs as follows:\\nyi =\\nφ Qi\\nT ∑j φ K j V j\\nT\\nφ Qi\\nT ∑k φ Kk\\nBy first computing ∑j φ K j V j\\nT and ∑k φ Kk , we can effectively linearize the space and\\ntime complexity of self-attention! The comparison between the two approaches is\\nillustrated in Figure 11-7 . Popular models that implement linearized self-attention\\ninclude Linear Transformer and Performer.8\\nFigure 11-7. Complexity difference between standard self-attention and linearized self-\\nattention (courtesy of Tianyang Lin)\\nIn this section we’ve seen how Transformer architectures in general and attention in\\nparticular can be scaled up to achieve even better performance on a wide range of\\ntasks. In the next section we’ll have a look at how transformers are branching out of\\nNLP into other domains such as audio and computer vision.\\nGoing Beyond Text\\nUsing text to train language models has been the driving force behind the success of\\ntransformer language models, in combination with transfer learning. On the one\\nhand, text is abundant and enables self-supervised training of large models. On the\\nother hand, textual tasks such as classification and question answering are common,\\n354 | Chapter 11: Future Directions'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 378, 'page_label': '355'}, page_content='9 J. Gordon and B. Van Durme, “Reporting Bias and Knowledge Extraction”, (2013).\\n10 M. Chen et al., “Generative Pretraining from Pixels, ” Proceedings of the 37th International Conference on\\nMachine Learning 119 (2020):1691–1703, https://proceedings.mlr.press/v119/chen20s.html.\\nand developing effective strategies for them allows us to address a wide range of real-\\nworld problems.\\nHowever, there are limits to this approach, including:\\nHuman reporting bias\\nThe frequencies of events in text may not represent their true frequencies. 9 A\\nmodel solely trained on text from the internet might have a very distorted image\\nof the world.\\nCommon sense\\nCommon sense is a fundamental quality of human reasoning, but is rarely writ‐\\nten down. As such, language models trained on text might know many facts\\nabout the world, but lack basic common-sense reasoning.\\nFacts\\nA probabilistic language model cannot store facts in a reliable way and can pro‐\\nduce text that is factually wrong. Similarly, such models can detect named enti‐\\nties, but have no direct way to access information about them.\\nModality\\nLanguage models have no way to connect to other modalities that could address\\nthe previous points, such as audio or visual signals or tabular data.\\nSo, if we could solve the modality limitations we could potentially address some of\\nthe others as well. Recently there has been a lot of progress in pushing transformers\\nto new modalities, and even building multimodal models. In this section we’ll high‐\\nlight a few of these advances.\\nVision\\nVision has been the stronghold of convolutional neural networks (CNNs) since they\\nkickstarted the deep learning revolution. More recently, transformers have begun to\\nbe applied to this domain and to achieve efficiency similar to or better than CNNs.\\nLet’s have a look at a few examples.\\niGPT\\nInspired by the success of the GPT family of models with text, iGPT (short for image\\nGPT) applies the same methods to images.10 By viewing images as sequences of pixels,\\niGPT uses the GPT architecture and autoregressive pretraining objective to predict\\nGoing Beyond Text | 355'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 379, 'page_label': '356'}, page_content='11 A. Dosovitskiy et al., “ An Image Is Worth 16x16 Words: Transformers for Image Recognition at Scale”, (2020).\\nthe next pixel values. Pretraining on large image datasets enables iGPT to “autocom‐\\nplete” partial images, as displayed in Figure 11-8. It also achieves performant results\\non classification tasks when a classification head is added to the model.\\nFigure 11-8. Examples of image completions with iGPT (courtesy of Mark Chen)\\nViT\\nWe saw that iGPT follows closely the GPT-style architecture and pretraining proce‐\\ndure. Vision Transformer (ViT)11 is a BERT-style take on transformers for vision, as\\nillustrated in Figure 11-9 . First the image is split into smaller patches, and each of\\nthese patches is embedded with a linear projection. The results strongly resemble the\\ntoken embeddings in BERT, and what follows is virtually identical. The patch embed‐\\ndings are combined with position embeddings and then fed through an ordinary\\ntransformer encoder. During pretraining some of the patches are masked or distor‐\\nted, and the objective is to predict the average color of the masked patch.\\n356 | Chapter 11: Future Directions'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 380, 'page_label': '357'}, page_content='Figure 11-9. The ViT architecture (courtesy of Alexey Dosovitskiy et al.)\\nAlthough this approach did not produce better results when pretrained on the stan‐\\ndard ImageNet dataset, it scaled significantly better than CNNs on larger datasets.\\nViT is integrated in \\n  Transformers, and using it is very similar to the NLP pipelines\\nthat we’ve used throughout this book. Let’s start by loading the image of a rather\\nfamous dog:\\nfrom PIL import Image\\nimport matplotlib.pyplot as plt\\nimage = Image.open(\"images/doge.jpg\")\\nplt.imshow(image)\\nplt.axis(\"off\")\\nplt.show()\\nGoing Beyond Text | 357'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 381, 'page_label': '358'}, page_content='12 G. Bertasius, H. Wang, and L. Torresani, “Is Space-Time Attention All Y ou Need for Video Understanding?”,\\n(2021).\\nTo load a ViT model, we just need to specify the image-classification pipeline,\\nand then we feed in the image to extract the predicted classes:\\nimport pandas as pd\\nfrom transformers import pipeline\\nimage_classifier = pipeline(\"image-classification\")\\npreds = image_classifier(image)\\npreds_df = pd.DataFrame(preds)\\npreds_df\\nscore label\\n0 0.643599 Eskimo dog, husky\\n1 0.207407 Siberian husky\\n2 0.060160 dingo, warrigal, warragal, Canis dingo\\n3 0.035359 Norwegian elkhound, elkhound\\n4 0.012927 malamute, malemute, Alaskan malamute\\nGreat, the predicted class seems to match the image!\\nA natural extension of image models is video models. In addition to the spatial\\ndimensions, videos come with a temporal dimension. This makes the task more chal‐\\nlenging as the volume of data gets much bigger and one needs to deal with the extra\\ndimension. Models such as TimeSformer introduce a spatial and temporal attention\\nmechanism to account for both.12 In the future, such models can help build tools for a\\nwide range of tasks such as classification or annotation of video sequences.\\n358 | Chapter 11: Future Directions'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 382, 'page_label': '359'}, page_content='13 J. Herzig et al., “TAPAS: Weakly Supervised Table Parsing via Pre-Training”, (2020).\\nTables\\nA lot of data, such as customer data within a company, is stored in structured data‐\\nbases instead of as raw text. We saw in Chapter 7 that with question answering mod‐\\nels we can query text with a question in natural text. Wouldn’t it be nice if we could\\ndo the same with tables, as shown in Figure 11-10?\\nFigure 11-10. Question answering over a table (courtesy of Jonathan Herzig)\\nTAPAS (short for Table Parser) 13 to the rescue! This model applies the Transformer\\narchitecture to tables by combining the tabular information with the query, as illus‐\\ntrated in Figure 11-11.\\nFigure 11-11. Architecture of TAPAS (courtesy of Jonathan Herzig)\\nLet’s look at an example of how TAPAS works in practice. We have created a fictitious\\nversion of this book’s table of contents. It contains the chapter number, the name of\\nthe chapter, as well as the starting and ending pages of the chapters:\\nbook_data = [\\n    {\"chapter\": 0, \"name\": \"Introduction\", \"start_page\": 1, \"end_page\": 11},\\n    {\"chapter\": 1, \"name\": \"Text classification\", \"start_page\": 12,\\n     \"end_page\": 48},\\n    {\"chapter\": 2, \"name\": \"Named Entity Recognition\", \"start_page\": 49,\\n     \"end_page\": 73},\\n    {\"chapter\": 3, \"name\": \"Question Answering\", \"start_page\": 74,\\nGoing Beyond Text | 359'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 383, 'page_label': '360'}, page_content='\"end_page\": 120},\\n    {\"chapter\": 4, \"name\": \"Summarization\", \"start_page\": 121,\\n     \"end_page\": 140},\\n    {\"chapter\": 5, \"name\": \"Conclusion\", \"start_page\": 141,\\n     \"end_page\": 144}\\n]\\nWe can also easily add the number of pages each chapter has with the existing fields.\\nIn order to play nicely with the TAPAS model, we need to make sure that all columns\\nare of type str:\\ntable = pd.DataFrame(book_data)\\ntable[\\'number_of_pages\\'] = table[\\'end_page\\']-table[\\'start_page\\']\\ntable = table.astype(str)\\ntable\\nchapter name start_page end_page number_of_pages\\n0 0 Introduction 1 11 10\\n1 1 Text classification 12 48 36\\n2 2 Named Entity Recognition 49 73 24\\n3 3 Question Answering 74 120 46\\n4 4 Summarization 121 140 19\\n5 5 Conclusion 141 144 3\\nBy now you should know the drill. We first load the table-question-answering\\npipeline:\\ntable_qa = pipeline(\"table-question-answering\")\\nand then pass some queries to extract the answers:\\ntable_qa = pipeline(\"table-question-answering\")\\nqueries = [\"What\\'s the topic in chapter 4?\",\\n           \"What is the total number of pages?\",\\n           \"On which page does the chapter about question-answering start?\",\\n           \"How many chapters have more than 20 pages?\"]\\npreds = table_qa(table, queries)\\nThese predictions store the type of table operation in an aggregator field, along with\\nthe answer. Let’s see how well TAPAS fared on our questions:\\nfor query, pred in zip(queries, preds):\\n    print(query)\\n    if pred[\"aggregator\"] == \"NONE\":\\n        print(\"Predicted answer: \" + pred[\"answer\"])\\n    else:\\n        print(\"Predicted answer: \" + pred[\"answer\"])\\n    print(\\'=\\'*50)\\n360 | Chapter 11: Future Directions'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 384, 'page_label': '361'}, page_content=\"What's the topic in chapter 4?\\nPredicted answer: Summarization\\n==================================================\\nWhat is the total number of pages?\\nPredicted answer: SUM > 10, 36, 24, 46, 19, 3\\n==================================================\\nOn which page does the chapter about question-answering start?\\nPredicted answer: AVERAGE > 74\\n==================================================\\nHow many chapters have more than 20 pages?\\nPredicted answer: COUNT > 1, 2, 3\\n==================================================\\nFor the first chapter, the model predicted exactly one cell with no aggregation. If we\\nlook at the table, we see that the answer is in fact correct. In the next example the\\nmodel predicted all the cells containing the number of pages in combination with the\\nsum aggregator, which again is the correct way of calculating the total number of\\npages. The answer to question three is also correct; the average aggregation is not\\nnecessary in that case, but it doesn’t make a difference. Finally, we have a question\\nthat is a little bit more complex. To determine how many chapters have more than 20\\npages we first need to find out which chapters satisfy that criterion and then count\\nthem. It seem that TAPAS again got it right and correctly determined that chapters 1,\\n2, and 3 have more than 20 pages, and added a count aggregator to the cells.\\nThe kinds of questions we asked can also be solved with a few simple Pandas com‐\\nmands; however, the ability to ask questions in natural language instead of Python\\ncode allows a much wider audience to query the data to answer specific questions.\\nImagine such tools in the hands of business analysts or managers who are able verify\\ntheir own hypotheses about the data!\\nMultimodal Transformers\\nSo far we’ve looked at extending transformers to a single new modality. TAPAS is\\narguably multimodal since it combines text and tables, but the table is also treated as\\ntext. In this section we examine transformers that combine two modalities at once:\\naudio plus text and vision plus text.\\nSpeech-to-Text\\nAlthough being able to use text to interface with a computer is a huge step forward,\\nusing spoken language is an even more natural way for us to communicate. Y ou can\\nsee this trend in industry, where applications such as Siri and Alexa are on the rise\\nand becoming progressively more useful. Also, for a large fraction of the population,\\nwriting and reading are more challenging than speaking. So, being able to process\\nand understand audio is not only convenient, but can help many people access more\\ninformation. A common task in this domain is automatic speech recognition (ASR),\\nMultimodal Transformers | 361\"), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 385, 'page_label': '362'}, page_content='14 A. Baevski et al., “wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations”, (2020).\\nwhich converts spoken words to text and enables voice technologies like Siri to\\nanswer questions like “What is the weather like today?”\\nThe wav2vec 2.0 family of models are one of the most recent developments in ASR:\\nthey use a transformer layer in combination with a CNN, as illustrated in\\nFigure 11-12.14 By leveraging unlabeled data during pretraining, these models achieve\\ncompetitive results with only a few minutes of labeled data.\\nFigure 11-12. Architecture of wav2vec 2.0 (courtesy of Alexei Baevski)\\nThe wav2vec 2.0 models are integrated in \\n  Transformers, and you won’t be sur‐\\nprised to learn that loading and using them follows the familiar steps that we have\\nseen throughout this book. Let’s load a pretrained model that was trained on 960\\nhours of speech audio:\\nasr = pipeline(\"automatic-speech-recognition\")\\nTo apply this model to some audio files we’ll use the ASR subset of the SUPERB data‐\\nset, which is the same dataset the model was pretrained on. Since the dataset is quite\\nlarge, we’ll just load one example for our demo purposes:\\nfrom datasets import load_dataset\\nds = load_dataset(\"superb\", \"asr\", split=\"validation[:1]\")\\nprint(ds[0])\\n{\\'chapter_id\\': 128104, \\'speaker_id\\': 1272, \\'file\\': \\'~/.cache/huggingf\\nace/datasets/downloads/extracted/e4e70a454363bec1c1a8ce336139866a39442114d86a433\\n362 | Chapter 11: Future Directions'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 386, 'page_label': '363'}, page_content='15 A. Baevski et al., “Unsupervised Speech Recognition”, (2021).\\n6014acd4b1ed55e55/LibriSpeech/dev-clean/1272/128104/1272-128104-0000.flac\\',\\n\\'id\\': \\'1272-128104-0000\\', \\'text\\': \\'MISTER QUILTER IS THE APOSTLE OF THE MIDDLE\\nCLASSES AND WE ARE GLAD TO WELCOME HIS GOSPEL\\'}\\nHere we can see that the audio in the file column is stored in the FLAC coding for‐\\nmat, while the expected transcription is given by the text column. To convert the\\naudio to an array of floats, we can use the SoundFile library to read each file in our\\ndataset with map():\\nimport soundfile as sf\\ndef map_to_array(batch):\\n    speech, _ = sf.read(batch[\"file\"])\\n    batch[\"speech\"] = speech\\n    return batch\\nds = ds.map(map_to_array)\\nIf you are using a Jupyter notebook you can easily play the sound files with the fol‐\\nlowing IPython widgets:\\nfrom IPython.display import Audio\\ndisplay(Audio(ds[0][\\'speech\\'], rate=16000))\\nFinally, we can pass the inputs to the pipeline and inspect the prediction:\\npred = asr(ds[0][\"speech\"])\\nprint(pred)\\n{\\'text\\': \\'MISTER QUILTER IS THE APOSTLE OF THE MIDDLE CLASSES AND WE ARE GLAD TO\\nWELCOME HIS GOSPEL\\'}\\nThis transcription seems to be correct. We can see that some punctuation is missing,\\nbut this is hard to get from audio alone and could be added in a postprocessing step.\\nWith only a handful of lines of code we can build ourselves a state-of-the-art speech-\\nto-text application!\\nBuilding a model for a new language still requires a minimum amount of labeled\\ndata, which can be challenging to obtain, especially for low-resource languages. Soon\\nafter the release of wav2vec 2.0, a paper describing a method named wav2vec-U was\\npublished.15 In this work, a combination of clever clustering and GAN training is\\nused to build a speech-to-text model using only independent unlabeled speech and\\nunlabeled text data. This process is visualized in detail in Figure 11-13 . No aligned\\nspeech and text data is required at all, which enables the training of highly perform‐\\nant speech-to-text models for a much larger spectrum of languages.\\nMultimodal Transformers | 363'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 387, 'page_label': '364'}, page_content='16 Y . Goyal et al., “Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question\\nAnswering”, (2016).\\nFigure 11-13. Training scheme for wav2vec-U (courtesy of Alexsei Baevski)\\nGreat, so transformers can now “read” text and “hear” audio—can they also “see”?\\nThe answer is yes, and this is one of the current hot research frontiers in the field.\\nVision and Text\\nVision and text are another natural pair of modalities to combine since we frequently\\nuse language to communicate and reason about the contents of images and videos. In\\naddition to the vision transformers, there have been several developments in the\\ndirection of combining visual and textual information. In this section we will look at\\nfour examples of models combining vision and text: VisualQA, LayoutLM, DALL·E,\\nand CLIP .\\nVQA\\nIn Chapter 7 we explored how we can use transformer models to extract answers to\\ntext-based questions. This can be done ad hoc to extract information from texts or\\noffline, where the question answering model is used to extract structured information\\nfrom a set of documents. There have been several efforts to expand this approach to\\nvision with datasets such as VQA,16 shown in Figure 11-14.\\n364 | Chapter 11: Future Directions'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 388, 'page_label': '365'}, page_content='17 H. Tan and M. Bansal, “LXMERT: Learning Cross-Modality Encoder Representations from Transformers”,\\n(2019); L.H. Li et al., “VisualBERT: A Simple and Performant Baseline for Vision and Language”, (2019).\\nFigure 11-14. Example of a visual question answering task from the VQA dataset (cour‐\\ntesy of Yash Goyal)\\nModels such as LXMERT and VisualBERT use vision models like ResNets to extract\\nfeatures from the pictures and then use transformer encoders to combine them with\\nthe natural questions and predict an answer.17\\nLayoutLM\\nAnalyzing scanned business documents like receipts, invoices, or reports is another\\narea where extracting visual and layout information can be a useful way to recognize\\ntext fields of interest. Here the LayoutLM family of models are the current state of the\\nart. They use an enhanced Transformer architecture that receives three modalities as\\ninput: text, image, and layout. Accordingly, as shown in Figure 11-15 , there are\\nembedding layers associated with each modality, a spatially aware self-attention\\nmechanism, and a mix of image and text/image pretraining objectives to align the\\ndifferent modalities. By pretraining on millions of scanned documents, LayoutLM\\nmodels are able to transfer to various downstream tasks in a manner similar to BERT\\nfor NLP .\\nMultimodal Transformers | 365'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 389, 'page_label': '366'}, page_content='18 A. Ramesh et al., “Zero-Shot Text-to-Image Generation”, (2021).\\nFigure 11-15. The model architecture and pretraining strategies for LayoutLMv2 (cour‐\\ntesy of Yang Xu)\\nDALL·E\\nA model that combines vision and text for generative tasks is DALL·E. 18 It uses the\\nGPT architecture and autoregressive modeling to generate images from text. Inspired\\nby iGPT, it regards the words and pixels as one sequence of tokens and is thus able to\\ncontinue generating an image from a text prompt, as shown in Figure 11-16.\\n366 | Chapter 11: Future Directions'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 390, 'page_label': '367'}, page_content='19 A. Radford et al., “Learning Transferable Visual Models from Natural Language Supervision”, (2021).\\nFigure 11-16. Generation examples with DALL·E (courtesy of Aditya Ramesh)\\nCLIP\\nFinally, let’s have a look at CLIP ,19 which also combines text and vision but is designed\\nfor supervised tasks. Its creators constructed a dataset with 400 million image/caption\\npairs and used contrastive learning to pretrain the model. The CLIP architecture con‐\\nsists of a text and an image encoder (both transformers) that create embeddings of\\nthe captions and images. A batch of images with captions is sampled, and the contras‐\\ntive objective is to maximize the similarity of the embeddings (as measured by the dot\\nproduct) of the corresponding pair while minimizing the similarity of the rest, as\\nillustrated in Figure 11-17.\\nIn order to use the pretrained model for classification the possible classes are embed‐\\nded with the text encoder, similar to how we used the zero-shot pipeline. Then the\\nembeddings of all the classes are compared to the image embedding that we want to\\nclassify, and the class with the highest similarity is chosen.\\nMultimodal Transformers | 367'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 391, 'page_label': '368'}, page_content='Figure 11-17. Architecture of CLIP (courtesy of Alec Radford)\\nThe zero-shot image classification performance of CLIP is remarkable and competi‐\\ntive with fully supervised trained vision models, while being more flexible with\\nregard to new classes. CLIP is also fully integrated in \\n  Transformers, so we can try it\\nout. For image-to-text tasks, we instantiate a processor that consists of a feature extrac‐\\ntor and a tokenizer. The role of the feature extractor is to convert the image into a\\n368 | Chapter 11: Future Directions'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 392, 'page_label': '369'}, page_content='form suitable for the model, while the tokenizer is responsible for decoding the mod‐\\nel’s predictions into text:\\nfrom transformers import CLIPProcessor, CLIPModel\\nclip_ckpt = \"openai/clip-vit-base-patch32\"\\nmodel = CLIPModel.from_pretrained(clip_ckpt)\\nprocessor = CLIPProcessor.from_pretrained(clip_ckpt)\\nThen we need a fitting image to try it out. What would be better suited than a picture\\nof Optimus Prime?\\nimage = Image.open(\"images/optimusprime.jpg\")\\nplt.imshow(image)\\nplt.axis(\"off\")\\nplt.show()\\nNext, we set up the texts to compare the image against and pass it through the model:\\nimport torch\\ntexts = [\"a photo of a transformer\", \"a photo of a robot\", \"a photo of agi\"]\\ninputs = processor(text=texts, images=image, return_tensors=\"pt\", padding=True)\\nwith torch.no_grad():\\n    outputs = model(**inputs)\\nlogits_per_image = outputs.logits_per_image\\nprobs = logits_per_image.softmax(dim=1)\\nprobs\\ntensor([[0.9557, 0.0413, 0.0031]])\\nWell, it almost got the right answer (a photo of AGI of course). Jokes aside, CLIP\\nmakes image classification very flexible by allowing us to define classes through text\\ninstead of having the classes hardcoded in the model architecture. This concludes our\\ntour of multimodal transformer models, but we hope we’ve whetted your appetite.\\nMultimodal Transformers | 369'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 393, 'page_label': '370'}, page_content='Where to from Here?\\nWell that’s the end of the ride; thanks for joining us on this journey through the trans‐\\nformers landscape! Throughout this book we’ve explored how transformers can\\naddress a wide range of tasks and achieve state-of-the-art results. In this chapter we’ve\\nseen how the current generation of models are being pushed to their limits with scal‐\\ning and how they are also branching out into new domains and modalities.\\nIf you want to reinforce the concepts and skills that you’ve learned in this book, here\\nare a few ideas for where to go from here:\\nJoin a Hugging Face community event\\nHugging Face hosts short sprints focused on improving the libraries in the eco‐\\nsystem, and these events are a great way to meet the community and get a taste\\nfor open source software development. So far there have been sprints on adding\\n600+ datasets to \\n  Datasets, fine-tuning 300+ ASR models in various languages,\\nand implementing hundreds of projects in JAX/Flax.\\nBuild your own project\\nOne very effective way to test your knowledge in machine learning is to build a\\nproject to solve a problem that interests you. Y ou could reimplement a trans‐\\nformer paper, or apply transformers to a novel domain.\\nContribute a model to \\n  Transformers\\nIf you’re looking for something more advanced, then contributing a newly pub‐\\nlished architecture to \\n  Transformers is a great way to dive into the nuts and\\nbolts of the library. There is a detailed guide to help you get started in the \\nTransformers documentation.\\nBlog about what you’ve learned\\nTeaching others what you’ve learned is a powerful test of your own knowledge,\\nand in a sense this was one of the driving motivations behind us writing this\\nbook! There are great tools to help you get started with technical blogging; we\\nrecommend fastpages as you can easily use Jupyter notebooks for everything.\\n370 | Chapter 11: Future Directions'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 394, 'page_label': '371'}, page_content='Index\\nA\\nabsolute positional representations, 74\\nabstractive QA, 205\\nabstractive summaries, 141\\nAccelerate library\\nabout, 18\\nas part of Hugging Face ecosystem, 15\\nchanges to training loop, 330\\ncomparison with Trainer, 330\\ninfrastructure configuration, 337\\nlaunching training jobs, 337\\nAccelerator\\nis_main_process, 332\\nprepare(), 330\\nprocess_index, 332\\naccuracy metric, 47, 163, 214\\nADAPET method, 288\\nAI Dungeon, 124\\nALBERT model, 81, 174\\nAmazon ASIN, 186\\nAmeisen, Emmanuel, 212\\nanalysis, of pretraining run, 338-343\\nApache Arrow, 24, 307\\nargmax, 102, 127, 177, 178, 240\\nASR (automatic speech recognition), 362\\nattention\\nblock local, 353\\ncausal, 59\\ndilated, 352\\nencoder-decoder, 76\\nglobal, 352\\nlinearized, 353\\nmasked multi-head self-, 76\\nmulti-headed, 67\\nscaled dot-product, 62\\nself-, 6, 351\\nsparse, 352\\n\"Attention Is All Y ou Need\", xii\\nattention head, 67\\nattention mechanisms, 4\\nattention scores, 62\\nattention weights, 61\\nauto classes, 38\\nAutoConfig\\ndefined, 65\\nfrom_pretrained(), 224\\noverriding default values, 101, 224, 325\\nAutoModel\\nabout, 38\\nfrom_pretrained(), 38\\noutput_attentions, 69\\nTensorFlow class, 39\\nAutoModelFor CausalLM\\nfrom_config(), 325\\nfrom_pretrained(), 127, 325\\ngradient_checkpointing, 333\\nAutoModelForMaskedLM, 291\\nAutoModelForQuestionAnswering, 176\\nAutoModelForSeq2SeqLM, 156\\nAutoModelForSequenceClassification\\nabout, 46\\nfrom_pretrained(), 46\\nTensorFlow class, 50\\nautoregressive attention, 59\\nautoregressive language models, 126\\nAutoTokenizer\\nadd_special_tokens, 311\\nas_target_tokenizer(), 159\\n371'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 395, 'page_label': '372'}, page_content='backend_tokenizer.normalizer, 314\\nbackend_tokenizer.pre_tokenizer, 314\\nconvert_ids_to_tokens(), 290\\nconvert_tokens_to_string(), 34\\ndecode(), 105, 127, 175\\nfrom_pretrained(), 33\\nloading from the cache, 33\\npadding, 35, 161\\npush_to_hub(), 322\\nreturn_special_tokens_mask, 290\\nreturn_tensors, 154\\ntruncation, 35\\nvocab_size, 34\\nB\\nback translation, 272\\nbalanced_split() function, 258\\nBALD (Bayesian Active Learning by Disagree‐\\nment), 296\\nband attention, 352\\nBART model, 84, 145\\nbaseline summarization, 143\\nbeam search decoding, 130-134\\nbeams, 130\\nBERT model, 1, 9, 79, 211, 217, 220, 224, 237,\\n260, 263\\nBertViz library, 63\\nbias, 19, 301\\nbidirectional attention, 59\\nBigBird model, 84, 353\\nBigQuery, 306\\nBigScience, 350\\nBLEU score, 148-152\\nbodies (of neural network), 98\\nBoltzmann distribution, 135\\nBookCorpus dataset, 9, 80, 301\\nBPE (Byte-Pair Encoding), 94, 312, 316\\nbyte-level, 314\\nC\\nC4 dataset, 83, 301, 310\\nCamemBERT tokenizer, 310\\ncausal attention, 59\\ncausal language modeling, 126, 323\\nCCMatrix corpus, 284\\ncharacter tokenization, 29\\nChaudhary, Amit, 272\\nclass distribution, 27\\nclassification heads, 75\\nclassifiers, fine-tuning, 47, 293\\nClassLabel\\nabout, 24\\nint2str(), 26, 91\\nnames, 101\\nstr2int(), 214\\nCLINC150 dataset, 213\\nCLIP model, 367\\nclosed-domain QA, 168\\n[CLS] token\\nabout, 34\\nexcluding from tokenizer, 65\\nrole in question answering, 179\\nrole in text classification, 37\\nspecial token ID, 35\\nCNN (convolutional neural network), 355\\nCNN/DailyMail dataset, 141, 154\\nCodeParrot model, 299, 337, 342\\nCodeSearchNet dataset, 304\\nColab notebook, xviii, 20\\nCommon Crawl corpus, 80, 93\\ncommon sense limitation, text and, 355\\ncommunity QA, 166\\ncompile() method, 221\\ncompute() function, 150\\ncompute_accuracy() method, 214, 241\\ncompute_loss() method, 221\\ncompute_metrics() function, 47, 108, 222\\ncompute_size() function, 215, 241\\nconcatenate_datasets() function, 118\\nconditional text generation, 126\\nCoNLL dataset, 92\\nconstant folding, 238\\ncontext, 12\\ncontext manager, 160\\ncontext size, 28, 34, 84, 321\\ncontextualized embeddings, 61\\nconvert_graph_to_onnx.convert() function,\\n239\\nconvert_ids_to_tokens() method, 34\\nconvert_tokens_to_string() method, 34\\ncorpus, 9, 80, 92, 284, 300-303, 310\\ncost, as a challenge of scaling, 349\\ncoverage metrics, 312\\ncross-entropy loss, 104, 219, 295, 347\\ncross-lingual transfer\\nabout, 115\\nfine-tuning multiple languages simultane‐\\nously, 118-120\\n372 | Index'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 396, 'page_label': '373'}, page_content='zero-shot transfer, 116\\nCSV dataset, 25\\nCTRL model, 82\\nCUAD dataset, 169\\ncustom datasets, 25\\ncustom models, 99-102, 101\\ncutoff, 138\\nD\\nDALL-E model, 366\\ndata\\naugmentation of, 271\\navailability of, as a challenge with trans‐\\nformers, 19\\ndomain of, 168\\nswitching formats of, 26\\ndata collators, 36, 107, 160, 289\\ndata parallelism, 330\\nDataFrame, dataset object from, 26, 109, 258\\nDataloader, implementing, 326-329\\nDataset (object)\\nchanging the output format, 26, 29, 40\\ncreating a FAISS index, 277\\nDataFrame converted to, 26, 109, 258\\nfeatures, 23\\nflatten(), 168\\nprocessing data with the map() function, 35,\\n51, 103-105\\nselect(), 90\\nshuffle(), 90\\ndataset cards, 16, 310\\ndatasets\\nadding to Hugging Face Hub, 309\\nadd_faiss_index() function, 277\\nadd_faiss_index_from_external_arrays()\\nfunction, 277\\nBookCorpus, 9, 80, 301\\nbuilding custom code, 303-306\\nC4, 83, 301, 310\\nCLINC150, 213\\nCNN/DailyMail, 141, 154\\nCodeParrot, 299, 337, 342\\nCodeSearchNet, 304\\nCommonCrawl, 80\\nCoNLL, 92\\ncreating with Google BigQuery, 304\\nCSV, 25\\nCUAD, 169\\ncuration of, as a challenge of scaling, 349\\ncustom, 25\\nEmotion, 23\\nfor building review-based QA systems,\\n167-172\\nfor multilingual named entity recognition,\\n88-92\\nGitHub, 252-257\\nGLUE, 23\\nImageNet, 7\\nJSON, 25\\nlarge, 300-310\\nloading in various formats, 25\\nMNLI, 265\\nNQ, 172\\nOSCAR, 310\\nPAN-X, 88\\nquirks of, 51\\nSAMSum, 157\\nSQUAD, 23, 171\\nSubjQA, 167-172\\nSUPERB, 362\\nSuperGLUE, 81, 83\\ntext, 25\\ntokenization of entire, 35\\nVQA, 364\\nWikiANN, 88\\nWikipedia, 80\\nXTREME, 88\\nDatasets library\\nabout, 18\\nas part of Hugging Face ecosystem, 16\\ninspecting all dataset configurations, 88\\ninspecting metadata, 23\\nlisting datasets on the Hub, 23\\nloading datasets from the Hub, 23\\nloading local datasets, 25\\nloading metrics from the Hub, 150, 153, 214\\nloading remote datasets, 25\\nDavison, Joe, 263\\nDDP (DataDistributedParallelism), 336\\nDeBERTa model, 81\\ndecoder, 58, 76\\ndecoder branch, 82\\ndecoder layers, 58\\ndecoder-only model, 59\\ndecoding\\nbeam search decoding, 130-134\\ngreedy search, 127-130\\ndecoding method, 125, 140\\nIndex | 373'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 397, 'page_label': '374'}, page_content='deep neural networks, 244\\ndeepset, xxi, 182, 187\\ndeployment, as a challenge of scaling, 350\\ndialogue (conversation), 141, 157\\ndialogue summaries, generating, 162\\ndilated attention, 352\\ndiscriminator, 81\\nDistilBERT model, 22, 28, 33, 36, 80\\ndocument length, as a challenge with trans‐\\nformers, 19\\ndocument store\\ncompatibility with Haystack retrievers, 182\\ndefined, 182\\ninitializing with Elasticsearch, 183\\nloading documents with, 185\\nloading labels with, 191\\ndomain\\nadaptation, 8, 199-203, 289\\nof data, 168\\ndomain adaptation, 8, 199-203, 289\\ndot product, 62-67, 77, 353, 367\\ndownsample, 90, 116\\nDPR (Dense Passage Retrieval), 194\\ndynamic quantization, 235\\nE\\nefficiency, 209-247\\nabout, 209\\nbenchmarking quantized models, 236\\ncreating performance benchmarks, 212-217\\nintent detection, 210\\nknowledge distillation, 217-230\\noptimizing inference with ONNX/ONNX\\nRuntime, 237-243\\nquantization, 230-236\\nweight pruning, 243-247\\nElasticsearch, 183, 186\\nElasticsearchRetriever.eval() method, 190\\nELECTRA model, 81\\nEleutherAI, 83, 350\\nELMO model, 8, 61\\nEM (Exact Match) score, 196\\nembeddings\\ncontextualized, 61\\ndense, 65\\ndistilBERT, 37\\npositional, 73\\ntoken, 57, 58\\nusing as a lookup table, 275-282\\nword, xii, 8\\nEmotion dataset, 23\\nencoder\\nabout, 60\\nadding classification heads, 75\\nadding layer normalization, 71\\ndefined, 57\\nfeed-forward layer, 70\\npositional embeddings, 73\\nself-attention, 61-70\\nencoder branch, 79-82\\nencoder layers, 58\\nencoder-decoder attention, 76\\nencoder-decoder branch, 83\\nencoder-decoder model, 2, 59\\nencoder-only model, 59\\nend-to-end, 37, 45, 181, 189, 193, 205\\nEnglish Wikipedia dataset, 80\\nEOS (end-of-sequence) token, 58\\nerror analysis, 50-53, 108-115\\nexponent, 231\\nextracting\\nanswers from text, 173-181\\nlast hidden states, 39\\nextractive QA, 13, 166\\nextractive summaries, 141\\nF\\nF .log_softmax() function, 221\\nF1-score(s), 48, 105, 120, 150, 196, 260, 285\\nfacts limitation, text and, 355\\nFAISS\\ndocument store, 183, 196\\nefficient similarity search with, 282\\nindex, adding to a Dataset object, 277\\nlibrary, 196, 282\\nfamily tree, of transformers, 78\\nFARM library\\nreader for question answering, 187\\ntraining models with, 199-203\\nFARMReader\\nabout, 187\\ncomparison with the pipeline() function,\\n187\\nloading a model with, 187\\npredict_on_texts(), 188\\ntrain(), 199, 202\\nFast Forward QA series, 207\\nfastdoc library, xxi\\n374 | Index'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 398, 'page_label': '375'}, page_content='fastpages, 370\\nfeature extractors, 38-45, 368\\nfeature matrix, creating, 41\\nfeed-forward layer, 70\\nfew-shot learning, 288\\nFF NNs (feed-forward neural networks), 6\\nfiltering noise, 306\\nfine-tuning\\nas a step in ULMFiT process, 8\\nclassifiers, 47, 293\\nknowledge distillation for, 217\\nlanguage models, 289-292\\nmultiple languages simultaneously, 118-120\\nPEGASUS, 158-162\\ntransformers, 45-54\\nvanilla transformers, 284\\nwith Keras, 50\\nXLM-RoBERTa, 106-115\\nfit() method, 50\\nfixed-point numbers, 231\\nflatten() method, 168\\nfloating-point numbers, 231\\nforward() function, 99, 100\\nframeworks, interoperability between, 39\\nfrom_config() method, 325\\nfrom_pandas() method, 258\\nfrom_pretrained() method, 33, 38, 101, 224,\\n325\\nfused operation, 238\\nG\\ngenerate() function, 127, 133, 135, 138, 156\\ngenerative QA, 205\\ngenerative tasks, 366\\nGeron, Aurelien, xvi\\ngetsizeof() function, 235\\nget_all_labels_aggregated() method, 192\\nget_dataset_config_names() function, 88, 168\\nget_dummies() function, 30\\nget_nearest_examples() function, 277\\nget_nearest_examples_batch() function, 278\\nget_preds() function, 267\\nGitHub\\nbuilding an Issues Tagger, 251-259\\nLicense API, 304\\nrepository, 252, 300\\nwebsite, 251\\nGitHub Copilot, 299, 303\\nGitHub REST API, 252, 304\\nglobal attention, 352\\nGLUE dataset, 23, 79\\nGoogle Colaboratory (Colab), xviii, 20\\nGoogle searches, 166\\nGoogle\\'s Meena, 124\\nGPT model, 1, 9, 82, 302\\nGPT-2 model, 82, 123, 129, 144, 146, 276, 302,\\n313, 321, 330\\nGPT-3 model, 83, 276, 346\\nGPT-J model, 83, 350\\nGPT-Neo model, 83, 350\\ngradient accumulation, 161, 335\\ngradient checkpointing, 335\\ngreedy search encoding, 127-130\\nGrid Dynamics, 207\\nground truth, 132, 147, 160, 190, 196, 214, 217,\\n223, 240\\nGugger, Sylvain, xvi\\nH\\nhardware requirements, xviii\\nhash symbols (#), 12\\nHaystack library\\nbuilding QA pipelines using, 181-189\\nevaluating reader, 196\\nevaluating retriever, 189-196\\nevaluating whole pipeline, 203\\ninitializing document store, 183\\ninitializing pipeline, 188\\ninitializing reader, 187\\ninitializing retriever, 185\\nretriever-reader architecture, 181\\ntutorial, 196, 208\\nwebsite, 182\\nheads (of neural network), 98\\nhead_view() function, 69\\nhidden state, 2, 37\\nHinton, Geoff, 217\\n\"How We Scaled Bert to Serve 1+ Billion Daily\\nRequests on CPUs\", 209\\nHoward, Jeremy, xvi\\nthe Hub (see Hugging Face Hub)\\nHugging Face\\nAccelerate library, 18\\ncommunity events, 370\\nDatasets library, 18\\necosystem, 15\\nTokenizers library, 17\\nHugging Face Datasets, 23\\nIndex | 375'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 399, 'page_label': '376'}, page_content='Hugging Face Hub\\nabout, 16\\nadding datasets to, 309\\nchoosing question answering models on,\\n168\\nlisting datasets on, 23\\nlogging into, 47\\nsaving custom tokenizers on, 322\\nsaving models on, 53\\nwidgets, 121\\nHugging Face Transformers, release of, 9\\n(see also transformers)\\nThe Hugging Face Course, xvi\\nhuman reporting bias limitation, text and, 355\\nhyperparameters, finding with Optuna, 226\\nhyperparameter_search() method, 228\\nI\\niGPT model, 355\\nImageNet dataset, 7\\nImbalanced-learn library, 28\\nIMDb, 8\\nin-context learning, 288\\nInference API, 54, 350\\ninference widget, 16\\nInferKit, 124\\ninformation bottleneck, 4\\ninfrastructure, as a challenge of scaling, 349\\ninitializing\\ndocument store, 183\\nmodels, 325\\nreaders, 187\\nretriever, 185\\ninit_weights() method, 100\\nint2str() method, 26\\nintent detection, 210\\nintermediate representation, 237\\ninteroperability, between frameworks, 39\\nISO 639-1 language code, 89\\nIssues tab, 251\\nIssues Tagger, building, 251-259\\niter() function, 328\\niterative_train_test_split() function, 258, 259\\nJ\\nJAX library, 10\\nJira, 251\\nJSON dataset, 25\\nJupyter Notebook, 47, 300, 363\\nK\\nKaggle Notebooks, xviii\\nKarpathy, Andrej, 3, 27, 77\\nKeras library, 50, 221\\nkernel function, 354\\nkey, 62\\nkey/value pair, 215\\nKite, 299\\nKL (Kullback-Leibler) divergence, 218\\nknowledge distillation\\nabout, 217\\nbenchmarking the model, 229\\nchoosing student initialization, 222-226\\ncreating a trainer, 220\\nfinding hyperparameters with Optuna, 226\\nfor fine-tuning, 217\\nfor pretraining, 220\\nL\\nlabels, 249-296\\nabout, 249\\nbuilding GitHub Issues tagger, 251-259\\nincorrect, 51\\nleveraging unlabeled data, 289-296\\nworking with a few, 271-289\\nworking with no labeled data, 263-271\\nlanguage models, fine-tuning, 289-292\\nlanguage, as a challenge with transformers, 19\\nlast hidden state, 3, 39\\nlatency, as a performance benchmark, 212\\nlayer normalization, 71\\nLayoutLM model, 365\\nLCS (longest common substring), 153\\nlearning rate warm-up, 71\\nLibraries.io, 304\\nlinearized attention, 353\\nlist_datasets() function, 23\\nloading\\ncustom datasets, 25\\ncustom models, 101\\npretrained models, 46\\ntokenizer, 33\\nload_dataset() function\\ndownload configuration, 306\\nloading a single configuration, 88, 168\\nloading a specific version, 141\\nstreaming, 308\\nlog probability, 131\\n376 | Index'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 400, 'page_label': '377'}, page_content='logits, 75, 102, 125, 127, 131, 134, 176-178, 187,\\n218, 221, 240, 286\\nlong-form QA, 166\\nLongformer model, 353\\nlookup table, using embeddings as a, 275-282\\nLSTM (long-short term memory) networks, 1\\nLucene, 186\\nLXMERT model, 365\\nM\\nM2M100 model, 84, 272, 284\\nMAD-X library, 121\\nmagnitude pruning, 245\\nmantissa, 231\\nmAP (mean average precision), 190\\nmap() method, 35, 40, 51, 103, 260, 267, 273\\nmask matrix, 76, 244\\nmasked multi-head self-attention, 76\\nmatrices, 66, 232, 244\\nmaximum content size, 28\\nmean pooling, 276\\nMeena (Google), 124\\nmemory mapping, 18, 306\\nmemory, as a performance benchmark, 212\\nmetrics\\nAccuracy, 47, 163, 214\\nadd() function, 150\\nadd_batch() function, 150\\nBLEU, 148-152\\ncompute(), 150\\nExact Match, 196\\nF1-score, 48, 105, 120, 150, 197, 260, 285\\nlog probability, 130\\nmean average precision, 189\\nPerplexity, 333\\nPrecision, 105, 148\\nRecall, 105, 150, 152, 189\\nROUGE, 152\\nSacreBLEU, 150\\nminGPT model, 77\\nMiniLM model, 174\\nMLM (masked language modeling), 9, 80, 324\\nMNLI dataset, 265\\nmodality limitation, text and, 355\\nmodel cards, 16\\nthe Model Hub, xii\\nmodel weights, 16\\nmodel widgets, interacting with, 121\\nmodels\\nALBERT, 81, 174\\nBART, 84, 145\\nBERT, 1, 9, 79, 211, 217, 220, 224, 237, 260,\\n263\\nBigBird, 84, 353\\nCamemBERT, 310\\nCLIP, 367\\nCodeParrot, 299, 337, 342\\nCTRL, 82\\nDALL-E, 366\\nDeBERTa, 81\\nDistilBERT, 22, 28, 33, 36, 80\\nDPR, 194\\nELECTRA, 81\\nELMO, 8, 61\\nevaluation of, as a challenge of scaling, 349\\nGPT, 1, 9, 82, 302\\nGPT-2, 82, 276, 302, 313, 321, 330\\nGPT-3, 83, 276, 346\\nGPT-J, 83, 350\\nGPT-Neo, 83, 350\\niGPT, 355\\ninitializing, 325\\nLayoutLM, 365\\nLongformer, 353\\nLSTM, 1\\nLXMERT, 365\\nM2M100, 84, 272, 284\\nMeena, 124\\nminGPT, 77\\nminiLM, 174\\nNaive Bayes, 260-263\\nPEGASUS, 145, 154, 158, 158-162\\nperformance of, as a performance bench‐\\nmark, 212\\nRAG, 205\\nReformer, 353\\nResNet, 6, 365\\nRNN, 2\\nRoBERTa, 80, 174\\nsaving, 53\\nsharing, 53\\nT5, 83, 144, 310\\nTAPAS, 166, 359\\ntraining, 47\\ntypes of, 221\\nULMFiT, 1, 8\\nVisualBERT, 365\\nViT, 356\\nIndex | 377'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 401, 'page_label': '378'}, page_content='Wav2Vec2, 362\\nXLM, 80\\nXLM-RoBERTa, 39, 80, 93, 106-115, 174\\nmodel_init() method, 107\\nmovement pruning, 246\\nmulti-headed attention, 67\\nmultilabel text classification problem, 251\\nmultilingual named entity recognition, 87-121\\nabout, 87\\nanatomy of Transformers Model class, 98\\nbodies, 98\\ncreating custom models for token classifica‐\\ntion, 99-102\\ncross-lingual transfer, 115-120\\ndataset, 88-92\\nerror analysis, 108-115\\nfine-tuning on multiple languages simulta‐\\nneously, 118-120\\nfine-tuning XLM-RoBERTa, 106-115\\nheads, 98\\ninteracting with model widgets, 121\\nloading custom models, 101-102\\nmultilingual transformers, 92\\nperformance measures, 105\\nSentencePiece tokenizer, 95\\ntokenization, 93-96\\ntokenizer pipeline, 94\\ntokenizing texts for NER, 103-105\\ntransformers for, 96\\nXLM-RoBERTa, 93\\nzero-shot transfer, 116\\nmultilingual transformers, 92\\nmultimodal transformers, 361-364\\nN\\nn-gram penalty, 133\\nn-grams, 152\\nNaive baseline, implementing, 260-263\\nNaive Bayes classifier, 260\\nnamed entities, 11\\nNER (named entity recognition)\\naligning predictions, 105\\nas a transformer application, 11\\n(see also multilingual named entity rec‐\\nognition)\\ntask, 92, 108, 115\\ntokenizing texts for, 103-105\\ntransformers for, 96\\nneural network architecture, xii, 1, 4\\nNeural Networks Block Movement Pruning\\nlibrary, 247\\nnext token probability, 133\\nNLI (natural language inference), 265-271\\nNLP (natural language processing), transfer\\nlearning in, 6-10\\nNlpAug library, 272\\nNLU (natural language understanding), 79\\nnoise, filtering, 306\\nnonlocal keyword, 321\\nnormalization, 71, 94\\nnotebook_login() function, 309\\nNQ dataset, 172\\nNSP (next sentence prediction), 80\\nnucleus sampling, 136-139\\nnumericalization, 29\\nO\\nobjective() function, 227\\noffset tracking, 314\\none-hot encoding, 30, 65\\none-hot vectors, 30, 37\\none_hot() function, 30\\nONNX-ML, 237\\nONNX/ONNX Runtime, optimizing inference\\nwith, 237-243\\nopacity, as a challenge with transformers, 19\\nopen source, 251, 304, 312, 350, 370\\nopen-domain QA, 168\\nOpenAI, 8, 82, 123, 129, 276, 350\\nOpenMP, 239\\noperator sets, 240\\n\"Optimal Brain Surgeon\" paper, 245\\nOptuna, finding hyperparameters with, 226\\nORT (ONNX Runtime), 242\\nOSCAR corpus, 310\\nout-of-scope queries, 210\\nP\\nPAN-X dataset, 88, 114\\npandas.Series.explode() function, 110\\nPaperspace Gradient Notebooks, xviii\\npartial hypotheses, 130\\nPath.stat() function, 215\\nPEGASUS model\\nabout, 145\\nevaluating on CNN/DailyMail dataset, 154\\nevaluating on SAMSum, 158\\nfine-tuning, 158-162\\n378 | Index'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 402, 'page_label': '379'}, page_content='performance\\ncreating benchmarks, 212-217\\ndefining metrics, 47\\nmeasures of, 105\\nrelationship with scale, 347\\nperf_counter() function, 215\\npermutation equivariant, 72\\npip, xii\\npipeline\\nbuilding using Haystack, 181-189\\ntokenizer, 94, 312\\nTransformers library, 10\\npipeline() function\\naggregation_strategy, 10\\ndefined, 10\\nnamed entity recognition, 10\\nquestion answering, 12\\nsummarization, 13\\ntext classification, 11\\ntext generation, 14\\ntranslation, 13\\nusing a model from the Hub, 13\\nplot_metrics() function, 229\\npooling, 276\\nPopen() function, 183\\nposition-wise feed-forward layer, 70\\npositional embeddings, 58, 73\\npost layer normalization, 71\\npostprocessing, 95\\npre layer normalization, 71\\npredict() method, 48, 115\\nprepare() function, 330\\npretokenization, 94\\npretrained models, 38, 46\\npretraining\\nabout, 7\\nas a step in ULMFiT process, 8\\nknowledge distillation for, 220\\nobjectives for, 323\\nprompts, 288\\nproportion, of continued words, 312\\npseudo-labels, 296\\npush_to_hub() method, 53, 322\\nPython, tokenizer for, 313-318\\nPyTorch library\\nabout, 10\\nclasses and methods, 64\\nhub, 17\\ninteroperability with, 39\\ntril() function, 76\\nwebsite, xvi\\nQ\\nQA (question answering), 165-207\\nabout, 165\\nabstractive, 205\\nas a transformer application, 12\\nbuilding pipeline using Haystack, 181-189\\nbuilding review-based systems, 166-189\\nclosed-domain, 168\\ncommunity, 166\\ndataset, 167-172\\ndomain adaptation, 199-203\\nevaluating reader, 196-199\\nevaluating retriever, 189-196\\nevaluating whole pipeline, 203\\nextracting answers from text, 173-181\\nextractive, 13, 166\\ngenerative, 205\\nimproving pipeline, 189-199\\nlong passages in, 179\\nlong-form, 166\\nopen-domain, 168\\nRAG (retrieval-augmented generation), 205\\nspan classification task, 173\\nSQuAD dataset, 171\\nTable QA, 359\\ntokenizing text for, 175-178\\nquality, of generated text, 148-154\\nquantization\\nabout, 230-235\\nbenchmarking models, 236\\ndynamic, 235\\nquantization-aware training, 236\\nstatic, 235\\nstrategies for, 235\\nquantize_dynamic() function, 236, 242\\nquantize_per_tensor() function, 233\\nquery, 62\\nquestion-answer pair, 166, 191, 197, 199\\nquestion-context pair, 179\\nR\\nradix point, 231\\nRAG (retrieval-augmented generation), 205\\nRAG-Sequence models, 205\\nRAG-Token models, 206\\nrandom attention, 352\\nIndex | 379'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 403, 'page_label': '380'}, page_content='readers\\nas a component of retriever-reader architec‐\\nture, 181\\nevaluating, 196-199\\ninitializing, 187\\nreading comprehension models, 166\\nREADME cards, 310\\nrecall, 189\\nrecv keyword, 320\\nReformer model, 353\\nrelative positional representations, 74\\nResNet model, 6, 365\\nretrieve() method, 186\\nretriever\\nas a component of retriever-reader architec‐\\nture, 181\\nevaluating, 189-196\\ninitializing, 185\\nretriever-reader architecture, 181\\nreview-based QA systems, building, 166-189\\nRNNs (recurrent neural networks), 2\\nRoBERTa model, 80, 174\\nROUGE score, 152\\nrun() method, 188, 190\\nrun_benchmark() method, 213\\nRust programming language, 17, 314\\nS\\nSacreBLEU, 150\\nsample efficiency, 348\\nsample() method, 169\\nsampling methods, 134-139\\nSAMSum dataset, 157\\nSamsung, 157\\nsaving\\ncustom tokenizers on Hugging Face Hub,\\n322\\nmodels, 53\\nscaled dot-product attention, 62-67\\nscaling laws, 347\\nscaling transformers\\nabout, 345\\nchallenges with, 349\\nlinearized attention, 353\\nscaling laws, 347\\nself-attention mechanisms, 351\\nsparse attention, 352\\nScikit-learn format, 41\\nScikit-multilearn library, 257\\nselect() method, 90\\nself-attention, 6, 351\\nself-attention layer\\nabout, 61\\nmulti-headed attention, 67-70\\nscaled dot-product attention, 62-67\\nSentencePiece tokenizer, 93, 95\\nsentiment analysis, 10\\nsent_tokenize() function, 146\\n[SEP] token, 34, 35, 65, 70, 94, 95, 176, 180, 290\\nseq2seq (sequence-to-sequence), 3, 324\\nseqeval library, 105\\nSequence class, 90\\nsetup_logging() method, 331\\nset_format() method, 26\\nsharing models, 53\\nshuffle() method, 90, 308\\nsign, 231\\nsignificand, 231\\nsilver standard, 114\\nsimilarity function, 62\\nskip connections, 71\\nsmooth power laws, 347\\nsoftmax, 62, 66, 77, 81, 125, 127, 134, 178, 187,\\n218, 221\\nsoftware requirements, xviii\\nSoundFile library, 363\\nspan classification task, 173\\nsparse attention, scaling and, 352\\nspeech-to-text, 361-364\\nspeedup, 284\\nsplit() function, 31\\nSQuAD (Stanford Question Answering Data‐\\nset), 23, 171, 198, 202\\nStack Overflow, 166, 212\\nstate of the art, 209\\nstatic quantization, 235\\nstr2int() method, 214\\nstreaming datasets, 308\\nsubjectivity, 167\\nSubjQA dataset, 167-172\\nsublayer, 60, 70, 76\\nsubword fertility, 312\\nsubword tokenization, 33\\nsummarization, 141-163\\nabout, 141\\nabstractive summaries, 141\\nas a transformer application, 13\\nbaseline, 143\\n380 | Index'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 404, 'page_label': '381'}, page_content='CNN/DailyMail dataset, 141\\ncomparing summaries, 146\\nevaluating PEGASUS on CNN/DailyMail\\ndataset, 154\\nextractive summaries, 141\\ngenerating dialogue summaries, 162\\nmeasuring quality of generated text,\\n148-154\\ntext summarization pipelines, 143-146\\ntraining models for, 157-163\\nSUPERB dataset, 362\\nSuperGLUE dataset, 81, 83\\nSutton, Richard, 345\\nT\\nT5 model, 83, 144, 310\\nTable QA, 359\\nTabNine, 299\\nTAPAS model, 166, 359\\ntask agnostic distillation, 223\\nTensor.masked_fill() function, 76\\nTensor.storage() function, 235\\nTensorBoard, 331\\nTensorFlow\\nabout, 10\\nclasses and methods, 64\\nfine-tuning models using Keras API, 50\\nhub, 17\\nwebsite, xvi\\ntensors\\nbatch matrix-matrix product of, 66\\nconverting to TensorFlow, 50\\ncreating one-hot encodings, 30\\nfilling elements with a mask, 76\\ninteger representation of, 233\\nquantization, 231\\nreturning in tokenizer, 39\\nstorage size, 235\\ntest_step() method, 221\\ntext\\nextracting answers from, 173-181\\ngoing beyond, 354-370\\ntokenizing for QA, 175-178\\nvision and, 364-370\\ntext classification, 21-55\\nabout, 21\\nas a transformer application, 10\\ncharacter tokenization, 29\\nclass distribution, 27\\nDataFrames, 26\\ndatasets, 22-25\\nDatasets library, 22-25\\nfine-tuning transformers, 45-54\\nlength of tweets, 28\\nsubword tokenization, 33\\ntokenizing whole datasets, 35\\ntraining text classifiers, 36-45\\ntransformers as feature extractors, 38-45\\nword tokenization, 31\\ntext dataset, 25\\ntext entailment, 265\\ntext generation, 123-140\\nabout, 123\\nas a transformer application, 14\\nbeam search decoding, 130-134\\nchallenges with, 125\\nchoosing decoding methods, 140\\ngreedy search encoding, 127\\nsampling methods, 134-139\\ntext summarization pipelines, 143-146\\nTextAttack library, 272\\nTF-IDF (Term Frequency-Inverse Document\\nFrequency) algorithm, 185\\nTimeSformer model, 358\\ntimestep, 4, 61, 76, 127, 130, 134\\ntime_pipeline() function, 215\\nTLM (translation language modeling), 80\\ntoken classification, creating custom models\\nfor, 99-102\\ntoken embeddings, 58, 61\\ntoken perturbations, 272\\ntokenization\\nabout, 29, 93\\ncharacter, 29\\nof entire dataset, 35\\nsubword, 33\\ntext for QA, 175-178\\ntexts for NER, 103-105\\nword, 31\\ntokenizer model, 95, 312\\ntokenizer pipeline, 94\\ntokenizers\\nabout, 12\\nbuilding, 310-321\\nfor Python, 313-318\\nmeasuring performance, 312\\nsaving on Hugging Face Hub, 322\\ntraining, 318-321\\nIndex | 381'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 405, 'page_label': '382'}, page_content='Tokenizers library\\nabout, 17\\nas part of Hugging Face ecosystem, 15\\nauto class, 38\\nloading tokenizers from the Hub, 38\\ntokenizing text, 38\\ntop-k sampling, 136-139\\ntop-p sampling, 136-139\\ntorch.bmm() function, 66\\ntorch.save() function, 214, 241\\ntorch.tril() function, 76\\nto_tf_dataset() method, 50\\ntrain() method, 199, 223\\nTrainer\\nabout, xviii\\ncomputing custom loss, 221\\ncreating a custom Trainer, 221\\ndefining metrics, 47, 107, 223, 286\\nfine-tuning models, 48\\ngenerating predictions, 48\\nhyperparameter_search(), 228\\nknowledge distillation, 220\\nlogging history, 291\\nmodel_init(), 107\\npush_to_hub(), 53\\nusing a data collator, 107, 160, 290\\ntraining\\nmodels, 47\\nsummarization models, 157-163\\ntext classifiers, 36\\ntokenizers, 318-321\\ntraining loop, defining, 330-337\\ntraining run, 337\\ntraining sets, 42, 257\\ntraining slices, creating, 259\\ntraining transformers from scratch, 299-343\\nabout, 299\\nadding datasets to Hugging Face Hub, 309\\nbuilding custom code datasets, 303-306\\nbuilding tokenizers, 310-321\\nchallenges of building large-scale corpus,\\n300-303\\ndefining training loop, 330-337\\nimplementing Dataloader, 326-329\\ninitializing models, 325\\nlarge datasets, 300-310\\nmeasuring tokenizer performance, 312\\npretraining objectives, 323\\nresults and analysis, 338-343\\nsaving custom tokenizers on Hugging Face\\nHub, 322\\ntokenizer for Python, 313-318\\ntokenizer model, 312\\ntraining run, 337\\ntraining tokenizers, 318-321\\nTrainingArguments\\nabout, 47\\ncreating a custom TrainingArguments, 220\\ngradient accumulation, 161\\nlabel_names, 222\\nsave_steps, 106\\ntrain_new_from_iterator() method, 318\\ntrain_on_subset() function, 119\\ntrain_step() method, 221\\nTransCoder model, 304\\ntransfer learning\\ncomparison with supervised learning, 6\\nin computer vision, 6\\nin NLP, 6-10\\nweight pruning and, 243\\ntransformer applications\\nabout, 10\\nnamed entity recognition, 11\\nquestion answering, 12\\nsummarization, 13\\ntext classification, 10\\ntext generation, 14\\ntranslation, 13\\nTransformer architecture, 57-84\\nabout, 1, 57-59\\nadding classification heads, 75\\nadding layer normalization, 71\\ndecoder, 76\\ndecoder branch, 82\\nencoder, 60-75\\nencoder branch, 79-82\\nencoder-decoder attention, 76\\nencoder-decoder branch, 83\\nfamily tree, 78\\nfeed-forward layer, 70\\npositional embeddings, 73\\nself-attention, 61-70\\ntransformers\\nabout, xii\\nas feature extractors, 38-45\\nBERT, 9\\nefficiency of, 209-247\\nfine-tuned on SQuAD, 173\\n382 | Index'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 406, 'page_label': '383'}, page_content='fine-tuning, 45-54\\nfor named entity recognition, 96\\nGPT, 9\\nmain challenges with, 19\\nmultilingual, 92\\nscaling (see scaling transformers)\\ntraining (see training transformers)\\nTransformers library\\nabout, 9\\nas part of the Hugging Face ecosystem, 15\\nauto classes, 33\\nfine-tuning models with, 45-50\\nloading models from the Hub, 38\\nloading tokenizers from the Hub, 33\\npipelines, 10-15\\nsaving models on the Hub, 53\\nTransformersReader, 187\\ntranslation, as a transformer application, 13\\nU\\nUDA (Unsupervised Data Augmentation), 250,\\n295\\nULMFiT (Universal Language Model Fine-\\nTuning), 1, 8\\nUMAP algorithm, 42\\nUnicode normalization, 94, 314\\nUnigram, 312\\nunlabeled data, leveraging, 289-296\\nupscale, 82\\nUST (Uncertainty-Aware Self-Training), 250,\\n295\\nV\\nvalue, 62\\nvanilla transformers, fine-tuning, 284\\nvision, 355-358, 364-370\\nVisualBERT model, 365\\nvisualizing training sets, 42\\nViT model, 356\\nVQA dataset, 364\\nW\\nWav2Vec2 models, 362\\nwebtext, 87, 303, 349\\nweight pruning\\nabout, 243\\nmethods for, 244-247\\nsparsity in deep neural networks, 244\\nweighted average, 61\\nWeights & Biases, 331\\nWikiANN dataset, 88\\nword tokenization, 31\\nWordPiece, 33, 93, 312\\nword_ids() function, 103\\nWrite With Transformer, 124\\nwrite_documents() method, 185\\nX\\nXLM model, 80\\nXLM-RoBERTa model, 39, 80, 93, 106-115, 174\\nXTREME benchmark, 88\\nZ\\nzero point, 231\\nzero-shot classification, 263-271\\nzero-shot cross-lingual transfer, 87\\nzero-shot learning, 88\\nzero-shot transfer, 88, 116\\nIndex | 383'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 407, 'page_label': '384'}, page_content='About the Authors\\nLewis Tunstall is a machine learning engineer at Hugging Face. He has built machine\\nlearning applications for startups and enterprises in the domains of NLP , topological\\ndata analysis, and time series. Lewis has a PhD in theoretical physics and has held\\nresearch positions in Australia, the USA, and Switzerland. His current work focuses\\non developing tools for the NLP community and teaching people to use them\\neffectively.\\nLeandro von Werra is a machine learning engineer in the open source team at Hug‐\\nging Face. He has several years of industry experience bringing NLP projects to pro‐\\nduction by working across the whole machine learning stack, and is the creator of a\\npopular Python library called TRL, which combines transformers with reinforcement\\nlearning.\\nThomas Wolf is chief science officer at and cofounder of Hugging Face. His team is\\non a mission to catalyze and democratize NLP research. Prior to cofounding Hugging\\nFace, Thomas earned a PhD in physics and later a law degree. He has worked as a\\nphysics researcher and a European patent attorney.\\nColophon\\nThe bird on the cover of Natural Language Processing with Transformers is a coconut\\nlorikeet ( Trichoglossus haematodus), a relative of parakeets and parrots. It is also\\nknown as the green-naped lorikeet and is native to Oceania.\\nThe plumage of coconut lorikeets blends into their colorful tropical and subtropical\\nsurroundings; their green nape meets a yellow collar beneath a deep dark blue head,\\nwhich ends in an orange-red bill. Their eyes are orange and the breast feathers are\\nred. Coconut lorikeets have one of the longest, pointed tails of the seven species of\\nlorikeet, which is green from above and yellow underneath. These birds measure 10\\nto 12 inches long and weigh 3.8 to 4.8 ounces.\\nCoconut lorikeets have one monogamous partner and lay two matte white eggs at a\\ntime. They build nests over 80 feet high in eucalyptus trees and live 15 to 20 years in\\nthe wild. This species suffers from habitat loss and capture for the pet trade. Many of\\nthe animals on O’Reilly’s covers are endangered; all of them are important to the\\nworld.\\nThe cover illustration is by Karen Montgomery, based on a black and white engraving\\nfrom English Cyclopedia. The cover fonts are Gilroy Semibold and Guardian Sans.\\nThe text font is Adobe Minion Pro; the heading font is Adobe Myriad Condensed;\\nand the code font is Dalton Maag’s Ubuntu Mono.'), Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2022-05-23T19:13:29+00:00', 'author': 'Lewis Tunstall;Leandro von Werra;Thomas Wolf;', 'moddate': '2022-05-27T05:59:18-07:00', 'title': 'Natural Language Processing with Transformers, Revised Edition', 'trapped': '/False', 'ebx_publisher': \"O'Reilly Media\", 'source': 'data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf', 'total_pages': 409, 'page': 408, 'page_label': '385'}, page_content='Learn from experts.  \\nBecome one yourself.\\nBooks | Live online courses   \\nInstant Answers | Virtual events \\nVideos | Interactive learning\\nGet started at oreilly.com. \\n©2022 O’Reilly Media, Inc. O’Reilly is a registered trademark of O’Reilly Media, Inc. | 175')]\n",
      "409\n"
     ]
    }
   ],
   "source": [
    "# Reading a PDF File\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "pdf_loader = PyPDFLoader(\"data/dokumen.pub_natural-language-processing-with-transformers-revised-edition-1098136799-9781098136796-9781098103248.pdf\")\n",
    "pdf_documents = pdf_loader.load()\n",
    "print(pdf_documents)\n",
    "print(len(pdf_documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': 'https://artificialanalysis.ai', 'title': 'AI Model & API Providers Analysis | Artificial Analysis', 'description': 'Comparison and analysis of AI models and API hosting providers. Independent benchmarks across key performance metrics including quality, price, output speed & latency.', 'language': 'en'}, page_content=\"AI Model & API Providers Analysis | Artificial AnalysisFollow us on Twitter or LinkedIn to stay up to date with future analysisMenuArtificial AnalysisLanguage ModelsSpeech, Image & Video ModelsLeaderboards🏆 ArenasAboutEmail addressSubscribeHOMELANGUAGE MODELSSPEECH, IMAGE & VIDEO MODELSLeaderboards🏆 ArenasAboutEmail addressSubscribeIndependent analysis of AI models and API providersUnderstand the AI landscape to choose the best model and provider for your use case2025 State of AI SurveyParticipate to receive the full survey report and win a pair of Ray-Ban Meta AI Glasses 🕶️ParticipateState of AI: China Report HighlightsIntelligenceArtificial Analysis Intelligence Index; Higher is betterSpeedOutput Tokens per Second; Higher is betterPriceUSD per 1M Tokens; Lower is betterHow do DeepSeek models compare?DeepSeek ComparedWhere can you get an API for DeepSeek R1?DeepSeek R1 ProvidersWhich models perform best in different languages?Multilingual ComparisonWho has the best Video Generation model?Video ArenaWhich model is fastest with 100k token prompts?Long Context LatencySee more information on any of our supported modelsModel NameCreatorLicenseContext WindowFurther analysiso1OpenAIProprietary200k  Model API Providerso3-miniOpenAIProprietary200k  Model API Providerso1-previewOpenAIProprietary128k  Model API Providerso1-miniOpenAIProprietary128k  Model API ProvidersGPT-4o (Aug '24)OpenAIProprietary128k  Model API ProvidersGPT-4o (May '24)OpenAIProprietary128k  Model API ProvidersGPT-4o (Nov '24)OpenAIProprietary128k  Model API ProvidersGPT-4o miniOpenAIProprietary128k  Model API ProvidersGPT-4o mini Realtime (Dec '24)OpenAIProprietary128k  Model API ProvidersGPT-4o (March 2025, chatgpt-4o-latest)OpenAIProprietary128k  Model API Providerso3-mini (high)OpenAIProprietary200k  Model API ProvidersGPT-4.5 (Preview)OpenAIProprietary128k  Model API ProvidersGPT-4o Realtime (Dec '24)OpenAIProprietary128k  Model API Providerso1-proOpenAIProprietary200k  Model API Providerso3OpenAIProprietary128k  Model API ProvidersGPT-4 TurboOpenAIProprietary128k  Model API ProvidersGPT-4OpenAIProprietary8k  Model API ProvidersGPT-4o (ChatGPT)OpenAIProprietary128k  Model API ProvidersLlama 3.3 Instruct 70BMetaOpen128k  Model API ProvidersLlama 3.1 Instruct 405BMetaOpen128k  Model API ProvidersLlama 3.2 Instruct 90B (Vision)MetaOpen128k  Model API ProvidersLlama 3.1 Instruct 70BMetaOpen128k  Model API ProvidersLlama 3.2 Instruct 11B (Vision)MetaOpen128k  Model API ProvidersLlama 3.1 Instruct 8BMetaOpen128k  Model API ProvidersLlama 3.2 Instruct 3BMetaOpen128k  Model API ProvidersLlama 3.2 Instruct 1BMetaOpen128k  Model API ProvidersLlama 4 MaverickMetaOpen1m  Model API ProvidersLlama 4 ScoutMetaOpen10m  Model API ProvidersLlama 3 Instruct 70BMetaOpen8k  Model API ProvidersLlama 3 Instruct 8BMetaOpen8k  Model API ProvidersLlama 2 Chat 13BMetaOpen4k  Model API ProvidersLlama 2 Chat 70BMetaOpen4k  Model API ProvidersLlama 2 Chat 7BMetaOpen4k  Model API ProvidersGemini 2.0 Pro Experimental (Feb '25)GoogleProprietary2m  Model API ProvidersGemini 2.0 Flash (Feb '25)GoogleProprietary1m  Model API ProvidersGemini 2.0 Flash (experimental)GoogleProprietary1m  Model API ProvidersGemini 1.5 Pro (Sep '24)GoogleProprietary2m  Model API ProvidersGemini 1.5 Flash (Sep '24)GoogleProprietary1m  Model API ProvidersGemini 1.5 Pro (May '24)GoogleProprietary2m  Model API ProvidersGemma 2 27BGoogleOpen8k  Model API ProvidersGemma 2 9BGoogleOpen8k  Model API ProvidersGemini 1.5 Flash-8BGoogleProprietary1m  Model API ProvidersGemma 3 27B InstructGoogleOpen128k  Model API ProvidersGemma 3 4B InstructGoogleOpen128k  Model API ProvidersGemma 3 1B InstructGoogleOpen32k  Model API ProvidersGemini 2.0 Flash Thinking Experimental (Dec '24)GoogleProprietary2m  Model API ProvidersGemini 2.0 Flash-Lite (Feb '25)GoogleProprietary1m  Model API ProvidersGemma 3 12B InstructGoogleOpen128k  Model API ProvidersGemini 2.5 Pro Experimental (Mar' 25)GoogleProprietary1m  Model API ProvidersGemini 2.0 Flash Thinking Experimental (Jan '25)GoogleProprietary1m  Model API ProvidersGemini 2.0 Flash-Lite (Preview)GoogleProprietary1m  Model API ProvidersGemini 1.0 ProGoogleProprietary33k  Model API ProvidersGemini 1.5 Flash (May '24)GoogleProprietary1m  Model API ProvidersClaude 3.5 Sonnet (Oct '24)AnthropicProprietary200k  Model API ProvidersClaude 3.5 Sonnet (June '24)AnthropicProprietary200k  Model API ProvidersClaude 3 OpusAnthropicProprietary200k  Model API ProvidersClaude 3.5 HaikuAnthropicProprietary200k  Model API ProvidersClaude 3 HaikuAnthropicProprietary200k  Model API ProvidersClaude 3.7 Sonnet (Extended Thinking)AnthropicProprietary200k  Model API ProvidersClaude 3.7 Sonnet (Standard)AnthropicProprietary200k  Model API ProvidersClaude 3 SonnetAnthropicProprietary200k  Model API ProvidersClaude 2.0AnthropicProprietary100k  Model API ProvidersClaude 2.1AnthropicProprietary200k  Model API ProvidersPixtral LargeMistralOpen128k  Model API ProvidersMistral Large 2 (Nov '24)MistralOpen128k  Model API ProvidersMistral Large 2 (Jul '24)MistralOpen128k  Model API ProvidersMistral Small 3MistralOpen32k  Model API ProvidersMistral Small (Sep '24)MistralOpen33k  Model API ProvidersMixtral 8x22B InstructMistralOpen65k  Model API ProvidersPixtral 12B (2409)MistralOpen128k  Model API ProvidersMinistral 8BMistralOpen128k  Model API ProvidersMistral NeMoMistralOpen128k  Model API ProvidersMinistral 3BMistralProprietary128k  Model API ProvidersMixtral 8x7B InstructMistralOpen33k  Model API ProvidersCodestral-MambaMistralOpen256k  Model API ProvidersCodestral (Jan '25)MistralProprietary256k  Model API ProvidersMistral Small 3.1MistralOpen128k  Model API ProvidersMistral SabaMistralProprietary32k  Model API ProvidersMistral Small (Feb '24)MistralProprietary33k  Model API ProvidersMistral Large (Feb '24)MistralProprietary33k  Model API ProvidersMistral 7B InstructMistralOpen8k  Model API ProvidersCodestral (May '24)MistralOpen33k  Model API ProvidersMistral MediumMistralProprietary33k  Model API ProvidersDeepSeek R1DeepSeekOpen128k  Model API ProvidersDeepSeek R1 Distill Llama 70BDeepSeekOpen128k  Model API ProvidersDeepSeek-V2.5 (Dec '24)DeepSeekOpen128k  Model API ProvidersDeepSeek-Coder-V2DeepSeekOpen128k  Model API ProvidersDeepSeek LLM 67B Chat (V1)DeepSeekOpen4k  Model API ProvidersDeepSeek V3 0324 (Mar' 25)DeepSeekOpen128k  Model API ProvidersDeepSeek R1 Distill Qwen 14BDeepSeekOpen128k  Model API ProvidersDeepSeek R1 Distill Qwen 32BDeepSeekOpen128k  Model API ProvidersDeepSeek Coder V2 Lite InstructDeepSeekOpen128k  Model API ProvidersDeepSeek R1 Distill Llama 8BDeepSeekOpen128k  Model API ProvidersDeepSeek R1 Distill Qwen 1.5BDeepSeekOpen128k  Model API ProvidersDeepSeek V3 (Dec '24)DeepSeekOpen128k  Model API ProvidersDeepSeek-V2.5DeepSeekOpen128k  Model API ProvidersDeepSeek-V2-ChatDeepSeekOpen128k  Model API ProvidersR1 1776PerplexityOpen128k  Model API ProvidersSonar ReasoningPerplexityProprietary127k  Model API ProvidersSonar ProPerplexityProprietary200k  Model API ProvidersSonarPerplexityProprietary127k  Model API ProvidersSonar Reasoning ProPerplexityProprietary127k  Model API ProvidersGrok BetaxAIProprietary128k  Model API ProvidersGrok 2 (Dec '24)xAIProprietary131k  Model API ProvidersGrok 3xAIProprietary1m  Model API ProvidersGrok 3 minixAIProprietary1m  Model API ProvidersGrok 3 Reasoning BetaxAIProprietary1m  Model API ProvidersGrok 3 mini ReasoningxAIProprietary1m  Model API ProvidersNova ProAmazonProprietary300k  Model API ProvidersNova LiteAmazonProprietary300k  Model API ProvidersNova MicroAmazonProprietary130k  Model API ProvidersPhi-4Microsoft AzureOpen16k  Model API ProvidersPhi-4 Mini InstructMicrosoft AzureOpen128k  Model API ProvidersPhi-4 Multimodal InstructMicrosoft AzureOpen128k  Model API ProvidersPhi-3 Medium Instruct 14BMicrosoft AzureOpen128k  Model API ProvidersPhi-3 Mini Instruct 3.8BMicrosoft AzureOpen4k  Model API ProvidersLFM 40BLiquid AIProprietary32k  Model API ProvidersSolar MiniUpstageOpen4k  Model API ProvidersDBRX InstructDatabricksOpen33k  Model API ProvidersMiniMax-Text-01MiniMaxOpen4m  Model API ProvidersLlama 3.1 Nemotron Instruct 70BNVIDIAOpen128k  Model API ProvidersLlama 3.3 Nemotron Super 49B v1NVIDIAOpen128k  Model API ProvidersLlama 3.1 Tulu3 405BAllen Institute for AIOpen128k  Model API ProvidersReka Flash (Sep '24)Reka AIProprietary128k  Model API ProvidersReka CoreReka AIProprietary128k  Model API ProvidersReka Flash (Feb '24)Reka AIProprietary128k  Model API ProvidersReka EdgeReka AIProprietary128k  Model API ProvidersReka Flash 3Reka AIOpen128k  Model API ProvidersCommand-R+ (Aug '24)CohereOpen128k  Model API ProvidersCommand-R+ (Apr '24)CohereOpen128k  Model API ProvidersCommand-R (Mar '24)CohereOpen128k  Model API ProvidersCommand-R (Aug '24)CohereOpen128k  Model API ProvidersAya Expanse 32BCohereOpen128k  Model API ProvidersCommand ACohereOpen256k  Model API ProvidersAya Expanse 8BCohereOpen8k  Model API ProvidersJamba 1.5 LargeAI21 LabsOpen256k  Model API ProvidersJamba 1.5 MiniAI21 LabsOpen256k  Model API ProvidersJamba 1.6 LargeAI21 LabsOpen256k  Model API ProvidersJamba 1.6 MiniAI21 LabsOpen256k  Model API ProvidersJamba InstructAI21 LabsProprietary256k  Model API ProvidersArctic InstructSnowflakeOpen4k  Model API ProvidersQwen2.5 MaxAlibabaProprietary32k  Model API ProvidersQwen2.5 Instruct 72BAlibabaOpen131k  Model API ProvidersQwen2.5 Coder Instruct 32BAlibabaOpen131k  Model API ProvidersQwen TurboAlibabaProprietary1m  Model API ProvidersQwen2 Instruct 72BAlibabaOpen131k  Model API ProvidersQwQ 32BAlibabaOpen131k  Model API ProvidersQwen2.5 Instruct 32BAlibabaOpen128k  Model API ProvidersQwen2.5 Coder Instruct 7B AlibabaOpen131k  Model API ProvidersQwen Chat 72BAlibabaOpen34k  Model API ProvidersQwen1.5 Chat 110BAlibabaOpen32k  Model API ProvidersQwQ 32B-PreviewAlibabaOpen33k  Model API ProvidersYi-Large01.AIProprietary32k  Model API ProvidersOpenChat 3.5 (1210)OpenChatOpen8k  Model API ProvidersFooterKey LinksCompare Language ModelsLanguage Models LeaderboardLanguage Model API LeaderboardImage ArenaVideo ArenaSpeech ArenaArtificial AnalysisFAQContact & Data accessTerms of UsePrivacy Policy[email\\xa0protected]Subscribe to our newsletterEmail addressSubscribeTwitterLinkedIn\")]\n"
     ]
    }
   ],
   "source": [
    "# Web Based Loader\n",
    "\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "url = \"https://artificialanalysis.ai\"\n",
    "web_loader = WebBaseLoader(\n",
    "    web_path=(url)\n",
    ")\n",
    "web_data = web_loader.load()\n",
    "print(web_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': 'https://artificialanalysis.ai'}, page_content='')]\n"
     ]
    }
   ],
   "source": [
    "# Web Based Loader\n",
    "\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "import bs4\n",
    "url = \"https://artificialanalysis.ai\"\n",
    "web_loader = WebBaseLoader(\n",
    "    web_path=(url),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only = bs4.SoupStrainer(\n",
    "            class_ = (\n",
    "                \"post-title\", \"post-content\", \"post-header\"\n",
    "            )\n",
    "        )\n",
    "    )\n",
    ")\n",
    "web_data = web_loader.load()\n",
    "print(web_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'Published': '2016-05-26', 'Title': 'Heat-bath random walks with Markov bases', 'Authors': 'Caprice Stanley, Tobias Windisch', 'Summary': 'Graphs on lattice points are studied whose edges come from a finite set of\\nallowed moves of arbitrary length. We show that the diameter of these graphs on\\nfibers of a fixed integer matrix can be bounded from above by a constant. We\\nthen study the mixing behaviour of heat-bath random walks on these graphs. We\\nalso state explicit conditions on the set of moves so that the heat-bath random\\nwalk, a generalization of the Glauber dynamics, is an expander in fixed\\ndimension.'}, page_content='arXiv:1605.08386v1  [math.CO]  26 May 2016\\nHEAT-BATH RANDOM WALKS WITH MARKOV BASES\\nCAPRICE STANLEY AND TOBIAS WINDISCH\\nAbstract. Graphs on lattice points are studied whose edges come from a ﬁnite set of\\nallowed moves of arbitrary length. We show that the diameter of these graphs on ﬁbers of a\\nﬁxed integer matrix can be bounded from above by a constant. We then study the mixing\\nbehaviour of heat-bath random walks on these graphs. We also state explicit conditions\\non the set of moves so that the heat-bath random walk, a generalization of the Glauber\\ndynamics, is an expander in ﬁxed dimension.\\nContents\\n1.\\nIntroduction\\n1\\n2.\\nGraphs and statistics\\n3\\n3.\\nBounds on the diameter\\n4\\n4.\\nHeat-bath random walks\\n8\\n5.\\nAugmenting Markov bases\\n14\\nReferences\\n19\\n1. Introduction\\nA ﬁber graph is a graph on the ﬁnitely many lattice points F ⊂Zd of a polytope where\\ntwo lattice points are connected by an edge if their diﬀerence lies in a ﬁnite set of allowed\\nmoves M ⊂Zd. The implicit structure of these graphs makes them a useful tool to explore\\nthe set of lattice points randomly: At the current lattice point u ∈F, an element m ∈±M\\nis sampled and the random walk moves along m if u + m ∈F and stays at u otherwise.\\nThe corresponding Markov chain is irreducible if the underlying ﬁber graph is connected and\\nthe set M is called a Markov basis for F in this case. This paper investigates the heat-bath\\nversion of this random walk: At the current lattice point u ∈F, we sample m ∈M and move\\nto a random element in the integer ray (u + Z · m) ∩F. The authors of [6] discovered that\\nthis random walk can be seen as a discrete version of the hit-and-run algorithm [15, 26, 16]\\nthat has been used frequently to sample from all the points of a polytope – not only from its\\nlattice points. The popularity of the continuous version of the hit-and-run algorithm has not\\nspread to its discrete analog, and not much is known about its mixing behaviour. One reason\\nis that it is already challenging to guarantee that all points in the underlying set F can be\\nDate: September 12, 2018.\\n2010 Mathematics Subject Classiﬁcation. Primary: 05C81, Secondary: 37A25, 11P21.\\nKey words and phrases. Heat-bath random walks, sampling, lattice points, Markov bases.\\n1\\n2\\nCAPRICE STANLEY AND TOBIAS WINDISCH\\nreached by a random walk that uses moves from M, whereas for the continuous version, a\\nrandom sampling from the unit sphere suﬃces. However, in many situations where a Markov\\nbasis is known, the heat-bath random walk is evidently fast. For instance, it was shown in [3]\\nthat the heat-bath random walk on contingency tables mixes rapidly when the number of\\ncolumns is ﬁxed. To work around the connectedness issue, a discrete hit-and-run algorithm\\nwas introduced in [1] for arbitrary ﬁnite sets F ⊂Zd. At each step in this random walk, a\\nsubordinate and unrestricted random walk starts at the current lattice point u ∈F and uses\\nthe unit vectors to collect a set of proposals S ⊂Zd. The random walk then moves from u\\nto a random point in S ∩F.\\nRandom walks of the heat-bath type, such as the one presented above, have been studied\\nrecently in [8] in a more general context. In this paper, we explore the mixing behaviour of\\nheat-bath random walks on the lattice points of polytopes with Markov bases. Throughout,\\nwe assume that a Markov basis has been found already and refer to the relevant literature\\nfor their computation [24, 25, 11, 17, 10, 21]. We call the underlying graph of the heat-bath\\nrandom walk a compressed ﬁber graph (Deﬁnition 2.5) and determine in Section 3 bounds on\\nits graph-diameter. We prove that for any A ∈Zm×d with kerZ(A)∩Nd = {0}, the diameter of\\ncompressed ﬁber graphs on {u ∈Nd : Au = b} that use a ﬁxed Markov bases M ⊂kerZ(A) is\\nbounded from above by a constant as b varies (Theorem 3.15). In contrast, we show that the\\ndiameter of conventional ﬁber graphs grow linearly under a dilation of the underlying polytope\\n(Remark 3.9). This gives rise to slow mixing results for conventional ﬁber walks as observed\\nin [27]. In Section 4, we study in more detail the combinatorial and analytical structure\\nof the transition matrices of heat-bath random walks on lattice points and prove upper and\\nlower bounds on their second largest eigenvalues. We also discuss how the distribution on the\\nmoves M aﬀects the speed of convergence (Example 4.21). Theorem 5.8 establishes with the\\ncanonical path approach from [23] an upper bound on the second largest eigenvalue when the\\nMarkov basis is augmenting (Deﬁnition 5.1) and the stationary distribution is uniform. From\\nthat, we conclude fast mixing results for random walks on lattice points in ﬁxed dimension.\\nAcknowledgements. CS was partially supported by the US National Science Foundation\\n(DMS 0954865). TW gratefully acknowledges the support received from the German National\\nAcademic Foundation.\\nConventions and Notation. The natural numbers are N := {0, 1, 2, . . .} and for any N ∈N,\\nN>N := {n ∈N : n > N} and N≥N := {N} ∪N>N. For n ∈N>0, let [n] := {1, . . . , n}. Let\\nM ⊂Qd be a ﬁnite set, then Z·M := {λm : m ∈M, λ ∈Z} and NM is the aﬃne semigroup\\nin Zd generated by M. For an integer matrix A ∈Zm×d with columns a1, . . . , ad ∈Zm,\\nwe write NA := N{a1, . . . , ad}. A graph is always undirected and can have multiple loops.\\nThe distance of two nodes u, v which are contained in the same connected component of a\\ngraph G, i.e. the number of edges in a shortest path between u and v in G, is denoted by\\ndistG(u, v). We set distG(u, v) := ∞if u and v are disconnected. A mass function on a ﬁnite\\nset Ωis a map f : Ω→[0, 1] such that P\\nω∈Ωf(ω) = 1. A mass function f on Ωis positive\\nif f(ω) > 0 for all ω ∈Ω. A set F ⊂Zd is normal if it there exists a polytope P ⊂Qd such\\nthat P ∩Zd = F.\\nHEAT-BATH RANDOM WALKS WITH MARKOV BASES\\n3\\n2. Graphs and statistics\\nWe ﬁrst introduce the statistical framework in which this paper lives and recall important\\naspects of the interplay between graphs and statistics. A random walk on a graph G = (V, E)\\nis a map H : V × V →[0, 1] such that for all v ∈V , P\\nu∈V H(v, u) = 1 and such that\\nH(v, u) = 0 if {v, u} ̸∈E. When there is no ambiguity, we represent a random walk as an\\n|V | × |V |-matrix, for example when it is clear how the elements of V are ordered. Fix a\\nrandom walk H on G. Then H is irreducible if for all v, u ∈V there exists t ∈N such that\\nHt(v, u) > 0. The random walk H is reversible if there exists a mass function µ : V →[0, 1]\\nsuch that µ(u) · H(u, v) = µ(v) · H(v, u) for all u, v ∈V and symmetric if H is a symmetric\\nmap. A mass function π : V →[0, 1] is a stationary distribution of H if π ◦H = π. For\\nsymmetric random walks, the uniform distribution on V is always a stationary distribution.\\nIf |V | = n, then we denote the eigenvalues of H by 1 = λ1(H) ≥λ2(H) ≥· · · ≥λn(H) ≥−1\\nand we write λ(H) := max{λ2(H), −λn(H)} for the second largest eigenvalue modulus of H.\\nAny irreducible random walk has a unique stationary distribution [14, Corollary 1.17] and\\nλ(H) ∈[0, 1] measures the convergence rate: the smaller λ(H), the faster the convergence.\\nThe aim of this paper is to study random walks on lattice points that use a set of moves.\\nTypically, this is achieved by constructing a graph on the set of lattice points as follows\\n(compare to [7, Section 1.3] and [24, Chapter 5]).\\nDeﬁnition 2.1. Let F ⊂Zd be a ﬁnite set and M ⊂Zd. The graph F(M) is the graph on\\nF where two nodes u, v ∈F are adjacent if u −v ∈M or v −u ∈M.\\nA normal set F ⊂Zd is ﬁnite and satisﬁes F = convQ(F)∩Zd. A canonical class of normal\\nsets that arise in many applications, is given by the ﬁbers of an integer matrix:\\nDeﬁnition 2.2. Let A ∈Zm×d and b ∈NA. The set FA,b := {u ∈Nd : Au = b} is the b-ﬁber\\nof A. The collection of all ﬁbers of A is PA := {FA,b : b ∈NA}. For M ⊂kerZ(A), the graph\\nFA,b (M) is a ﬁber graph.\\nLet F, M ⊂Zd be ﬁnite. If the membership in F can be veriﬁed eﬃciently – for instance\\nwhen F is given implicitly by linear equations and inequalities – then it is possible to explore\\nF randomly using M as follows: At a given node v ∈F, a uniform element m ∈M is\\nselected. If v + m ∈M, then the random walk moves along m to v + m and if v + m ̸∈M,\\nthe we stay at v. Formally, we obtain the following random walk.\\nDeﬁnition 2.3. Let F ⊂Zd and M ⊂Zd be two ﬁnite sets. The simple walk is the random\\nwalk on F(M) where the probability to traverse between to adjacent nodes u and v is |±M|−1\\nand the probability to stay at a node u is |{m ∈±M : u + m ̸∈F}| · | ± M|−1.\\nThe simple walk is symmetric and hence the uniform distribution is a stationary distribu-\\ntion (see also [27, Section 2]). To ensure convergence, the random walk has to be irreducible,\\nthat is, the underlying graph has to be connected. The following deﬁnition is a slight adaption\\nof the generalized Markov basis as deﬁned in [21, Deﬁnition 1].\\nDeﬁnition 2.4. Let P be a collection of ﬁnite subsets of Zd. A ﬁnite set M ⊂Zd is a\\nMarkov basis of P, if for all F ∈P, F(M) is a connected graph.\\n4\\nCAPRICE STANLEY AND TOBIAS WINDISCH\\nWe refer to [6, Theorem 3.1] for a proof that for collections PA, a ﬁnite Markov basis\\nalways exists and can be computed with tools from commutative algebra (see also [11] for\\nmore on the computation of Markov bases). We now introduce a construction of graphs on\\nlattice points that also give rise to implementable random walks, but whose edges have far\\nmore reach.\\nDeﬁnition 2.5. Let F ⊂Zd and M ⊂Zd be ﬁnite sets. The compression of the graph\\nF(M) is the graph Fc(M) := F(Z · M).\\nFigure 1. Compressing graphs.\\nCompressing a graph F(M) preserves its connectedness: F(M) is connected if and only\\nif Fc(M) is connected.\\n3. Bounds on the diameter\\nIn general knowledge of the diameter of the graph underlying a Markov chain can provide\\ninformation about the mixing time. For random walks on ﬁber graphs, the chains which we\\nconsider, the underlying graph coincides with the ﬁber graph. In this section, we determine\\nlower and upper bounds on the diameter of ﬁber graphs and their compressed counterparts.\\nFor a ﬁnite set M ⊂Zd and any norm ∥· ∥on Rd, let ∥M∥:= maxm∈M ∥m∥.\\nLemma 3.1. Let F ⊂Zd and M ⊂Zd be ﬁnite sets, then\\ndiam(F(M)) ≥\\n1\\n∥M∥· max{∥u −v∥: u, v ∈F}.\\nProof. If F(M) is not connected, then the statement holds trivially, so assume that M is a\\nMarkov basis for F. Let u′, v′ ∈F such that ∥u′ −v′∥= max{∥u −v∥: u, v ∈F} and let\\nm1, . . . , mr ∈M so that u′ = v′+Pr\\ni=1 mi is a path of minimal length, then ∥u′−v′∥≤r·∥M∥\\nand the claim follows from diam(F(M)) ≥distF(M)(u′, v′) = r.\\n□\\nRemark 3.2. Let F ⊂Zd be a normal set. For all l ∈{−1, 0, 1}d and u, v ∈F we have\\n(u −v)T l ≤∥u −v∥1 and thus widthl(F) := max{(u −v)T l : u, v ∈F} ≤max{∥u −v∥1 :\\nu, v ∈F}. Suppose that u′, v′ ∈F are such that ∥u′ −v′∥1 = max{∥u −v∥1 : u, v ∈F} and\\nlet l′\\ni := sign(u′\\ni −v′\\ni) for i ∈[d], then\\n∥u′ −v′∥1 = (u′ −v′)T · l′ ≤widthl′(F) ≤max{∥u −v∥1 : u, v ∈F} = ∥u′ −v′∥1.\\nThe lattice width of F is width(F) := minl∈Zd widthl(F) and thus Lemma 3.1 gives\\n∥M∥1 · diam(F(M)) ≥width(F).\\nHEAT-BATH RANDOM WALKS WITH MARKOV BASES\\n5\\nDeﬁnition 3.3. Let P be a collection of ﬁnite subsets of Zd.\\nA ﬁnite set M ⊂Zd is\\nnorm-like for P if there exists a constant C ∈N such that for all F ∈P and all u, v ∈F,\\ndistF(M)(u, v) ≤C · ∥u −v∥. The set M is ∥· ∥-norm-reducing for P if for all F ∈P and all\\nu, v ∈F there exists m ∈M such that u + m ∈F and ∥u + m −v∥< ∥u −v∥.\\nThe property of being norm-like does not depend on the norm, whereas being norm-\\nreducing does.\\nNorm-reducing sets are always norm-like, and norm-like sets are in turn\\nalways Markov bases, but the reverse of both statements is false in general (Example 3.4 and\\nExample 3.5). For collections PA however, every Markov basis is norm-like (Proposition 3.7).\\nExample 3.4. For any n ∈N, consider the normal set Fn := ([2]×[n]×{0})∪{(2, n, 1)} with\\nthe Markov basis {(0, 1, 0), (0, 0, 1), (−1, 0, −1)}. The distance between (1, 1, 0) and (2, 1, 0)\\nin Fn(M) is 2n and thus M is not norm-like for {Fn : n ∈N} (see also Figure 2).\\nExample 3.5. Let d ∈N and consider A := (1, . . . , 1) ∈Z1×d, then the set M := {e1 −ei :\\n2 ≤i ≤d} is a Markov basis for the collection PA. However, M is not ∥·∥p-norm-reducing for\\nany d ≥3 and any p ∈[1, ∞]. For instance, consider e2 and e3 in FA,1 (M). The only move\\nfrom M that can be applied on e2 is e1−e2, but ∥(e2+e1−e2)−e3)∥p = ∥e2−e3∥p. On the other\\nhand, in the case we cannot ﬁnd a move that decreases the 1-norm of two nodes u, v ∈FA,b\\nby 1, we can ﬁnd instead two moves m1, m2 ∈M such that u + m1, u + m1 + m2 ∈FA,b and\\n∥u + m1 + m2 −v∥= ∥u −v∥−2. Thus, the graph-distance of any two elements u and v in\\nFA,b (M) is at most ∥u −v∥1 and hence M is norm-like for PA.\\nFigure 2. The graph from Example 3.4\\nRemark 3.6. Let P be a collection of ﬁnite subsets of Zd and M ⊂Zd be norm-like for P.\\nIt follows from the deﬁnition that there exists a constant C ∈Q≥0 such that for all F ∈P\\ndiam(F(M)) ≤C · max{∥u −v∥: u, v ∈F}.\\nThe proof of our next results uses the Graver basis GA ⊂Zd for an integer matrix A ∈Zm×d\\nwith kerZ(A) ∩Nd = {0}. We refer to [4, Chapter 3] for a precise deﬁnition.\\nProposition 3.7. Let A ∈Zm×d with kerZ(A) ∩Nd = {0} and M ⊂kerZ(A) be a Markov\\nbasis of PA. Then M is norm-like for PA.\\nProof. Let M be a Markov basis for PA. The Graver basis GA for A is a ﬁnite set which\\nis ∥· ∥1-norm-reducing for PA. Thus, deﬁne C := maxg∈GA diam(FA,Ag+ (M)). Now, pick\\nu, v ∈FA,b arbitrarily and let u = v + Pr\\ni=1 gi be a walk from u to v in FA,b (GA) of minimal\\nlength. Since the Graver basis is norm-reducing for FA,b, there always exists a path of length\\nat most ∥u −v∥1 and hence r ≤∥u −v∥1. Every gi can be replaced by a path in FA,Ag+\\ni (M)\\n6\\nCAPRICE STANLEY AND TOBIAS WINDISCH\\nof length at most C and these paths stay in FA,b. This gives a path of length C · r, hence\\ndistFA,b(M)(u, v) ≤C∥u −v∥1.\\n□\\nProposition 3.8. Let P ⊂Zd be a polytope with dim(P ∩Zd) > 0 and let M be a Markov\\nbasis for Fi := (i · P) ∩Zd for all i ∈N. There exists a constant C′ ∈Q>0 such that for all\\ni ∈N, C′ · i ≤diam(Fi(M)). If M is norm-like for {Fi : i ∈N}, then there exists a constant\\nC ∈Q>0 such that diam(Fi(M)) ≤C · i for all i ∈N.\\nProof. For the lower bound on the diameter, it suﬃces to show the existence of C′ such that\\nC′ · i ≤max{∥u −v∥: u, v ∈Fi} for all i ∈N due to Lemma 3.1. Since dim(P ∩Zd) > 0,\\nwe can pick distinct w, w′ ∈P ∩Zd. For all i ∈N, i · w, i · w′ ∈Fi and hence i · ∥w −w′∥≤\\nmax{∥u −v∥: u, v ∈Fi}.\\nTo show the upper bound, assume that M is norm-like. It suﬃces to show that there\\nexists C ∈Q≥0 such that max{∥u −v∥: u, v ∈Fi} ≤i · C by Remark 3.6.\\nNow, let\\nv1, . . . , vr ∈Qd such that P = convQ(v1, . . . , vr) and deﬁne C := max{∥vs −vt∥: s ̸= t}.\\nSince Fi = (i·P)∩Zd ⊂convQ(iv1, . . . , ivr) for all i ∈N, we have max{∥u−v∥: u, v ∈Fi} ≤\\nmax{∥ivs −ivt∥: s ̸= t} ≤C · i.\\n□\\nRemark 3.9. Let A ∈Zm×n with kerZ(A) ∩Nd = {0} and let M be a Markov basis for PA.\\nThen M is norm-like due to Proposition 3.7 and thus for all b ∈NA there exists C, C′ ∈Q≥0\\nsuch that\\ni · C′ ≤diam(FA,ib (M)) ≤i · C\\nfor all i ∈N. This generalizes for instance [20, Proposition 2.10] and [27, Example 4.7], where\\nlinear diameters on a ray in NA have been observed. This also implies that the construction\\nof expanders from [27, Section 4] works for every right-hand side b ∈NA.\\nRemark 3.10. Let A ∈Zm×d with kerZ(A)∩Nd = {0}, b ∈NA, and let M be a Markov basis\\nfor PA. Proposition 3.8 provides a new proof that the simple walk on (FA,ib (M))i∈N cannot\\nmix rapidly. The lower bound on the diameter from Proposition 3.8 implies, in general, the\\nfollowing upper bound on the edge-expansion (see for example [9, Proposition 1.30]):\\nh(FA,i·b (M)) ≤|M|\\n\\x12\\nexp\\n\\x12log |FA,i·b|\\nD · i\\n\\x13\\n−1\\n\\x13\\n.\\nIn particular, the edge-expansion cannot be bounded from below by Ω( 1\\np(i))i∈N for a polyno-\\nmial p ∈Q[t] and since (|FA,i·b|)i∈N ∈O(ir)i∈N, the simple walk cannot mix rapidly. In [27],\\nit was shown that the edge-expansion can be bounded from above by O(1\\ni )i∈N, which cannot\\nbe concluded from the upper expression.\\nWe now turn our attention to the diameter of compressed ﬁber graphs.\\nIn particular,\\nwe want to know for which collections of normal sets is their diameter bounded. In general,\\ncompressing a ﬁber graph does not necessarily have an eﬀect on the diameter (Example 3.11).\\nAlthough a low diameter is a necessary condition for good mixing, it is not suﬃcient. For\\ninstance, let Gn be the disjoint union of two complete graphs Kn connected by a single edge.\\nThen diam(Gn) = 3, but h(Gn) ≤1\\nn implies that the simple walk does not mix rapidly.\\nHEAT-BATH RANDOM WALKS WITH MARKOV BASES\\n7\\nExample 3.11. For any n ∈N, let Fn := {(0, 0), (0, 1), (1, 1), (1, 2), . . . , (n, n)} ⊂Z2. The\\nunit vectors M = {e1, e2} are a Markov basis for {Fn : n ∈N}. However, Fc\\nn(M) = Fn(M)\\nand thus diam(Fc\\nn(M)) = diam(Fn(M)) = 2n is unbounded.\\nLemma 3.12. Let A ∈Zm×d and z ∈kerZ(A). There exists r ∈[2d −2], distinct elements\\ng1, . . . , gr ∈GA, and λ1, . . . , λr ∈N>0 such that z = Pr\\ni=1 λigi and gi ⊑z for all i ∈[r]\\nProof. This is [4, Lemma 3.2.3], although it only becomes clear from the original proof of [22,\\nTheorem 2.1] that the appearing elements are all distinct.\\n□\\nProposition 3.13. Let A ∈Zm×d and P :=\\n\\x08\\n{x ∈Zd : Ax = b, l ≤x ≤u} : l, u ∈Zd, b ∈Zm\\t\\n.\\nThen for all F ∈P, diam(Fc(GA)) ≤2d −2.\\nProof. Let s, t ∈{x ∈Zd : Ax = b, l ≤x ≤u}, then s−t ∈kerZ(A) and thus s = t+Pr\\ni=1 λigi\\nwith r ≤2d −2, λ1, . . . , λr ∈N>0, and distinct g1, . . . , gr ∈GA such that gi ⊑s −t according\\nto Lemma 3.12. It’s now a consequence from [4, Lemma 3.2.4] that all intermediate points\\nt + Pk\\ni=1 λigi for k ≤r are in {x ∈Zd : Ax = b, l ≤x ≤u}.\\n□\\nLemma 3.14. Let F ⊂Zd be ﬁnite and let Fi := (i · convQ(F)) ∩Zd for i ∈N. For all\\nu, v ∈F, distFc\\ni (M)(iu, iv) ≤distF(M)(u, v) for all i ∈N.\\nProof. The statement is trivially true if u and v are disconnected in F(M). Thus, assume\\nthe contrary and let u = v + Pk\\nj=1 mj with mj ∈M be a path in F(M) of length k =\\ndistF(M)(u, v) and let i ∈N. Clearly, i · u = i · v + i · Pk\\nj=1 mj = i · v + Pk\\nl=1 i · mj, so\\nit is left to prove that the elements traversed by this paths are in Fi. Let l ∈[k], since\\nv + Pl\\nj=1 mj ∈F, we have i · v + Pl\\nj=1 i · mj ∈i · F ⊆Fi. Hence, this is a path in Fc\\ni (M)\\nof length k = distF(M)(u, v).\\n□\\nWe are ready to prove that the diameter of compressed ﬁber graphs coming from an integer\\nmatrix can be bounded for all right-hand sides simultaneously.\\nTheorem 3.15. Let A ∈Zm×d with kerZ(A) ∩Nd = {0} and let M be a Markov basis for\\nPA. There exists a constant C ∈N such that diam(Fc(M)) ≤C for all F ∈PA.\\nProof. Our proof relies on basic properties of the Graver basis GA of A. For any g ∈GA,\\nlet Fg := FA,Ag+ and let K := max{distFg(M)(g+, g−) : g ∈GA}.\\nWe show that the\\ndiameter of any compressed ﬁber graph of A is bounded from above by (2d −2) · K. Let\\nb ∈NA arbitrary and choose elements u, v ∈FA,b. According to Proposition 3.13, there\\nexists r ∈[2d −2], g1, . . . , gr ∈GA and λ1, . . . , λr ∈Z such that u = v + Pr\\ni=1 λigi, and\\nv + Pl\\ni=1 λigi ∈Nd for all l ∈[r].\\nAccording to Lemma 3.14, for any i ∈[r] there are\\nmi\\n1, . . . , mi\\nki ∈M and α1, . . . , αki ∈Z such that λig+\\ni = λig−\\ni + Pki\\nj=1 αjmi\\nj is a path in the\\ncompression of FA,Aλig+\\ni (M) of length ki ≤K. Lifting these paths for every i ∈[r] yields a\\npath u = v + Pr\\ni=1\\nPki\\nj=1 αjmi\\nj in Fc\\nA,b (M) of length r · K ≤(2d −2) · K.\\n□\\n8\\nCAPRICE STANLEY AND TOBIAS WINDISCH\\n4. Heat-bath random walks\\nIn this section, we establish the heat-bath random walk on compressed ﬁber graphs. We\\nrefer to [8] for a more general introduction on random walks of heat-bath type. Let F ⊂Zd\\nbe ﬁnite set. For any u ∈F and m ∈Zd, the ray in F through u along m is denoted by\\nRF,m(u) := (u + m · Z) ∩F. Additionally, given a mass function π : F →[0, 1], we deﬁne\\nHπ\\nF,m(x, y) :=\\n(\\nπ(y)\\nπ(RF,m(x))\\n, if y ∈RF,m(x)\\n0\\n, otherwise\\nfor x, y ∈F. For M ⊂Zd and a mass function f : M →[0, 1], the heat-bath random walk is\\n(4.1)\\nHπ,f\\nF,M =\\nX\\nm∈M\\nf(m) · Hπ\\nF,m.\\nThe underlying graph of the heat-bath random walk is the compression Fc(M) and in this\\nsection, we assume throughout that for all m ∈M and λ ∈Z \\\\ {−1, 1}, λ · m ̸∈M. Let us\\nﬁrst recall the basic properties of this random walk (compare also to [6, Lemma 2.2]).\\nAlgorithm 1 Heat-bath random walk on compressed ﬁber graphs\\nInput: F ⊂Zd, M ⊂Zd, v ∈F, mass functions f : M →[0, 1] and π : F →[0, 1], r ∈N\\n1: procedure HeatBath:\\n2:\\nv0 := v\\n3:\\nFOR s = 0; s = s + 1, s < r\\n4:\\nSample m ∈M according to f\\n5:\\nSample vs+1 ∈RF,m(vs) according to RF,m(vs) →[0, 1], y 7→\\nπ(y)\\nπ(RF,m(vs))\\n6:\\nRETURN v1, . . . , vr\\nProposition 4.1. Let F ⊂Zd and M ⊂Zd be ﬁnite sets. Let f : M →[0, 1] and π : F →\\n(0, 1) be mass functions. Then Hπ,f\\nF,M is aperiodic, has stationary distribution π, is reversible\\nwith respect to π, and all of its eigenvalues are non-negative. The random walk is irreducible\\nif and only if {m ∈M : f(m) > 0} is a Markov basis for F.\\nProof. Since for any u ∈F and any m ∈M, Hπ\\nF,m(u, u) > 0, there are halting states and\\nthus Hπ,f\\nF,M is aperiodic. By deﬁnition, π(x)Hπ\\nF,m(x, y) = π(y)Hπ\\nF,m(y, x) and thus Hπ,f\\nF,M\\nis reversible with respect to π and π is a stationary distribution.\\nThe statement on the\\neigenvalues is exactly [8, Lemma 1.2]. Let M′ = {m ∈M : f(m) > 0} and f ′ = f|M′, then\\nHπ,f\\nF,M = Hπ,f′\\nF,M′ and thus the heat-bath random walk is irreducible if and only if M′ is a\\nMarkov basis for F.\\n□\\nRemark 4.2. Analyzing the speed of convergence of random walks with second largest\\neigenvalues does not take the computation time of a single transition into account. From\\na computational point of view, the diﬀerence of the simple walk and the heat-bath random\\nwalk is Step 4 of Algorithm 1. However, we argue that Step 4 can be done eﬃciently in\\nmany cases. For instance, a hard normalizing constant of π cancels out. If π is the uniform\\ndistribution, then one needs to sample uniformly from RF,m(v) in Step 4, which can be done\\nHEAT-BATH RANDOM WALKS WITH MARKOV BASES\\n9\\neﬃciently. If the input of Algorithm 1 is a normal set F = {u ∈Zd : Au ≤b} that is given\\nin H-representation, then the length of the ray RF,m(v) can be computed with a number of\\nrounding, division, and comparing operations that is linear in the number of rows of A.\\nThere are situations in which the heat-bath random walk provides no speed-up compared\\nwith the simple walk (Example 4.3). Intuitively, adding more moves to the set of allowed\\nmoves should improve the mixing time of the random walk. In general, however, this is not\\ntrue for the heat-bath walk (Example 4.4).\\nExample 4.3. For n ∈N, consider the normal set\\nFn :=\\n\\x1a\\x14\\n0\\n1\\n1\\n· · ·\\n1\\n1\\n0\\n0\\n· · ·\\n0\\n\\x15\\n,\\n\\x14\\n1\\n0\\n1\\n· · ·\\n1\\n0\\n1\\n0\\n· · ·\\n0\\n\\x15\\n, . . . ,\\n\\x14\\n1\\n1\\n· · ·\\n1\\n0\\n0\\n0\\n· · ·\\n0\\n1\\n\\x15\\x1b\\n⊂Q2×n.\\nIn the language of [7, Section 1.1], Fn is precisely the ﬁber of the 2 × n independence model\\nwhere row sums are (n −1, 1) and column sums are (1, 1, . . . , 1).\\nThe minimal Markov\\nbasis of the independence model, often referred to as the basic moves, is precisely the set\\nMn := {v −u : u, v ∈Fn} \\\\ {0}. In particular, the ﬁber graph Fn(Mn) is the complete\\ngraph on n nodes. All rays along basic moves have length 2 and thus the transition matrices\\nof the simple random walk and the heat-bath random walk coincide. There are n · (n −1)\\nmany basic moves and the transition matrix of both random walks is\\n1\\nn(n −1)\\n\\uf8ee\\n\\uf8ef\\uf8f0\\n1\\n. . .\\n1\\n...\\n...\\n1\\n. . .\\n1\\n\\uf8f9\\n\\uf8fa\\uf8fb+ (n(n −1) −n)\\nn(n −1)\\n· In.\\nThe second largest eigenvalue is 1 −\\n1\\nn−1 which implies that for n →∞, neither the simple\\nwalk nor the heat-bath random walk are rapidly mixing.\\nExample 4.4. Let F = [2] × [5] ⊂Z2, M = {e1, e2, 2e1 + e2}, and let π be the uniform\\ndistribution on F.\\nSince {e2, 2e1 + e2} is not a Markov basis for F, any mass function\\nf : M →[0, 1] must have f(e1) > 0 in order to make the corresponding heat-bath random\\nwalk irreducible.\\nComparing the second largest eigenvalue modulus of heat-bath random\\nwalks that sample from {e1, e2} and M uniformly, we obtain\\nλ\\n\\x121\\n2Hπ\\nF,e1 + 1\\n2Hπ\\nF,e2\\n\\x13\\n= 1\\n2 < 2\\n3 = λ\\n\\x121\\n3Hπ\\nF,e1 + 1\\n3Hπ\\nF,e2 + 1\\n3Hπ\\nF,2e1+e2\\n\\x13\\n.\\nSo, adding 2e1 + e2 to the set of allowed moves slows the walk down. This phenomenon does\\nnot appear for the simple walk on F, where the second largest eigenvalue modulus improves\\nfrom ≈0.905 to ≈0.888 when adding the move 2e1 + e2 to the Markov basis.\\n=\\n+\\n+\\nFigure 3. Decomposition of the graph in Example 4.4\\n10\\nCAPRICE STANLEY AND TOBIAS WINDISCH\\nRemark 4.5. Let F ⊂Zd be ﬁnite and M = {m1, . . . , md} ⊂Zd be a linearly independent\\nMarkov basis of F. If the moves are selected uniformly, then the heat-bath random walk on\\nF coincides with the Glauber dynamics on F. To see it, choose u ∈F and let\\nF′ := {λ ∈Zd : u + λ1m1 + · · · + λdmd ∈F}.\\nIt is easy to check that F′ is unique up to translation and depends only on F, u, and M.\\nSince the vectors in M are linearly independent, every element of F can be represented by\\na unique choice of coeﬃcients in F′. Thus, the heat-bath random walk on F using M is\\nequivalent to the heat-bath random walk on on F′ using the unit vectors as moves. For any\\nunit vector ei ∈Zd, the ray through an element v ∈F′ is {w ∈F : wj = vj∀j ̸= i} and this\\nis precisely the form desired in the Glauber dynamics [14, Section 3.3.2].\\nFor the remainder of this section, we primarily focus on heat-bath random walks Hπ,f\\nF,M\\nthat converge to the uniform distribution π on a ﬁnite, but not necessarily normal, set\\nF. We particularly aim for bounds on its second largest eigenvalue by making use of the\\ndecomposition from equation 4.1. Our ﬁrst observations consider its summands Hπ\\nF,m that\\ncan be well understood analytically (Proposition 4.6) and combinatorially (Proposition 4.7).\\nProposition 4.6. Let F ⊂Zd be a ﬁnite set, m ∈Zd, and π : F →[0, 1] be the uniform\\ndistribution. Let R1, . . . , Rk be the disjoint rays through F along m. Then\\n1. Hπ\\nF,m is symmetric and idempotent.\\n2. img(Hπ\\nF,m) = spanR\\nnP\\nx∈R1 ex, P\\nx∈R2 ex, . . . , P\\nx∈Rk ex\\no\\n.\\n3. ker(Hπ\\nF,m) = Lk\\ni=1 spanR {ex −ey : x, y ∈Ri, x ̸= y}.\\n4. rank(Hπ\\nF,m) = k and dim ker(Hπ\\nF,m) = |F| −k.\\n5. The spectrum of Hπ\\nF,m is {0, 1}.\\nProof. Symmetry of Hπ\\nF,m follows from the deﬁnition. By assumption, F is the disjoint union\\nof R1, . . . , Rk and hence there exists a permutation matrix S such that SHπ\\nF,mST is a block\\nmatrix whose building blocks are the matrices\\n1\\n|Ri|\\n\\uf8ee\\n\\uf8ef\\uf8f0\\n1\\n. . .\\n1\\n...\\n...\\n1\\n. . .\\n1\\n\\uf8f9\\n\\uf8fa\\uf8fb∈Q|Ri|×|Ri|.\\nThus, Hπ\\nF,m is idempotent and the rank of Hπ\\nF,m is k. A basis of its image and its kernel can\\nbe read oﬀdirectly and idempotent matrices can only have the eigenvalues 0 and 1.\\n□\\nProposition 4.7. Let F ⊂Zd and M ⊂Zd be ﬁnite sets, π : F →[0, 1] be the uniform\\ndistribution, and let V1, . . . , Vc ⊆F be the nodes of the connected components of F(M), then\\n\\\\\\nm∈M\\nimg(Hπ\\nF,m) = spanR\\n\\uf8f1\\n\\uf8f2\\n\\uf8f3\\nX\\nx∈V1\\nex, . . . ,\\nX\\nx∈Vc\\nex\\n\\uf8fc\\n\\uf8fd\\n\\uf8fe.\\nProof. It is clear by Proposition 4.6 that the set on the right-hand side is contained in any\\nimg(Hπ\\nF,m) since any Vi decomposes disjointly into rays along m ∈M. To show the other\\ninclusion, write M = {m1, . . . , mk} and let for any i ∈[k], Ri\\n1, . . . , Ri\\nni be the disjoint rays\\nHEAT-BATH RANDOM WALKS WITH MARKOV BASES\\n11\\nthrough F parallel to mi. In particular, {Ri\\n1, . . . , Ri\\nni} is a partition of F for any i ∈[k]. Let\\nv ∈T\\nm∈M img(Hπ\\nF,m). Again by Proposition 4.6, there exists for any i ∈[k], λi\\n1, . . . , λi\\nni ∈Q\\nsuch that\\nv =\\nni\\nX\\nj=1\\nX\\nx∈Ri\\nj\\nλi\\njex.\\nNotice that if two distinct Markov moves mi and mi′ and two indices j ∈[ni] and j′ ∈[ni′]\\nsatisfy Ri\\nj ∩Ri′\\nj′ ̸= ∅, then λi\\nj = λi′\\nj′. We show that for any i ∈[k] and any a ∈[c], λi\\nj = λi\\nj′\\nwhen Ri\\nj and Ri\\nj′ are a subset of Va. This implies the proposition. So take distinct x, x′ ∈Va\\nand assume that x and x′ lie on diﬀerent rays of mi and let that be x ∈Ri\\nj and x′ ∈Ri\\nj′ with\\nj ̸= j′. Since x and x′ are in the same connected component Va of F(M), let yi0, . . . , yir ∈F\\nbe the nodes on a minimal path in Fc(M) with yi0 = x and yir = x′. For any s ∈[r], yis\\nand yis−1 are contained in the same ray Rks\\nts coming from a Markov move mks. In particular,\\nRts−1\\nks−1 ∩Rks\\nts ̸= ∅and due to our observation made above λi\\nj = λk1\\nt1 = λk2\\nt2 = · · · = λkr\\ntr = λi\\nj′\\nwhich ﬁnishes the proof.\\n□\\nDeﬁnition 4.8. Let F ⊂Zd and M ⊂Zd be ﬁnite sets and M′ ⊆M. Let V be the set of\\nconnected components of F(M\\\\M′) and R be the set of all rays through F along all elements\\nof M′. The ray matrix of F(M) along M′ is AF(M, M′) := (|R ∩V |)R∈R,V ∈V ∈NR×V.\\nExample 4.9. Let F = [3]×[3], M = {e1, e2, e1 +e2}, and M′ = {e1, e2}. Then F(M\\\\M′)\\nhas ﬁve connected components and the ray matrix of F(M) along M′ is\\nAF(M, M′) =\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n1\\n1\\n1\\n0\\n0\\n0\\n1\\n1\\n1\\n0\\n0\\n0\\n1\\n1\\n1\\n0\\n0\\n1\\n1\\n1\\n0\\n1\\n1\\n1\\n0\\n1\\n1\\n1\\n0\\n0\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n.\\nRemark 4.10. Let F ⊂Z2, then the rays through F along e1 are the connected components\\nof F({e1, e2} \\\\ {e2}) and the rays through F along e2 are the connected components of\\nF({e1, e2} \\\\ {e1}), thus AF(M, e1) = AF(M, e2)T .\\nProposition 4.11. Let F ⊂Zd and M ⊂Zd be ﬁnite sets, π : F →[0, 1] be the uniform\\ndistribution, and M′ ⊆M. Then\\nker(AF(M, M′)) ∼=\\n\\\\\\nm∈M\\\\M′\\nimg(Hπ\\nF,m) ∩\\n\\\\\\nm∈M′\\nker(Hπ\\nF,m).\\nProof. Let V1, . . . , Vc be the connected components of F(M \\\\ M′) and R1, . . . , Rr be the\\nrays along elements in M′. Let I := T\\nm∈M\\\\M′ img(Hπ\\nF,m) and K := T\\nm∈M′ ker(Hπ\\nF,m). By\\nProposition 4.7, any element of I has the form v = Pc\\ni=1(λi\\nP\\nx∈Vi ex) for λ1, . . . , λc ∈Q.\\nAssume additionally that v ∈ker(Hπ\\nF,m) for m ∈M′ and let Ri1, . . . Rij be the rays which\\nbelong to m, then for any k ∈[j], 0 = P\\nx∈Rik vx = Pc\\nj=1 λj|Rik∩Vj|. Put diﬀerently, a vector\\nλ ∈Rc is in the kernel of (|Ri ∩Vj|)i∈[r],j∈[c] if and only if Pc\\ni=1(λi\\nP\\nx∈Vi ex) ∈I ∩K.\\n□\\n12\\nCAPRICE STANLEY AND TOBIAS WINDISCH\\nConditions on the kernel of the ray matrix allow us to give a lower bound on the second\\nlargest eigenvalue of the heat-bath random walk.\\nProposition 4.12. Let F ⊂Zd and M ⊂Zd be ﬁnite sets and π be the uniform distribution.\\nLet M′ ⊆M such that ker(AF(M, M′)) ̸= {0}, then λ(Hπ,f\\nF,M) ≥1 −P\\nm∈M′ f(m) for any\\nmass function f : M →[0, 1].\\nProof. Using the isomorphism from Proposition 4.11, we can choose a non-zero v ∈QP such\\nthat Hπ\\nF,mv = v for all m ∈M \\\\ M′ and Hπ\\nF,mv = 0 for all m ∈M′. In particular\\nHπ,f\\nF,Mv =\\nX\\nm∈M\\nf(m)Hπ\\nF,mv =\\nX\\nm∈M\\\\M′\\nf(m)Hπ\\nF,mv =\\nX\\nm∈M\\\\M\\nf(m)v.\\nSince f is a mass function, 1 −P\\nm∈M′ f(m) is an eigenvalue of Hπ,f\\nF,M.\\n□\\nDeﬁnition 4.13. Let F ⊂Zd and m, m′ ∈Zd not collinear.\\nThe pair (m, m′) has the\\nintersecting ray property in F if the following holds: For any pair of rays R1, R2 parallel\\nto m and any pair of rays R′\\n1, R′\\n2 parallel to m′ where both R1 ∩R′\\n1 and R2 ∩R′\\n2 are not\\nempty, then R1 ∩R′\\n2 ̸= ∅implies R′\\n1 ∩R2 ̸= ∅and |R1| · |R′\\n1|−1 = |R2| · |R′\\n2|−1. For a\\nﬁnite set M ⊂Zd, the graph Fc(M) has the intersecting ray property if all (m, m′) have the\\nintersecting ray property in F.\\nExample 4.14. The compressed ﬁber graph on [n1] × · · · × [nd] ⊂Zd that uses the unit\\nvectors {e1, . . . , ed} as moves has the intersecting ray property. On the other hand, consider\\nF = {u ∈N2 : u1 + u2 ≤1} and take the rays R1 := {(0, 0), (0, 1)} and R2 := {(1, 0)}\\nthat are parallel to e2 and the rays R′\\n1 := {(0, 1)} and R′\\n2 := {(0, 0), (1, 0)} that are parallel\\nto e1. Then R1 ∩R′\\n1 = {(1, 0)} and R2 ∩R′\\n2 = {(0, 1)}, but R1 ∩R′\\n2 = {(0, 0)} ̸= ∅and\\nR′\\n1 ∩R2 = ∅.\\nProposition 4.15. Let m, m′ ∈Zd not collinear and F ⊂Zd be a ﬁnite set. The matrices\\nHπ\\nF,m and Hπ\\nF,m′ commute if and only if (m, m′) have the intersecting ray property in F.\\nProof. Let u1, u2 ∈F. Then\\n(Hπ\\nF,m · Hπ\\nF,m′)u1,u2 =\\n(\\n|RF,m(u1)|−1 · |RF,m′(u2)|−1,\\nif RF,m(u1) ∩RF,m′(u2) ̸= ∅\\n0,\\notherwise\\n.\\nLet R1 := RF,m(u1), R′\\n1 := RF,m′(u1), R2 := RF,m(u2), and R′\\n2 := RF,m′(u2) Thus,\\n(Hπ\\nF,m · Hπ\\nF,m′)u1,u2 = (Hπ\\nF,m′ · Hπ\\nF,m)u1,u2. It is easy to see that the matrices commute if and\\nonly if (m, m′) have the intersecting ray property.\\n□\\nLemma 4.16. Let H1, . . . , Hn ∈Rn×n be pairwise commuting matrices. Then any eigenvalue\\nof Pn\\ni=1 Hi has the form λ1 + · · · + λn where λi is an eigenvalue of Hi.\\nProof. This is a straightforward extension of the case n = 2 in [12, Theorem 2.4.8.1] and\\nrelies on the fact that commuting matrices are simultaneously triangularizable.\\n□\\nHEAT-BATH RANDOM WALKS WITH MARKOV BASES\\n13\\nProposition 4.17. Let F ⊂Zd and M ⊂Zd be ﬁnite sets and suppose there exists m ∈M\\nsuch that (m, m′) has the intersecting ray property in F for all m′ ∈M′ := M \\\\ {m}. Let\\nV1, . . . , Vc be the connected components of F(M′), πi : Vi →[0, 1] the uniform distribution,\\nand f ′ = (1 −f(m))−1 · f|M′, then\\nλ(Hπ,f\\nF,M) ≤f(m) + (1 −f(m)) · max{λ(Hπi,f′\\nVi,M′) : i ∈[c]}.\\nProof. Let H := Hπ,f′\\nF,M′ be the heat-bath random walk on F(M) that samples moves from M′\\naccording to f ′, then Hπ,f\\nF,M = f(m)·Hπ\\nF,m +(1−f(m))·H. By assumption, all pairs (m, m′)\\nwith m′ ∈M′ have the intersecting ray property and thus the matrices Hπ\\nF,m and H commute\\naccording to Proposition 4.15. The eigenvalues of all involved matrices are non-negative and\\nthus Lemma 4.16 implies that the second largest eigenvalue of Hπ,f\\nF,M has the form λ + λ′\\nwhere λ ∈{0, f(m)} by Proposition 4.6 and where λ′ is an eigenvalue of (1 −f(m)) · H. The\\nmatrix H is a block matrix whose building blocks are the matrices Hπ,f′\\nVi,M′ = Hπi,f′\\nVi,M′ and thus\\nthe statement follows.\\n□\\nProposition 4.18. Let F ⊂Zd and M ⊂Zk be ﬁnite sets. If F(M) has the intersecting\\nray property, then λ(Hπ,f\\nF,M) ≤1 −min(f).\\nProof. Let M = {m1, . . . , mk}.\\nThe intersecting ray property and Proposition 4.15 give\\nthat the matrices f(m1) · Hπ\\nF,mi, . . . , f(mk) · Hπ\\nF,mk commute pairwise. According to Propo-\\nsition 4.6, the eigenvalues of f(mi) · Hπ\\nF,mi are {0, f(mi)}. Lemma 4.16 gives that the second\\nlargest eigenvalue of Hπ,f\\nF,M, which equals the second largest eigenvalue modulus since all of\\nits eigenvalues are non-negative, fulﬁlls λ(Hπ,f\\nF,M) = P\\ni∈I f(mi) for a subset I ⊆[k]. Since\\nλ(Hπ,f\\nF,M) < 1 and Pk\\ni=1 f(mi) = 1, we have I ̸= [k] and the claim follows.\\n□\\nProposition 4.19. Let n1, . . . , nd ∈N>1, F = [n1]×· · ·×[nd], and M = {e1, . . . , ed}. Then\\nfor any positive mass function f : M →[0, 1], λ(Hπ,f\\nF,M) = 1 −min(f).\\nProof. It is easy to verify that Fc(M) has the intersecting ray property and thus Proposi-\\ntion 4.18 shows λ(Hπ,f\\nF,M) ≤1−min(f). Assume that min(f) = f(ei). The connected compo-\\nnents of Fc({e1, . . . , ed} \\\\ {ei}) are the layers Vj := {u ∈F : ui = j} for any j ∈[ni] and the\\nrays through F parallel are Rk := {(0, k)+s·ei : s ∈[ni]} for k = (k1, . . . , ki−1, ki+1, . . . , kd) ∈\\n[n1] × · · · × [ni−1] × [ni+1] × · · · × [nd]. In particular, any ray intersects any connected com-\\nponent exactly once.\\nThus, the matrix (|Rk ∩Vj|)k,j is the all-ones matrix, which has a\\nnon-trivial kernel. Proposition 4.12 implies λ(Hπ,f\\nF,M) ≥1 −f(ei).\\n□\\nRemark 4.20. In the special case n := n1 = · · · = nd and f : {e1, . . . , ed} →[0, 1] the\\nuniform distribution in Proposition 4.19, the heat-bath random walk on [n]d is known as\\nRook’s walk in the literature. In this case, Proposition 4.19 is exactly [13, Proposition 2.3].\\nIn [18], upper bounds on the mixing time of the Rook’s walk were obtained with path-coupling.\\nThe stationary distribution of the heat-bath random walk is independent of the actual\\nmass function on the Markov moves. The problem of ﬁnding the mass function which leads\\n14\\nCAPRICE STANLEY AND TOBIAS WINDISCH\\nto the fastest mixing behaviour can be formulated as the following optimization problem:\\n(4.2)\\narg min\\n(\\nλ(Hπ,f\\nF,M) : f : M →(0, 1),\\nX\\nm∈M\\nf(m) = 1\\n)\\n.\\nIt follows from Proposition 4.19 that the optimal value of (4.2) for F = [n1] × · · · × [nd],\\nM = {e1, . . . , ed}, and the uniform distribution π on F is the uniform distribution on M.\\nAnother example where the uniform distribution is the optimal solution to (4.2), but where\\nthe veriﬁcation is more involved, is presented in Example 4.21.\\nExample 4.21. Let F = [2] × [5] as in Example 4.4 and consider M = {e1, 2e1 + e2}. We\\ninvestigate for which µ ∈(0, 1), the transition matrix µHπ\\nF,e1 + (1 −µ)Hπ\\nF,2e1+e2 has the\\nsmallest second largest eigenvalue modulus. Its characteristic polynomial in Q[µ, x] is\\n−1\\n25x4(x −1)(µ + x −1)6(−5x2 + 5x + 2µ2 −2µ)(−5x2 + 5x + 4µ2 −4µ)\\nand hence its eigenvalues are\\nx1(µ) := 1,\\nx2(µ) := 1 −µ,\\nx3(µ) := 1\\n2\\n\"\\n1 +\\nr\\n1 + 8\\n5(µ2 −µ)\\n#\\n,\\nx4(µ) := 1\\n2\\n\"\\n1 −\\nr\\n1 + 8\\n5(µ2 −µ)\\n#\\n,\\nx5(µ) := 1\\n2\\nh\\n1 +\\np\\n1 + 4(µ2 −µ)\\ni\\n,\\nx6(µ) := 1\\n2\\nh\\n1 −\\np\\n1 + 4(µ2 −µ)\\ni\\n.\\nIt is straightforward to check that x5(µ) > 1\\n2 > x6(µ), x3(µ) > 1\\n2 > x4(µ). Since µ2 −µ < 0\\nfor u ∈(0, 1) and x3(µ) ≥x6(µ). We can show that x4(µ) ≥x2(µ) and thus\\nλ(µHπ\\nF,e1 + (1 −µ)Hπ\\nF,2e1+e2) = 1\\n2\\n\"\\n1 +\\nr\\n1 + 8\\n5(µ2 −µ)\\n#\\n.\\nThe fastest heat-bath random walk on F(M) which converges to uniform is thus obtained for\\nµ = 1\\n2, i.e. when the moves are selected uniformly. The second largest eigenvalue in this case\\nis\\n1\\n10(5 +\\n√\\n15) ≈0.887, which is larger than the second largest eigenvalue of the heat-bath\\nwalk that selects uniformly from {e1, e2} (see Proposition 4.19).\\n5. Augmenting Markov bases\\nIt follows from our investigation in Section 3 that the diameter of all compressed ﬁber\\ngraphs coming from a ﬁxed integer matrix A ∈Zm×d can be bounded from above by a\\nconstant. However, Markov moves can be used twice in a minimal path which can make the\\ndiameter of the compressed ﬁber graph larger than the size of the Markov basis. The next\\ndeﬁnition puts more constraints on the Markov basis and postulates the existence of a path\\nthat uses every move from the Markov basis at most once.\\nDeﬁnition 5.1. Let F ⊂Zd be a ﬁnite set and M = {m1, . . . , mk} ⊂Zd. An augmenting\\npath between distinct u, v ∈F of length r ∈N is a path in Fc(M) of the form\\nu →u + λi1mi1 →u + λi1mi1 + λi2mi2 →· · · →u +\\nr\\nX\\nk=1\\nλikmik = v\\nHEAT-BATH RANDOM WALKS WITH MARKOV BASES\\n15\\nwith distinct indices i1, . . . , ir ∈[k]. An augmenting path is minimal for u, v ∈F if there\\nexists no shorter augmenting path between u and v in Fc(M).\\nA Markov basis M for\\nF is augmenting if there is an augmenting path between any distinct nodes in F.\\nThe\\naugmentation length AM(F) of an augmenting Markov basis M is the maximum length of\\nall minimal augmenting paths in Fc(M).\\nNot every Markov basis is augmenting (see Example 3.11), but the diameter of compressed\\nﬁber graphs that use an augmenting Markov basis is at most the number of the moves. For\\nﬁber graphs coming from an integer matrix, an augmenting Markov basis for all of its ﬁbers\\ncan be computed (Remark 5.2).\\nRemark 5.2. Let A ∈Zm×d with kerZ(A) ∩Nd = {0} and let b ∈NA. The Graver basis\\nis an augmenting Markov basis for FA,b for any b ∈NA. We claim that when A is totally\\nunimodular, then AGA(FA,b) ≤d2(rank(A) + 1).\\nIn particular, the augmentation length\\nis independent of the right-hand side b.\\nLet u, v ∈FA,b be arbitrary and for i ∈N, let\\nli := min{ui, vi}, wi := max{ui, vi}, and ci := sign(ui −vi) ∈{−1, 0, 1}. Then v is the unique\\noptimal value of the linear integer optimization problem\\nmin{cT x : Ax = b, l ≤x ≤w, x ∈Zd}.\\nA discrete steepest decent as deﬁned in [5, Deﬁnition 3] using Graver moves needs at most\\n∥c∥1 · d · (rank(A) + 1) ≤d2 · (rank(A) + 1) many augmentations from u to reach the optimal\\nvalue v. We refer to [5, Corollary 8] which ensures that every Graver move is used at most\\nonce. Note that in [5], x is constrained to x ≥0 instead to x ≥l, but their argument works\\nfor any lower bound.\\nExample 5.3. Fix d ∈N and consider A and M from Example 3.5. We show that M is an\\naugmenting Markov basis for FA,b for any b ∈N. Let u, v ∈FA,b be distinct, then there exists\\ni ∈[d] such that ui > vi or ui < vi, thus, we can walk from u to u′ := u + (ui −vi)(e1 −ei)\\nor from v to v′ := v + (vi −ui)(e1 −ei). In any case, after that augmentation, the pairs\\n(u′, v) and (v′, u) coincide in the ith coordinate and thus we ﬁnd an augmenting path by\\ninduction on the dimension d. We have used at most d −1 many edges in these paths and\\nhence AM(FA,b) ≤d −1 for all b ∈N.\\nWe now show that the augmentation length is essentially bounded from below by the\\ndimension of the node set and hence the bound observed in Example 5.3 cannot be improved.\\nWe ﬁrst need the following lemma.\\nLemma 5.4. Let v1, . . . , vk ∈Qd such that any v ∈spanQ {v1, . . . , vk} can be represented by\\na linear combination of r vectors. Then dim(spanQ {v1, . . . , vk}) ≤r.\\nProof. Let B ⊂P(v1, . . . , vk) the set of all subsets of cardinality r. By our assumption,\\n∪B∈BspanQ {B} = spanQ {v1, . . . , vk}. Since dim(spanQ {B}) ≤r for all B ∈B and since B\\nis ﬁnite, the claim follows.\\n□\\nProposition 5.5. Let P ⊂Qd be polytope and let M ⊂Zd be an augmenting Markov basis\\nfor Fi := (i · P) ∩Zd for all i ∈N. Then dim(P) ≤maxi∈N AM(Fi).\\n16\\nCAPRICE STANLEY AND TOBIAS WINDISCH\\nProof. Without restricting generality, we can assume that 0 ∈P. Let V := spanQ {P} be\\nthe Q-span of P, then dim(P) = dim(V ). We must have dim(spanQ {M}) = dim(V ) since\\ndim(P) = dim(convQ(Fi)) for i suﬃciently large and since M is a Markov basis for Fi. Deﬁne\\nr := maxi∈N AM(Fi) and choose any non-zero v ∈V and u ∈relint(P) ⊂Qd. Then there\\nexists δ ∈Q>0 such that u + δv ∈P. Thus, 1\\nδu + v ∈1\\nδP. Let c ∈N≥1 such that i := c\\nδ ∈N\\nand w := c\\nδu ∈Zd. Then w + cv = c(1\\nδ u + v) ∈(i · P) ∩Zd = Fi. By assumption, there exists\\nan augmenting path from w to w + cv using only r elements from M. Put diﬀerently, the\\nelement cv from V can be represented by a linear combination of r vectors from M. Since v\\nwas chosen arbitrarily, Lemma 5.4 implies dim(P) = dim(V ) ≤r.\\n□\\nRemark 5.6. It is a consequence from Proposition 5.5 that for any matrix A ∈Zm×d with\\nkerZ(A) ∩Nd = {0} and an augmenting Markov basis M, there exists F ∈PA such that\\nAM(F) ≥dim(kerZ(A)).\\nLet us now shortly recall the framework from [23] which is necessary to prove our main\\ntheorem. Let G = (V, E) be a graph. For any ordered pair of distinct nodes (x, y) ∈V × V ,\\nlet px,y ⊆E be a path from x to y in G and let Γ := {px,y : (x, y) ∈V × V, x ̸= y} be\\nthe collection of these paths, then Γ is a set of canonical paths. Let for any edge e ∈E,\\nΓe := {p ∈Γ : e ∈p} be the set of paths from Γ that use e. Now, let H : V × V →[0, 1] be a\\nsymmetric random walk on G and deﬁne\\nρ(Γ, H) := max{|p| : p ∈Γ}\\n|V |\\n· max\\n{u,v}∈E\\n|Γ{u,v}|\\nH(u, v).\\nObserve that symmetry of H is needed to make ρ(Γ, H) well-deﬁned. This can be used to\\nprove the following upper bound on the second largest eigenvalue.\\nLemma 5.7. Let G be a graph, H be a symmetric random walk on G, and Γ be a set of\\ncanonical paths in G. Then λ2(H) ≤1 −\\n1\\nρ(Γ,H).\\nProof. The stationary distribution of H is the uniform distribution and thus the statement\\nis a direct consequence of [23, Theorem 5], since ρ(Γ, H) is an upper bound on the constant\\ndeﬁned in [23, equation 4].\\n□\\nTheorem 5.8. Let F ⊂Zd be ﬁnite and let M := {m1, . . . , mk} ⊂Zd be an augmenting\\nMarkov basis. Let π be the uniform and f be a positive distribution on F and M respectively.\\nFor i ∈[k], let ri := max{|RF,mi(u)| : u ∈F} and suppose that r1 ≥r2 ≥· · · ≥rk. Then\\nλ(Hπ,f\\nM,F) ≤1 −\\n|F| · min(f)\\nAM(F) · AM(F)! · 3AM(F)−1 · 2|M| · r1r2 · · · rAM(F)\\n.\\nProof. Choose for any distinct u, v ∈F an augmenting path pu,v of minimal length in Fc(M)\\nand let Γ be the collection of all these paths. Let u + µmk = v be an edge in Fc(M), then\\nour goal is to bound |Γ{u,v}| from above. Let S := {S ⊆[r] : |S| ≤AM(F), k ∈S} and take\\nany path px,y ∈Γ{u,v}. Then there exists S := {i1, . . . , is} with s := |S| ≤AM(F) such that\\nx + Ps\\nk=1 λikmik = y. Since px,y uses the edge {u, v}, there is j ∈[s] such that ij = k and\\nλij = µ. Since |λik| ≤rik, there are at most\\ns! · (2ri1 + 1) · · · (2rij−1 + 1) · (2rij+1 + 1) · · · (2ris + 1) ≤s! · 3s−1\\nY\\nt∈S\\\\{k}\\nrt\\nHEAT-BATH RANDOM WALKS WITH MARKOV BASES\\n17\\npaths in Γ{u,v} that uses the edge {u, v} and the moves mi1, . . . , mij−1, mij+1 . . . , mis. Since\\nall the paths are minimal, they have length at most AM(F) so indeed every path in Γ has\\nthat form.\\n|Γu,v|\\nHπ,f\\nF,M(u, v)\\n≤3AM(F)−1\\nP\\nS∈S\\n\\x10\\n|S|! Q\\nt∈S\\\\{k} rt\\n\\x11\\nf(mij) ·\\n1\\n|Rmij (u)|\\n≤3AM(F)−1 · AM(F)! · |S| · r1r2 . . . rAM(F)\\nf(mij)\\n,\\nwhere we have used the assumption r1 ≥r2 ≥· · · ≥rk. Bounding |S| rigorously from above\\nby 2|M|, the claim follows from Lemma 5.7.\\n□\\nDeﬁnition 5.9. Let F ⊂Zd and M ⊂Zd be ﬁnite sets. The longest ray through F along\\nvectors of M is RF,M := arg max{|RF,m(u)| : m ∈M, u ∈F}.\\nCorollary 5.10. Let (Fi)i∈N be a sequence of ﬁnite sets in Zd and let πi be the uniform\\ndistribution on Fi. Let M ⊂Zd be an augmenting Markov basis for Fi with AM(Fi) ≤\\ndim(Fi) and suppose that (|RFi,M|)dim(Fi))i∈N ∈O(|Fi|)i∈N.\\nThen for any positive mass\\nfunction f : M →[0, 1], there exists ǫ > 0 such that λ(Hπi,f\\nFi,M) ≤1 −ǫ for all i ∈N.\\nProof. This is a straightforward application of Theorem 5.8.\\n□\\nCorollary 5.11. Let P ⊂Zd be a polytope, Fi := (i · P) ∩Zd for i ∈N, and let πi be the\\nuniform distribution on Fi. Suppose that M ⊂Zd is an augmenting Markov basis {Fi : i ∈N}\\nsuch that AM(Fi) ≤dim(P) for all i ∈N. Then for any positive mass function f : M →[0, 1],\\nthere exists ǫ > 0 such that λ(Hπi,f\\nFi,M) ≤1 −ǫ for all i ∈N.\\nProof. Let r := dim(P). We ﬁrst show that (|RFi,M|)i∈N ∈O(i)i∈N. Write M = {m1, . . . , mk}\\nand denote by li := max{|(u + mi · Z) ∩P| : u ∈P} be the length of the longest ray through\\nthe polytope P along mi. It suﬃces to prove that i · (lk + 1) is an upper bound on the\\nlength of any ray along mk through Fi. For that, let u ∈Fi such that u + λmk ∈Fi for\\nsome λ ∈N, then 1\\ni u + λ\\ni mk ∈P and thus ⌊λ\\ni ⌋≤lk, which gives λ ≤i · (lk + 1). With\\nC := max{l1, . . . , lk} + 1 we have |RFi,M| ≤C · i. Ehrhart’s theorem [2, Theorem 3.23]\\ngives (|Fi|)i∈N ∈Ω(ir)i∈N and since |RFi,M| ≤C · i, we have (|RFi,M|r)i∈N ∈O(|Fi|)i∈N. An\\napplication of Corollary 5.10 proves the claim.\\n□\\nExample 5.12. Fix d, r ∈N and let Cd,r := {u ∈Zd : ∥u∥1 ≤r} be the set of integers of the\\nd-dimensional cross-polytope with radius r. The set Md = {e1, . . . , ed} is a Markov basis for\\nCd,r for any r ∈N. We show that Md is an augmenting Markov basis whose augmentation\\nlength is at most d. For that, let u, v ∈Cd,r distinct elements. We claim that there exists\\ni ∈[d] such that xi ̸= vi and ui + (vi −ui) ∈Cd,r. Let S ⊆[d] be the set of indices where u\\nand v diﬀer and let s = r −||u||1. If |S| = 1, then the result is clear so suppose |S| ≥2. If\\nthe result doesn’t hold then for all i ∈S, |vi| −|ui| > s. It follows that\\n∥v∥1 =\\nX\\ni/∈S\\n|ui| +\\nX\\ni∈S\\n|vi| >\\nX\\ni/∈S\\n|ui| +\\nX\\ni∈S\\ns + |ui| = |Suv| · s + ∥u∥1 = (|S| −1) · s + r.\\nBut we assumed that v ∈Cd,r. It follows that for any pair of points u, v in Cd,r, there is a\\nwalk, using the unit vectors as moves, that uses each move at most once. Corollary 5.10 yield\\nthat for any d ∈N, the second largest eigenvalue modulus of the heat-bath random walk on\\nCd,r with uniform as stationary distribution can be strictly bounded away from 1 for r →∞.\\n18\\nCAPRICE STANLEY AND TOBIAS WINDISCH\\nThe bound on the second largest eigenvalue in Theorem 5.8 is quite general and can be\\nimproved vastly, provided one has better control over the paths. For example, this can be\\nachieved for hyperrectangles intersected with a halfspace.\\nProposition 5.13. Let a ∈Nd\\n>0, b ∈N, F = {u ∈Nd : aT · u ≤b}, and M := {e1, . . . , ed}.\\nIf π and f are the uniform distributions on F and M respectively, then\\nλ(Hπ,f\\nF,M) ≤1 −|F|\\nd2\\nd\\nY\\ni=1\\nai\\nb .\\nProof. Observe that M is a Markov basis for F since all nodes are connected with 0 ∈F.\\nLet u, v ∈F be distinct. We ﬁrst show that there exists k ∈[d] such that uk ̸= vk and\\nu + (vk −uk)ek ∈F. If u ≤v, the statement trivially holds. Otherwise, there exists k ∈[d]\\nsuch that uk > vk and the vector obtained by replacing the kth coordinate of u by vk remains\\nin F. Now, consider for the following path between u and v: Choose the smallest index\\nk ∈[d] such that uk ̸= vk and such that u + (vk −uk) · ek ∈F and proceed recursively with\\nu + (vk −uk) and v. This gives a path pu,v between u and v of length at most d. Let Γ be\\nthe collection of all these paths. We want to apply Lemma 5.7. Thus, let x ∈F and consider\\nthe edge x →x + c · es. Let us count the paths pu,v that use that edge. Let u, v ∈F and let\\nk1, . . . , kr ∈[d] be distinct indices such that\\nu →u + (vk1 −uk1)ek1 →u + (vk1 −uk1)ek1 + (vk2 −uk2)ek2 →· · · →v\\nrepresents the path pu,v constructed by the upper rule.\\nAssume that pu,v uses the edge\\n{x, x + ces} and let kl = s and (vkl −ukl) = c. In particular,\\nu + (vk1 −uk1)ek1 + · · · + (vkl−1 −ukl−1)ekl−1 = x\\nx + (vkl −ukl)ekl + · · · + (vkr −ukr)ekr = v.\\nWe see that vkt = xkt for all t < l and that ukt = xkt for all t ≥l. In particular, vkl =\\nukl + c = xkl + c is also ﬁxed. The coordinates ukt and vkt are bounded from above by\\nb\\nakt\\nfor all t ∈[r], and hence there can be at most\\n l−1\\nY\\nt=1\\nb\\nakt\\n!\\n·\\n \\nr\\nY\\nt=l+1\\nb\\nakt\\n!\\n.\\nSince k1, . . . , kt are distinct coordinate indices, we have\\n|Γx,x+c·es|\\nHπ,f\\nF,M(x, x + c · es)\\n≤d ·\\nd\\nY\\ni=1\\nb\\nai\\n.\\nLemma 5.7 ﬁnishes the proof.\\n□\\nIn ﬁxed dimension, Proposition 5.13 leads to rapid mixing, but for d →∞, no statement\\ncan be made. In [19], it was shown that the simple walk with an additional halting probability\\non {u ∈Nd : atu ≤b} ∩{0, 1}d has mixing time in O(d4.5+ǫ). For zero-one polytopes, simple\\nand heat-bath walk coincide and we are conﬁdent that a similar statement holds without the\\nrestriction on zero-one polytopes.\\nHEAT-BATH RANDOM WALKS WITH MARKOV BASES\\n19\\nThe heat-bath random walk mixes rapidly when an augmenting Markov basis with a small\\naugmentation length is used.\\nWe think that it is interesting to question how might an\\naugmenting Markov bases be obtained and how their augmentation length can be improved.\\nQuestion 5.14. Let M be an augmenting Markov basis of A. Can we ﬁnd ﬁnitely many\\nmoves m1, . . . , mk such that the augmentation length of M∪{m1, . . . , mk} on FA,b is at most\\ndim(kerZ(A)) for all b ∈NA?\\nReferences\\n1. Stephen Baumert, Archis Ghate, Seksan Kiatsupaibul, Yanfang Shen, Robert L. Smith, and Zelda B.\\nZabinsky, Discrete Hit-and-Run for Sampling Points from Arbitrary Distributions Over Subsets of Integer\\nHyperrectangles, Operations Research 57 (2009), no. 3, 727–739.\\n2. Matthias Beck and Sinai Robins, Computing the Continuous Discretely, Springer, New York, 2007.\\n3. Mary Cryan, Martin Dyer, Leslie Ann Goldberg, Mark Jerrum, and Russell Martin, Rapidly Mixing\\nMarkov Chains for Sampling Contingency Tables with a Constant Number of Rows, SIAM Journal on\\nComputing 36 (2006), no. 1, 247–278.\\n4. Jes´us A. De Loera, Raymond Hemmecke, and Matthias K¨oppe, Algebraic and Geometric Ideas in the\\nTheory of Discrete Optimization, MPS-SIAM Series on Optimization, SIAM, Cambridge, 2013.\\n5. Jes´us A. De Loera, Raymond Hemmecke, and Jon Lee, On Augmentation Algorithms for Linear and\\nInteger-Linear Programming: From Edmonds–Karp to Bland and Beyond, SIAM Journal on Optimization\\n25 (2015), no. 4, 2494–2511.\\n6. Persi Diaconis and Bernd Sturmfels, Algebraic algorithms for sampling from conditional distributions, The\\nAnnals of statistics 26 (1998), no. 1, 363–397.\\n7. Mathias Drton, Bernd Sturmfels, and Seth Sullivant, Lectures on algebraic statistics, Oberwolfach Semi-\\nnars, vol. 39, Springer, Berlin, 2009, A Birkh¨auser book.\\n8. Martin Dyer, Catherine Greenhill, and Mario Ullrich, Structure and eigenvalues of heat-bath Markov\\nchains, Linear Algebra and its Applications 454 (2014), 57–71.\\n9. Giles Gardam, Expander Graphs and Kazhdan’ s Property (T), Bachelor’s thesis, University of Sidney,\\n2012.\\n10. Hisayuki Hara, Akimichi Takemura, and Ruriko Yoshida, On connectivity of ﬁbers with positive marginals\\nin multiple logistic regression, Journal of Multivariate Analysis (2010), 1–26.\\n11. Raymond Hemmecke and Peter N. Malkin, Computing generating sets of lattice ideals and Markov bases\\nof lattices, Journal of Symbolic Computation 44 (2009), no. 10, 1463–1476.\\n12. Roger A. Horn, Matrix Analysis, 2nd ed., Cambridge University Press, New York, 2013.\\n13. Steven S. Kim, Mixing Time of a Rook’s Walk, Undergraduate certiﬁcate paper (2012).\\n14. David A. Levin, Yuval Peres, and Elisabeth L. Wilmer, Markov chains and mixing times, American\\nMathematical Society, Providence, RI, 2009.\\n15. L´aszl´o Lov´asz, Hit-and-run mixes fast, Mathematical Programming 86 (1999), no. 3, 443–461.\\n16. L´aszl´o Lov´asz and Santosh Vempala, Hit-and-Run from a Corner, SIAM Journal on Computing 35 (2006),\\nno. 4, 985–1005.\\n17. Peter N. Malkin, Computing Markov bases, Gr¨obner bases, and extreme rays, Phd thesis, 2007, p. 223.\\n18. Cam McLeman, Peter T. Otto, John Rahmani, and Matthew Sutter, Mixing times for the Rook’s walk\\nvia path coupling, to appear in Involve (2016), 1–12.\\n19. Ben Morris and Alistair Sinclair, Random Walks on Truncated Cubes and Sampling 0-1 Knapsack Solutions,\\nSIAM Journal on Computing 34 (2004), no. 1, 195–226.\\n20. Samu Potka, Higher connectivity of ﬁber graphs of Gr¨obner bases, Journal of Algebraic Statistics 4 (2013),\\nno. 1, 93–107.\\n21. Johannes Rauh and Seth Sullivant, Lifting Markov bases and higher codimension toric ﬁber products,\\nJournal of Symbolic Computation 74 (2016), 276–307.\\n22. Andr´as Seb¨o, Hilbert Bases, Caratheodory’s Theorem and Combinatorial Optimization, (1990), 431–455.\\n20\\nCAPRICE STANLEY AND TOBIAS WINDISCH\\n23. Alistair Sinclair, Improved Bounds for Mixing Rates of Markov Chains and Multicommodity Flow, Com-\\nbinatorics, Probability and Computing 1 (1992), no. 4, 351–370.\\n24. Bernd Sturmfels, Gr¨obner bases and convex polytopes, American Mathematical Society, Providence, R.I.,\\n1996.\\n25. Seth Sullivant, Markov bases of binary graph models, Annals of Combinatorics 7 (2003), 441–466.\\n26. Santosh S. Vempala, Geometric Random Walks: A Survey, MSRI Combinatorial and Computational\\nGeometry 52 (2005), 573–612.\\n27. Tobias Windisch, Rapid mixing and Markov bases, preprint, arXiv:1505.03018 (2015), 1–18.\\nNC State University, Raleigh, NC 27695, USA\\nE-mail address: crstanl2@ncsu.edu\\nOtto-von-Guericke Universit¨at, Magdeburg, Germany\\nE-mail address: windisch@ovgu.de\\n')]\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# Arxiv data ingestion (research paper source)\n",
    "from langchain_community.document_loaders import ArxivLoader\n",
    "\n",
    "arxiv_loader = ArxivLoader(\n",
    "    query=\"1605.08386\",\n",
    "    load_max_docs=3\n",
    ").load()\n",
    "print(arxiv_loader)\n",
    "print(len(arxiv_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'Published': '2016-05-26', 'Title': 'Heat-bath random walks with Markov bases', 'Authors': 'Caprice Stanley, Tobias Windisch', 'Summary': 'Graphs on lattice points are studied whose edges come from a finite set of\\nallowed moves of arbitrary length. We show that the diameter of these graphs on\\nfibers of a fixed integer matrix can be bounded from above by a constant. We\\nthen study the mixing behaviour of heat-bath random walks on these graphs. We\\nalso state explicit conditions on the set of moves so that the heat-bath random\\nwalk, a generalization of the Glauber dynamics, is an expander in fixed\\ndimension.'}, page_content='arXiv:1605.08386v1  [math.CO]  26 May 2016\\nHEAT-BATH RANDOM WALKS WITH MARKOV BASES\\nCAPRICE STANLEY AND TOBIAS WINDISCH\\nAbstract. Graphs on lattice points are studied whose edges come from a ﬁnite set of\\nallowed moves of arbitrary length. We show that the diameter of these graphs on ﬁbers of a\\nﬁxed integer matrix can be bounded from above by a constant. We then study the mixing\\nbehaviour of heat-bath random walks on these graphs. We also state explicit conditions\\non the set of moves so that the heat-bath random walk, a generalization of the Glauber\\ndynamics, is an expander in ﬁxed dimension.\\nContents\\n1.\\nIntroduction\\n1\\n2.\\nGraphs and statistics\\n3\\n3.\\nBounds on the diameter\\n4\\n4.\\nHeat-bath random walks\\n8\\n5.\\nAugmenting Markov bases\\n14\\nReferences\\n19\\n1. Introduction\\nA ﬁber graph is a graph on the ﬁnitely many lattice points F ⊂Zd of a polytope where\\ntwo lattice points are connected by an edge if their diﬀerence lies in a ﬁnite set of allowed\\nmoves M ⊂Zd. The implicit structure of these graphs makes them a useful tool to explore\\nthe set of lattice points randomly: At the current lattice point u ∈F, an element m ∈±M\\nis sampled and the random walk moves along m if u + m ∈F and stays at u otherwise.\\nThe corresponding Markov chain is irreducible if the underlying ﬁber graph is connected and\\nthe set M is called a Markov basis for F in this case. This paper investigates the heat-bath\\nversion of this random walk: At the current lattice point u ∈F, we sample m ∈M and move\\nto a random element in the integer ray (u + Z · m) ∩F. The authors of [6] discovered that\\nthis random walk can be seen as a discrete version of the hit-and-run algorithm [15, 26, 16]\\nthat has been used frequently to sample from all the points of a polytope – not only from its\\nlattice points. The popularity of the continuous version of the hit-and-run algorithm has not\\nspread to its discrete analog, and not much is known about its mixing behaviour. One reason\\nis that it is already challenging to guarantee that all points in the underlying set F can be\\nDate: September 12, 2018.\\n2010 Mathematics Subject Classiﬁcation. Primary: 05C81, Secondary: 37A25, 11P21.\\nKey words and phrases. Heat-bath random walks, sampling, lattice points, Markov bases.\\n1\\n2\\nCAPRICE STANLEY AND TOBIAS WINDISCH\\nreached by a random walk that uses moves from M, whereas for the continuous version, a\\nrandom sampling from the unit sphere suﬃces. However, in many situations where a Markov\\nbasis is known, the heat-bath random walk is evidently fast. For instance, it was shown in [3]\\nthat the heat-bath random walk on contingency tables mixes rapidly when the number of\\ncolumns is ﬁxed. To work around the connectedness issue, a discrete hit-and-run algorithm\\nwas introduced in [1] for arbitrary ﬁnite sets F ⊂Zd. At each step in this random walk, a\\nsubordinate and unrestricted random walk starts at the current lattice point u ∈F and uses\\nthe unit vectors to collect a set of proposals S ⊂Zd. The random walk then moves from u\\nto a random point in S ∩F.\\nRandom walks of the heat-bath type, such as the one presented above, have been studied\\nrecently in [8] in a more general context. In this paper, we explore the mixing behaviour of\\nheat-bath random walks on the lattice points of polytopes with Markov bases. Throughout,\\nwe assume that a Markov basis has been found already and refer to the relevant literature\\nfor their computation [24, 25, 11, 17, 10, 21]. We call the underlying graph of the heat-bath\\nrandom walk a compressed ﬁber graph (Deﬁnition 2.5) and determine in Section 3 bounds on\\nits graph-diameter. We prove that for any A ∈Zm×d with kerZ(A)∩Nd = {0}, the diameter of\\ncompressed ﬁber graphs on {u ∈Nd : Au = b} that use a ﬁxed Markov bases M ⊂kerZ(A) is\\nbounded from above by a constant as b varies (Theorem 3.15). In contrast, we show that the\\ndiameter of conventional ﬁber graphs grow linearly under a dilation of the underlying polytope\\n(Remark 3.9). This gives rise to slow mixing results for conventional ﬁber walks as observed\\nin [27]. In Section 4, we study in more detail the combinatorial and analytical structure\\nof the transition matrices of heat-bath random walks on lattice points and prove upper and\\nlower bounds on their second largest eigenvalues. We also discuss how the distribution on the\\nmoves M aﬀects the speed of convergence (Example 4.21). Theorem 5.8 establishes with the\\ncanonical path approach from [23] an upper bound on the second largest eigenvalue when the\\nMarkov basis is augmenting (Deﬁnition 5.1) and the stationary distribution is uniform. From\\nthat, we conclude fast mixing results for random walks on lattice points in ﬁxed dimension.\\nAcknowledgements. CS was partially supported by the US National Science Foundation\\n(DMS 0954865). TW gratefully acknowledges the support received from the German National\\nAcademic Foundation.\\nConventions and Notation. The natural numbers are N := {0, 1, 2, . . .} and for any N ∈N,\\nN>N := {n ∈N : n > N} and N≥N := {N} ∪N>N. For n ∈N>0, let [n] := {1, . . . , n}. Let\\nM ⊂Qd be a ﬁnite set, then Z·M := {λm : m ∈M, λ ∈Z} and NM is the aﬃne semigroup\\nin Zd generated by M. For an integer matrix A ∈Zm×d with columns a1, . . . , ad ∈Zm,\\nwe write NA := N{a1, . . . , ad}. A graph is always undirected and can have multiple loops.\\nThe distance of two nodes u, v which are contained in the same connected component of a\\ngraph G, i.e. the number of edges in a shortest path between u and v in G, is denoted by\\ndistG(u, v). We set distG(u, v) := ∞if u and v are disconnected. A mass function on a ﬁnite\\nset Ωis a map f : Ω→[0, 1] such that P\\nω∈Ωf(ω) = 1. A mass function f on Ωis positive\\nif f(ω) > 0 for all ω ∈Ω. A set F ⊂Zd is normal if it there exists a polytope P ⊂Qd such\\nthat P ∩Zd = F.\\nHEAT-BATH RANDOM WALKS WITH MARKOV BASES\\n3\\n2. Graphs and statistics\\nWe ﬁrst introduce the statistical framework in which this paper lives and recall important\\naspects of the interplay between graphs and statistics. A random walk on a graph G = (V, E)\\nis a map H : V × V →[0, 1] such that for all v ∈V , P\\nu∈V H(v, u) = 1 and such that\\nH(v, u) = 0 if {v, u} ̸∈E. When there is no ambiguity, we represent a random walk as an\\n|V | × |V |-matrix, for example when it is clear how the elements of V are ordered. Fix a\\nrandom walk H on G. Then H is irreducible if for all v, u ∈V there exists t ∈N such that\\nHt(v, u) > 0. The random walk H is reversible if there exists a mass function µ : V →[0, 1]\\nsuch that µ(u) · H(u, v) = µ(v) · H(v, u) for all u, v ∈V and symmetric if H is a symmetric\\nmap. A mass function π : V →[0, 1] is a stationary distribution of H if π ◦H = π. For\\nsymmetric random walks, the uniform distribution on V is always a stationary distribution.\\nIf |V | = n, then we denote the eigenvalues of H by 1 = λ1(H) ≥λ2(H) ≥· · · ≥λn(H) ≥−1\\nand we write λ(H) := max{λ2(H), −λn(H)} for the second largest eigenvalue modulus of H.\\nAny irreducible random walk has a unique stationary distribution [14, Corollary 1.17] and\\nλ(H) ∈[0, 1] measures the convergence rate: the smaller λ(H), the faster the convergence.\\nThe aim of this paper is to study random walks on lattice points that use a set of moves.\\nTypically, this is achieved by constructing a graph on the set of lattice points as follows\\n(compare to [7, Section 1.3] and [24, Chapter 5]).\\nDeﬁnition 2.1. Let F ⊂Zd be a ﬁnite set and M ⊂Zd. The graph F(M) is the graph on\\nF where two nodes u, v ∈F are adjacent if u −v ∈M or v −u ∈M.\\nA normal set F ⊂Zd is ﬁnite and satisﬁes F = convQ(F)∩Zd. A canonical class of normal\\nsets that arise in many applications, is given by the ﬁbers of an integer matrix:\\nDeﬁnition 2.2. Let A ∈Zm×d and b ∈NA. The set FA,b := {u ∈Nd : Au = b} is the b-ﬁber\\nof A. The collection of all ﬁbers of A is PA := {FA,b : b ∈NA}. For M ⊂kerZ(A), the graph\\nFA,b (M) is a ﬁber graph.\\nLet F, M ⊂Zd be ﬁnite. If the membership in F can be veriﬁed eﬃciently – for instance\\nwhen F is given implicitly by linear equations and inequalities – then it is possible to explore\\nF randomly using M as follows: At a given node v ∈F, a uniform element m ∈M is\\nselected. If v + m ∈M, then the random walk moves along m to v + m and if v + m ̸∈M,\\nthe we stay at v. Formally, we obtain the following random walk.\\nDeﬁnition 2.3. Let F ⊂Zd and M ⊂Zd be two ﬁnite sets. The simple walk is the random\\nwalk on F(M) where the probability to traverse between to adjacent nodes u and v is |±M|−1\\nand the probability to stay at a node u is |{m ∈±M : u + m ̸∈F}| · | ± M|−1.\\nThe simple walk is symmetric and hence the uniform distribution is a stationary distribu-\\ntion (see also [27, Section 2]). To ensure convergence, the random walk has to be irreducible,\\nthat is, the underlying graph has to be connected. The following deﬁnition is a slight adaption\\nof the generalized Markov basis as deﬁned in [21, Deﬁnition 1].\\nDeﬁnition 2.4. Let P be a collection of ﬁnite subsets of Zd. A ﬁnite set M ⊂Zd is a\\nMarkov basis of P, if for all F ∈P, F(M) is a connected graph.\\n4\\nCAPRICE STANLEY AND TOBIAS WINDISCH\\nWe refer to [6, Theorem 3.1] for a proof that for collections PA, a ﬁnite Markov basis\\nalways exists and can be computed with tools from commutative algebra (see also [11] for\\nmore on the computation of Markov bases). We now introduce a construction of graphs on\\nlattice points that also give rise to implementable random walks, but whose edges have far\\nmore reach.\\nDeﬁnition 2.5. Let F ⊂Zd and M ⊂Zd be ﬁnite sets. The compression of the graph\\nF(M) is the graph Fc(M) := F(Z · M).\\nFigure 1. Compressing graphs.\\nCompressing a graph F(M) preserves its connectedness: F(M) is connected if and only\\nif Fc(M) is connected.\\n3. Bounds on the diameter\\nIn general knowledge of the diameter of the graph underlying a Markov chain can provide\\ninformation about the mixing time. For random walks on ﬁber graphs, the chains which we\\nconsider, the underlying graph coincides with the ﬁber graph. In this section, we determine\\nlower and upper bounds on the diameter of ﬁber graphs and their compressed counterparts.\\nFor a ﬁnite set M ⊂Zd and any norm ∥· ∥on Rd, let ∥M∥:= maxm∈M ∥m∥.\\nLemma 3.1. Let F ⊂Zd and M ⊂Zd be ﬁnite sets, then\\ndiam(F(M)) ≥\\n1\\n∥M∥· max{∥u −v∥: u, v ∈F}.\\nProof. If F(M) is not connected, then the statement holds trivially, so assume that M is a\\nMarkov basis for F. Let u′, v′ ∈F such that ∥u′ −v′∥= max{∥u −v∥: u, v ∈F} and let\\nm1, . . . , mr ∈M so that u′ = v′+Pr\\ni=1 mi is a path of minimal length, then ∥u′−v′∥≤r·∥M∥\\nand the claim follows from diam(F(M)) ≥distF(M)(u′, v′) = r.\\n□\\nRemark 3.2. Let F ⊂Zd be a normal set. For all l ∈{−1, 0, 1}d and u, v ∈F we have\\n(u −v)T l ≤∥u −v∥1 and thus widthl(F) := max{(u −v)T l : u, v ∈F} ≤max{∥u −v∥1 :\\nu, v ∈F}. Suppose that u′, v′ ∈F are such that ∥u′ −v′∥1 = max{∥u −v∥1 : u, v ∈F} and\\nlet l′\\ni := sign(u′\\ni −v′\\ni) for i ∈[d], then\\n∥u′ −v′∥1 = (u′ −v′)T · l′ ≤widthl′(F) ≤max{∥u −v∥1 : u, v ∈F} = ∥u′ −v′∥1.\\nThe lattice width of F is width(F) := minl∈Zd widthl(F) and thus Lemma 3.1 gives\\n∥M∥1 · diam(F(M)) ≥width(F).\\nHEAT-BATH RANDOM WALKS WITH MARKOV BASES\\n5\\nDeﬁnition 3.3. Let P be a collection of ﬁnite subsets of Zd.\\nA ﬁnite set M ⊂Zd is\\nnorm-like for P if there exists a constant C ∈N such that for all F ∈P and all u, v ∈F,\\ndistF(M)(u, v) ≤C · ∥u −v∥. The set M is ∥· ∥-norm-reducing for P if for all F ∈P and all\\nu, v ∈F there exists m ∈M such that u + m ∈F and ∥u + m −v∥< ∥u −v∥.\\nThe property of being norm-like does not depend on the norm, whereas being norm-\\nreducing does.\\nNorm-reducing sets are always norm-like, and norm-like sets are in turn\\nalways Markov bases, but the reverse of both statements is false in general (Example 3.4 and\\nExample 3.5). For collections PA however, every Markov basis is norm-like (Proposition 3.7).\\nExample 3.4. For any n ∈N, consider the normal set Fn := ([2]×[n]×{0})∪{(2, n, 1)} with\\nthe Markov basis {(0, 1, 0), (0, 0, 1), (−1, 0, −1)}. The distance between (1, 1, 0) and (2, 1, 0)\\nin Fn(M) is 2n and thus M is not norm-like for {Fn : n ∈N} (see also Figure 2).\\nExample 3.5. Let d ∈N and consider A := (1, . . . , 1) ∈Z1×d, then the set M := {e1 −ei :\\n2 ≤i ≤d} is a Markov basis for the collection PA. However, M is not ∥·∥p-norm-reducing for\\nany d ≥3 and any p ∈[1, ∞]. For instance, consider e2 and e3 in FA,1 (M). The only move\\nfrom M that can be applied on e2 is e1−e2, but ∥(e2+e1−e2)−e3)∥p = ∥e2−e3∥p. On the other\\nhand, in the case we cannot ﬁnd a move that decreases the 1-norm of two nodes u, v ∈FA,b\\nby 1, we can ﬁnd instead two moves m1, m2 ∈M such that u + m1, u + m1 + m2 ∈FA,b and\\n∥u + m1 + m2 −v∥= ∥u −v∥−2. Thus, the graph-distance of any two elements u and v in\\nFA,b (M) is at most ∥u −v∥1 and hence M is norm-like for PA.\\nFigure 2. The graph from Example 3.4\\nRemark 3.6. Let P be a collection of ﬁnite subsets of Zd and M ⊂Zd be norm-like for P.\\nIt follows from the deﬁnition that there exists a constant C ∈Q≥0 such that for all F ∈P\\ndiam(F(M)) ≤C · max{∥u −v∥: u, v ∈F}.\\nThe proof of our next results uses the Graver basis GA ⊂Zd for an integer matrix A ∈Zm×d\\nwith kerZ(A) ∩Nd = {0}. We refer to [4, Chapter 3] for a precise deﬁnition.\\nProposition 3.7. Let A ∈Zm×d with kerZ(A) ∩Nd = {0} and M ⊂kerZ(A) be a Markov\\nbasis of PA. Then M is norm-like for PA.\\nProof. Let M be a Markov basis for PA. The Graver basis GA for A is a ﬁnite set which\\nis ∥· ∥1-norm-reducing for PA. Thus, deﬁne C := maxg∈GA diam(FA,Ag+ (M)). Now, pick\\nu, v ∈FA,b arbitrarily and let u = v + Pr\\ni=1 gi be a walk from u to v in FA,b (GA) of minimal\\nlength. Since the Graver basis is norm-reducing for FA,b, there always exists a path of length\\nat most ∥u −v∥1 and hence r ≤∥u −v∥1. Every gi can be replaced by a path in FA,Ag+\\ni (M)\\n6\\nCAPRICE STANLEY AND TOBIAS WINDISCH\\nof length at most C and these paths stay in FA,b. This gives a path of length C · r, hence\\ndistFA,b(M)(u, v) ≤C∥u −v∥1.\\n□\\nProposition 3.8. Let P ⊂Zd be a polytope with dim(P ∩Zd) > 0 and let M be a Markov\\nbasis for Fi := (i · P) ∩Zd for all i ∈N. There exists a constant C′ ∈Q>0 such that for all\\ni ∈N, C′ · i ≤diam(Fi(M)). If M is norm-like for {Fi : i ∈N}, then there exists a constant\\nC ∈Q>0 such that diam(Fi(M)) ≤C · i for all i ∈N.\\nProof. For the lower bound on the diameter, it suﬃces to show the existence of C′ such that\\nC′ · i ≤max{∥u −v∥: u, v ∈Fi} for all i ∈N due to Lemma 3.1. Since dim(P ∩Zd) > 0,\\nwe can pick distinct w, w′ ∈P ∩Zd. For all i ∈N, i · w, i · w′ ∈Fi and hence i · ∥w −w′∥≤\\nmax{∥u −v∥: u, v ∈Fi}.\\nTo show the upper bound, assume that M is norm-like. It suﬃces to show that there\\nexists C ∈Q≥0 such that max{∥u −v∥: u, v ∈Fi} ≤i · C by Remark 3.6.\\nNow, let\\nv1, . . . , vr ∈Qd such that P = convQ(v1, . . . , vr) and deﬁne C := max{∥vs −vt∥: s ̸= t}.\\nSince Fi = (i·P)∩Zd ⊂convQ(iv1, . . . , ivr) for all i ∈N, we have max{∥u−v∥: u, v ∈Fi} ≤\\nmax{∥ivs −ivt∥: s ̸= t} ≤C · i.\\n□\\nRemark 3.9. Let A ∈Zm×n with kerZ(A) ∩Nd = {0} and let M be a Markov basis for PA.\\nThen M is norm-like due to Proposition 3.7 and thus for all b ∈NA there exists C, C′ ∈Q≥0\\nsuch that\\ni · C′ ≤diam(FA,ib (M)) ≤i · C\\nfor all i ∈N. This generalizes for instance [20, Proposition 2.10] and [27, Example 4.7], where\\nlinear diameters on a ray in NA have been observed. This also implies that the construction\\nof expanders from [27, Section 4] works for every right-hand side b ∈NA.\\nRemark 3.10. Let A ∈Zm×d with kerZ(A)∩Nd = {0}, b ∈NA, and let M be a Markov basis\\nfor PA. Proposition 3.8 provides a new proof that the simple walk on (FA,ib (M))i∈N cannot\\nmix rapidly. The lower bound on the diameter from Proposition 3.8 implies, in general, the\\nfollowing upper bound on the edge-expansion (see for example [9, Proposition 1.30]):\\nh(FA,i·b (M)) ≤|M|\\n\\x12\\nexp\\n\\x12log |FA,i·b|\\nD · i\\n\\x13\\n−1\\n\\x13\\n.\\nIn particular, the edge-expansion cannot be bounded from below by Ω( 1\\np(i))i∈N for a polyno-\\nmial p ∈Q[t] and since (|FA,i·b|)i∈N ∈O(ir)i∈N, the simple walk cannot mix rapidly. In [27],\\nit was shown that the edge-expansion can be bounded from above by O(1\\ni )i∈N, which cannot\\nbe concluded from the upper expression.\\nWe now turn our attention to the diameter of compressed ﬁber graphs.\\nIn particular,\\nwe want to know for which collections of normal sets is their diameter bounded. In general,\\ncompressing a ﬁber graph does not necessarily have an eﬀect on the diameter (Example 3.11).\\nAlthough a low diameter is a necessary condition for good mixing, it is not suﬃcient. For\\ninstance, let Gn be the disjoint union of two complete graphs Kn connected by a single edge.\\nThen diam(Gn) = 3, but h(Gn) ≤1\\nn implies that the simple walk does not mix rapidly.\\nHEAT-BATH RANDOM WALKS WITH MARKOV BASES\\n7\\nExample 3.11. For any n ∈N, let Fn := {(0, 0), (0, 1), (1, 1), (1, 2), . . . , (n, n)} ⊂Z2. The\\nunit vectors M = {e1, e2} are a Markov basis for {Fn : n ∈N}. However, Fc\\nn(M) = Fn(M)\\nand thus diam(Fc\\nn(M)) = diam(Fn(M)) = 2n is unbounded.\\nLemma 3.12. Let A ∈Zm×d and z ∈kerZ(A). There exists r ∈[2d −2], distinct elements\\ng1, . . . , gr ∈GA, and λ1, . . . , λr ∈N>0 such that z = Pr\\ni=1 λigi and gi ⊑z for all i ∈[r]\\nProof. This is [4, Lemma 3.2.3], although it only becomes clear from the original proof of [22,\\nTheorem 2.1] that the appearing elements are all distinct.\\n□\\nProposition 3.13. Let A ∈Zm×d and P :=\\n\\x08\\n{x ∈Zd : Ax = b, l ≤x ≤u} : l, u ∈Zd, b ∈Zm\\t\\n.\\nThen for all F ∈P, diam(Fc(GA)) ≤2d −2.\\nProof. Let s, t ∈{x ∈Zd : Ax = b, l ≤x ≤u}, then s−t ∈kerZ(A) and thus s = t+Pr\\ni=1 λigi\\nwith r ≤2d −2, λ1, . . . , λr ∈N>0, and distinct g1, . . . , gr ∈GA such that gi ⊑s −t according\\nto Lemma 3.12. It’s now a consequence from [4, Lemma 3.2.4] that all intermediate points\\nt + Pk\\ni=1 λigi for k ≤r are in {x ∈Zd : Ax = b, l ≤x ≤u}.\\n□\\nLemma 3.14. Let F ⊂Zd be ﬁnite and let Fi := (i · convQ(F)) ∩Zd for i ∈N. For all\\nu, v ∈F, distFc\\ni (M)(iu, iv) ≤distF(M)(u, v) for all i ∈N.\\nProof. The statement is trivially true if u and v are disconnected in F(M). Thus, assume\\nthe contrary and let u = v + Pk\\nj=1 mj with mj ∈M be a path in F(M) of length k =\\ndistF(M)(u, v) and let i ∈N. Clearly, i · u = i · v + i · Pk\\nj=1 mj = i · v + Pk\\nl=1 i · mj, so\\nit is left to prove that the elements traversed by this paths are in Fi. Let l ∈[k], since\\nv + Pl\\nj=1 mj ∈F, we have i · v + Pl\\nj=1 i · mj ∈i · F ⊆Fi. Hence, this is a path in Fc\\ni (M)\\nof length k = distF(M)(u, v).\\n□\\nWe are ready to prove that the diameter of compressed ﬁber graphs coming from an integer\\nmatrix can be bounded for all right-hand sides simultaneously.\\nTheorem 3.15. Let A ∈Zm×d with kerZ(A) ∩Nd = {0} and let M be a Markov basis for\\nPA. There exists a constant C ∈N such that diam(Fc(M)) ≤C for all F ∈PA.\\nProof. Our proof relies on basic properties of the Graver basis GA of A. For any g ∈GA,\\nlet Fg := FA,Ag+ and let K := max{distFg(M)(g+, g−) : g ∈GA}.\\nWe show that the\\ndiameter of any compressed ﬁber graph of A is bounded from above by (2d −2) · K. Let\\nb ∈NA arbitrary and choose elements u, v ∈FA,b. According to Proposition 3.13, there\\nexists r ∈[2d −2], g1, . . . , gr ∈GA and λ1, . . . , λr ∈Z such that u = v + Pr\\ni=1 λigi, and\\nv + Pl\\ni=1 λigi ∈Nd for all l ∈[r].\\nAccording to Lemma 3.14, for any i ∈[r] there are\\nmi\\n1, . . . , mi\\nki ∈M and α1, . . . , αki ∈Z such that λig+\\ni = λig−\\ni + Pki\\nj=1 αjmi\\nj is a path in the\\ncompression of FA,Aλig+\\ni (M) of length ki ≤K. Lifting these paths for every i ∈[r] yields a\\npath u = v + Pr\\ni=1\\nPki\\nj=1 αjmi\\nj in Fc\\nA,b (M) of length r · K ≤(2d −2) · K.\\n□\\n8\\nCAPRICE STANLEY AND TOBIAS WINDISCH\\n4. Heat-bath random walks\\nIn this section, we establish the heat-bath random walk on compressed ﬁber graphs. We\\nrefer to [8] for a more general introduction on random walks of heat-bath type. Let F ⊂Zd\\nbe ﬁnite set. For any u ∈F and m ∈Zd, the ray in F through u along m is denoted by\\nRF,m(u) := (u + m · Z) ∩F. Additionally, given a mass function π : F →[0, 1], we deﬁne\\nHπ\\nF,m(x, y) :=\\n(\\nπ(y)\\nπ(RF,m(x))\\n, if y ∈RF,m(x)\\n0\\n, otherwise\\nfor x, y ∈F. For M ⊂Zd and a mass function f : M →[0, 1], the heat-bath random walk is\\n(4.1)\\nHπ,f\\nF,M =\\nX\\nm∈M\\nf(m) · Hπ\\nF,m.\\nThe underlying graph of the heat-bath random walk is the compression Fc(M) and in this\\nsection, we assume throughout that for all m ∈M and λ ∈Z \\\\ {−1, 1}, λ · m ̸∈M. Let us\\nﬁrst recall the basic properties of this random walk (compare also to [6, Lemma 2.2]).\\nAlgorithm 1 Heat-bath random walk on compressed ﬁber graphs\\nInput: F ⊂Zd, M ⊂Zd, v ∈F, mass functions f : M →[0, 1] and π : F →[0, 1], r ∈N\\n1: procedure HeatBath:\\n2:\\nv0 := v\\n3:\\nFOR s = 0; s = s + 1, s < r\\n4:\\nSample m ∈M according to f\\n5:\\nSample vs+1 ∈RF,m(vs) according to RF,m(vs) →[0, 1], y 7→\\nπ(y)\\nπ(RF,m(vs))\\n6:\\nRETURN v1, . . . , vr\\nProposition 4.1. Let F ⊂Zd and M ⊂Zd be ﬁnite sets. Let f : M →[0, 1] and π : F →\\n(0, 1) be mass functions. Then Hπ,f\\nF,M is aperiodic, has stationary distribution π, is reversible\\nwith respect to π, and all of its eigenvalues are non-negative. The random walk is irreducible\\nif and only if {m ∈M : f(m) > 0} is a Markov basis for F.\\nProof. Since for any u ∈F and any m ∈M, Hπ\\nF,m(u, u) > 0, there are halting states and\\nthus Hπ,f\\nF,M is aperiodic. By deﬁnition, π(x)Hπ\\nF,m(x, y) = π(y)Hπ\\nF,m(y, x) and thus Hπ,f\\nF,M\\nis reversible with respect to π and π is a stationary distribution.\\nThe statement on the\\neigenvalues is exactly [8, Lemma 1.2]. Let M′ = {m ∈M : f(m) > 0} and f ′ = f|M′, then\\nHπ,f\\nF,M = Hπ,f′\\nF,M′ and thus the heat-bath random walk is irreducible if and only if M′ is a\\nMarkov basis for F.\\n□\\nRemark 4.2. Analyzing the speed of convergence of random walks with second largest\\neigenvalues does not take the computation time of a single transition into account. From\\na computational point of view, the diﬀerence of the simple walk and the heat-bath random\\nwalk is Step 4 of Algorithm 1. However, we argue that Step 4 can be done eﬃciently in\\nmany cases. For instance, a hard normalizing constant of π cancels out. If π is the uniform\\ndistribution, then one needs to sample uniformly from RF,m(v) in Step 4, which can be done\\nHEAT-BATH RANDOM WALKS WITH MARKOV BASES\\n9\\neﬃciently. If the input of Algorithm 1 is a normal set F = {u ∈Zd : Au ≤b} that is given\\nin H-representation, then the length of the ray RF,m(v) can be computed with a number of\\nrounding, division, and comparing operations that is linear in the number of rows of A.\\nThere are situations in which the heat-bath random walk provides no speed-up compared\\nwith the simple walk (Example 4.3). Intuitively, adding more moves to the set of allowed\\nmoves should improve the mixing time of the random walk. In general, however, this is not\\ntrue for the heat-bath walk (Example 4.4).\\nExample 4.3. For n ∈N, consider the normal set\\nFn :=\\n\\x1a\\x14\\n0\\n1\\n1\\n· · ·\\n1\\n1\\n0\\n0\\n· · ·\\n0\\n\\x15\\n,\\n\\x14\\n1\\n0\\n1\\n· · ·\\n1\\n0\\n1\\n0\\n· · ·\\n0\\n\\x15\\n, . . . ,\\n\\x14\\n1\\n1\\n· · ·\\n1\\n0\\n0\\n0\\n· · ·\\n0\\n1\\n\\x15\\x1b\\n⊂Q2×n.\\nIn the language of [7, Section 1.1], Fn is precisely the ﬁber of the 2 × n independence model\\nwhere row sums are (n −1, 1) and column sums are (1, 1, . . . , 1).\\nThe minimal Markov\\nbasis of the independence model, often referred to as the basic moves, is precisely the set\\nMn := {v −u : u, v ∈Fn} \\\\ {0}. In particular, the ﬁber graph Fn(Mn) is the complete\\ngraph on n nodes. All rays along basic moves have length 2 and thus the transition matrices\\nof the simple random walk and the heat-bath random walk coincide. There are n · (n −1)\\nmany basic moves and the transition matrix of both random walks is\\n1\\nn(n −1)\\n\\uf8ee\\n\\uf8ef\\uf8f0\\n1\\n. . .\\n1\\n...\\n...\\n1\\n. . .\\n1\\n\\uf8f9\\n\\uf8fa\\uf8fb+ (n(n −1) −n)\\nn(n −1)\\n· In.\\nThe second largest eigenvalue is 1 −\\n1\\nn−1 which implies that for n →∞, neither the simple\\nwalk nor the heat-bath random walk are rapidly mixing.\\nExample 4.4. Let F = [2] × [5] ⊂Z2, M = {e1, e2, 2e1 + e2}, and let π be the uniform\\ndistribution on F.\\nSince {e2, 2e1 + e2} is not a Markov basis for F, any mass function\\nf : M →[0, 1] must have f(e1) > 0 in order to make the corresponding heat-bath random\\nwalk irreducible.\\nComparing the second largest eigenvalue modulus of heat-bath random\\nwalks that sample from {e1, e2} and M uniformly, we obtain\\nλ\\n\\x121\\n2Hπ\\nF,e1 + 1\\n2Hπ\\nF,e2\\n\\x13\\n= 1\\n2 < 2\\n3 = λ\\n\\x121\\n3Hπ\\nF,e1 + 1\\n3Hπ\\nF,e2 + 1\\n3Hπ\\nF,2e1+e2\\n\\x13\\n.\\nSo, adding 2e1 + e2 to the set of allowed moves slows the walk down. This phenomenon does\\nnot appear for the simple walk on F, where the second largest eigenvalue modulus improves\\nfrom ≈0.905 to ≈0.888 when adding the move 2e1 + e2 to the Markov basis.\\n=\\n+\\n+\\nFigure 3. Decomposition of the graph in Example 4.4\\n10\\nCAPRICE STANLEY AND TOBIAS WINDISCH\\nRemark 4.5. Let F ⊂Zd be ﬁnite and M = {m1, . . . , md} ⊂Zd be a linearly independent\\nMarkov basis of F. If the moves are selected uniformly, then the heat-bath random walk on\\nF coincides with the Glauber dynamics on F. To see it, choose u ∈F and let\\nF′ := {λ ∈Zd : u + λ1m1 + · · · + λdmd ∈F}.\\nIt is easy to check that F′ is unique up to translation and depends only on F, u, and M.\\nSince the vectors in M are linearly independent, every element of F can be represented by\\na unique choice of coeﬃcients in F′. Thus, the heat-bath random walk on F using M is\\nequivalent to the heat-bath random walk on on F′ using the unit vectors as moves. For any\\nunit vector ei ∈Zd, the ray through an element v ∈F′ is {w ∈F : wj = vj∀j ̸= i} and this\\nis precisely the form desired in the Glauber dynamics [14, Section 3.3.2].\\nFor the remainder of this section, we primarily focus on heat-bath random walks Hπ,f\\nF,M\\nthat converge to the uniform distribution π on a ﬁnite, but not necessarily normal, set\\nF. We particularly aim for bounds on its second largest eigenvalue by making use of the\\ndecomposition from equation 4.1. Our ﬁrst observations consider its summands Hπ\\nF,m that\\ncan be well understood analytically (Proposition 4.6) and combinatorially (Proposition 4.7).\\nProposition 4.6. Let F ⊂Zd be a ﬁnite set, m ∈Zd, and π : F →[0, 1] be the uniform\\ndistribution. Let R1, . . . , Rk be the disjoint rays through F along m. Then\\n1. Hπ\\nF,m is symmetric and idempotent.\\n2. img(Hπ\\nF,m) = spanR\\nnP\\nx∈R1 ex, P\\nx∈R2 ex, . . . , P\\nx∈Rk ex\\no\\n.\\n3. ker(Hπ\\nF,m) = Lk\\ni=1 spanR {ex −ey : x, y ∈Ri, x ̸= y}.\\n4. rank(Hπ\\nF,m) = k and dim ker(Hπ\\nF,m) = |F| −k.\\n5. The spectrum of Hπ\\nF,m is {0, 1}.\\nProof. Symmetry of Hπ\\nF,m follows from the deﬁnition. By assumption, F is the disjoint union\\nof R1, . . . , Rk and hence there exists a permutation matrix S such that SHπ\\nF,mST is a block\\nmatrix whose building blocks are the matrices\\n1\\n|Ri|\\n\\uf8ee\\n\\uf8ef\\uf8f0\\n1\\n. . .\\n1\\n...\\n...\\n1\\n. . .\\n1\\n\\uf8f9\\n\\uf8fa\\uf8fb∈Q|Ri|×|Ri|.\\nThus, Hπ\\nF,m is idempotent and the rank of Hπ\\nF,m is k. A basis of its image and its kernel can\\nbe read oﬀdirectly and idempotent matrices can only have the eigenvalues 0 and 1.\\n□\\nProposition 4.7. Let F ⊂Zd and M ⊂Zd be ﬁnite sets, π : F →[0, 1] be the uniform\\ndistribution, and let V1, . . . , Vc ⊆F be the nodes of the connected components of F(M), then\\n\\\\\\nm∈M\\nimg(Hπ\\nF,m) = spanR\\n\\uf8f1\\n\\uf8f2\\n\\uf8f3\\nX\\nx∈V1\\nex, . . . ,\\nX\\nx∈Vc\\nex\\n\\uf8fc\\n\\uf8fd\\n\\uf8fe.\\nProof. It is clear by Proposition 4.6 that the set on the right-hand side is contained in any\\nimg(Hπ\\nF,m) since any Vi decomposes disjointly into rays along m ∈M. To show the other\\ninclusion, write M = {m1, . . . , mk} and let for any i ∈[k], Ri\\n1, . . . , Ri\\nni be the disjoint rays\\nHEAT-BATH RANDOM WALKS WITH MARKOV BASES\\n11\\nthrough F parallel to mi. In particular, {Ri\\n1, . . . , Ri\\nni} is a partition of F for any i ∈[k]. Let\\nv ∈T\\nm∈M img(Hπ\\nF,m). Again by Proposition 4.6, there exists for any i ∈[k], λi\\n1, . . . , λi\\nni ∈Q\\nsuch that\\nv =\\nni\\nX\\nj=1\\nX\\nx∈Ri\\nj\\nλi\\njex.\\nNotice that if two distinct Markov moves mi and mi′ and two indices j ∈[ni] and j′ ∈[ni′]\\nsatisfy Ri\\nj ∩Ri′\\nj′ ̸= ∅, then λi\\nj = λi′\\nj′. We show that for any i ∈[k] and any a ∈[c], λi\\nj = λi\\nj′\\nwhen Ri\\nj and Ri\\nj′ are a subset of Va. This implies the proposition. So take distinct x, x′ ∈Va\\nand assume that x and x′ lie on diﬀerent rays of mi and let that be x ∈Ri\\nj and x′ ∈Ri\\nj′ with\\nj ̸= j′. Since x and x′ are in the same connected component Va of F(M), let yi0, . . . , yir ∈F\\nbe the nodes on a minimal path in Fc(M) with yi0 = x and yir = x′. For any s ∈[r], yis\\nand yis−1 are contained in the same ray Rks\\nts coming from a Markov move mks. In particular,\\nRts−1\\nks−1 ∩Rks\\nts ̸= ∅and due to our observation made above λi\\nj = λk1\\nt1 = λk2\\nt2 = · · · = λkr\\ntr = λi\\nj′\\nwhich ﬁnishes the proof.\\n□\\nDeﬁnition 4.8. Let F ⊂Zd and M ⊂Zd be ﬁnite sets and M′ ⊆M. Let V be the set of\\nconnected components of F(M\\\\M′) and R be the set of all rays through F along all elements\\nof M′. The ray matrix of F(M) along M′ is AF(M, M′) := (|R ∩V |)R∈R,V ∈V ∈NR×V.\\nExample 4.9. Let F = [3]×[3], M = {e1, e2, e1 +e2}, and M′ = {e1, e2}. Then F(M\\\\M′)\\nhas ﬁve connected components and the ray matrix of F(M) along M′ is\\nAF(M, M′) =\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n1\\n1\\n1\\n0\\n0\\n0\\n1\\n1\\n1\\n0\\n0\\n0\\n1\\n1\\n1\\n0\\n0\\n1\\n1\\n1\\n0\\n1\\n1\\n1\\n0\\n1\\n1\\n1\\n0\\n0\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n.\\nRemark 4.10. Let F ⊂Z2, then the rays through F along e1 are the connected components\\nof F({e1, e2} \\\\ {e2}) and the rays through F along e2 are the connected components of\\nF({e1, e2} \\\\ {e1}), thus AF(M, e1) = AF(M, e2)T .\\nProposition 4.11. Let F ⊂Zd and M ⊂Zd be ﬁnite sets, π : F →[0, 1] be the uniform\\ndistribution, and M′ ⊆M. Then\\nker(AF(M, M′)) ∼=\\n\\\\\\nm∈M\\\\M′\\nimg(Hπ\\nF,m) ∩\\n\\\\\\nm∈M′\\nker(Hπ\\nF,m).\\nProof. Let V1, . . . , Vc be the connected components of F(M \\\\ M′) and R1, . . . , Rr be the\\nrays along elements in M′. Let I := T\\nm∈M\\\\M′ img(Hπ\\nF,m) and K := T\\nm∈M′ ker(Hπ\\nF,m). By\\nProposition 4.7, any element of I has the form v = Pc\\ni=1(λi\\nP\\nx∈Vi ex) for λ1, . . . , λc ∈Q.\\nAssume additionally that v ∈ker(Hπ\\nF,m) for m ∈M′ and let Ri1, . . . Rij be the rays which\\nbelong to m, then for any k ∈[j], 0 = P\\nx∈Rik vx = Pc\\nj=1 λj|Rik∩Vj|. Put diﬀerently, a vector\\nλ ∈Rc is in the kernel of (|Ri ∩Vj|)i∈[r],j∈[c] if and only if Pc\\ni=1(λi\\nP\\nx∈Vi ex) ∈I ∩K.\\n□\\n12\\nCAPRICE STANLEY AND TOBIAS WINDISCH\\nConditions on the kernel of the ray matrix allow us to give a lower bound on the second\\nlargest eigenvalue of the heat-bath random walk.\\nProposition 4.12. Let F ⊂Zd and M ⊂Zd be ﬁnite sets and π be the uniform distribution.\\nLet M′ ⊆M such that ker(AF(M, M′)) ̸= {0}, then λ(Hπ,f\\nF,M) ≥1 −P\\nm∈M′ f(m) for any\\nmass function f : M →[0, 1].\\nProof. Using the isomorphism from Proposition 4.11, we can choose a non-zero v ∈QP such\\nthat Hπ\\nF,mv = v for all m ∈M \\\\ M′ and Hπ\\nF,mv = 0 for all m ∈M′. In particular\\nHπ,f\\nF,Mv =\\nX\\nm∈M\\nf(m)Hπ\\nF,mv =\\nX\\nm∈M\\\\M′\\nf(m)Hπ\\nF,mv =\\nX\\nm∈M\\\\M\\nf(m)v.\\nSince f is a mass function, 1 −P\\nm∈M′ f(m) is an eigenvalue of Hπ,f\\nF,M.\\n□\\nDeﬁnition 4.13. Let F ⊂Zd and m, m′ ∈Zd not collinear.\\nThe pair (m, m′) has the\\nintersecting ray property in F if the following holds: For any pair of rays R1, R2 parallel\\nto m and any pair of rays R′\\n1, R′\\n2 parallel to m′ where both R1 ∩R′\\n1 and R2 ∩R′\\n2 are not\\nempty, then R1 ∩R′\\n2 ̸= ∅implies R′\\n1 ∩R2 ̸= ∅and |R1| · |R′\\n1|−1 = |R2| · |R′\\n2|−1. For a\\nﬁnite set M ⊂Zd, the graph Fc(M) has the intersecting ray property if all (m, m′) have the\\nintersecting ray property in F.\\nExample 4.14. The compressed ﬁber graph on [n1] × · · · × [nd] ⊂Zd that uses the unit\\nvectors {e1, . . . , ed} as moves has the intersecting ray property. On the other hand, consider\\nF = {u ∈N2 : u1 + u2 ≤1} and take the rays R1 := {(0, 0), (0, 1)} and R2 := {(1, 0)}\\nthat are parallel to e2 and the rays R′\\n1 := {(0, 1)} and R′\\n2 := {(0, 0), (1, 0)} that are parallel\\nto e1. Then R1 ∩R′\\n1 = {(1, 0)} and R2 ∩R′\\n2 = {(0, 1)}, but R1 ∩R′\\n2 = {(0, 0)} ̸= ∅and\\nR′\\n1 ∩R2 = ∅.\\nProposition 4.15. Let m, m′ ∈Zd not collinear and F ⊂Zd be a ﬁnite set. The matrices\\nHπ\\nF,m and Hπ\\nF,m′ commute if and only if (m, m′) have the intersecting ray property in F.\\nProof. Let u1, u2 ∈F. Then\\n(Hπ\\nF,m · Hπ\\nF,m′)u1,u2 =\\n(\\n|RF,m(u1)|−1 · |RF,m′(u2)|−1,\\nif RF,m(u1) ∩RF,m′(u2) ̸= ∅\\n0,\\notherwise\\n.\\nLet R1 := RF,m(u1), R′\\n1 := RF,m′(u1), R2 := RF,m(u2), and R′\\n2 := RF,m′(u2) Thus,\\n(Hπ\\nF,m · Hπ\\nF,m′)u1,u2 = (Hπ\\nF,m′ · Hπ\\nF,m)u1,u2. It is easy to see that the matrices commute if and\\nonly if (m, m′) have the intersecting ray property.\\n□\\nLemma 4.16. Let H1, . . . , Hn ∈Rn×n be pairwise commuting matrices. Then any eigenvalue\\nof Pn\\ni=1 Hi has the form λ1 + · · · + λn where λi is an eigenvalue of Hi.\\nProof. This is a straightforward extension of the case n = 2 in [12, Theorem 2.4.8.1] and\\nrelies on the fact that commuting matrices are simultaneously triangularizable.\\n□\\nHEAT-BATH RANDOM WALKS WITH MARKOV BASES\\n13\\nProposition 4.17. Let F ⊂Zd and M ⊂Zd be ﬁnite sets and suppose there exists m ∈M\\nsuch that (m, m′) has the intersecting ray property in F for all m′ ∈M′ := M \\\\ {m}. Let\\nV1, . . . , Vc be the connected components of F(M′), πi : Vi →[0, 1] the uniform distribution,\\nand f ′ = (1 −f(m))−1 · f|M′, then\\nλ(Hπ,f\\nF,M) ≤f(m) + (1 −f(m)) · max{λ(Hπi,f′\\nVi,M′) : i ∈[c]}.\\nProof. Let H := Hπ,f′\\nF,M′ be the heat-bath random walk on F(M) that samples moves from M′\\naccording to f ′, then Hπ,f\\nF,M = f(m)·Hπ\\nF,m +(1−f(m))·H. By assumption, all pairs (m, m′)\\nwith m′ ∈M′ have the intersecting ray property and thus the matrices Hπ\\nF,m and H commute\\naccording to Proposition 4.15. The eigenvalues of all involved matrices are non-negative and\\nthus Lemma 4.16 implies that the second largest eigenvalue of Hπ,f\\nF,M has the form λ + λ′\\nwhere λ ∈{0, f(m)} by Proposition 4.6 and where λ′ is an eigenvalue of (1 −f(m)) · H. The\\nmatrix H is a block matrix whose building blocks are the matrices Hπ,f′\\nVi,M′ = Hπi,f′\\nVi,M′ and thus\\nthe statement follows.\\n□\\nProposition 4.18. Let F ⊂Zd and M ⊂Zk be ﬁnite sets. If F(M) has the intersecting\\nray property, then λ(Hπ,f\\nF,M) ≤1 −min(f).\\nProof. Let M = {m1, . . . , mk}.\\nThe intersecting ray property and Proposition 4.15 give\\nthat the matrices f(m1) · Hπ\\nF,mi, . . . , f(mk) · Hπ\\nF,mk commute pairwise. According to Propo-\\nsition 4.6, the eigenvalues of f(mi) · Hπ\\nF,mi are {0, f(mi)}. Lemma 4.16 gives that the second\\nlargest eigenvalue of Hπ,f\\nF,M, which equals the second largest eigenvalue modulus since all of\\nits eigenvalues are non-negative, fulﬁlls λ(Hπ,f\\nF,M) = P\\ni∈I f(mi) for a subset I ⊆[k]. Since\\nλ(Hπ,f\\nF,M) < 1 and Pk\\ni=1 f(mi) = 1, we have I ̸= [k] and the claim follows.\\n□\\nProposition 4.19. Let n1, . . . , nd ∈N>1, F = [n1]×· · ·×[nd], and M = {e1, . . . , ed}. Then\\nfor any positive mass function f : M →[0, 1], λ(Hπ,f\\nF,M) = 1 −min(f).\\nProof. It is easy to verify that Fc(M) has the intersecting ray property and thus Proposi-\\ntion 4.18 shows λ(Hπ,f\\nF,M) ≤1−min(f). Assume that min(f) = f(ei). The connected compo-\\nnents of Fc({e1, . . . , ed} \\\\ {ei}) are the layers Vj := {u ∈F : ui = j} for any j ∈[ni] and the\\nrays through F parallel are Rk := {(0, k)+s·ei : s ∈[ni]} for k = (k1, . . . , ki−1, ki+1, . . . , kd) ∈\\n[n1] × · · · × [ni−1] × [ni+1] × · · · × [nd]. In particular, any ray intersects any connected com-\\nponent exactly once.\\nThus, the matrix (|Rk ∩Vj|)k,j is the all-ones matrix, which has a\\nnon-trivial kernel. Proposition 4.12 implies λ(Hπ,f\\nF,M) ≥1 −f(ei).\\n□\\nRemark 4.20. In the special case n := n1 = · · · = nd and f : {e1, . . . , ed} →[0, 1] the\\nuniform distribution in Proposition 4.19, the heat-bath random walk on [n]d is known as\\nRook’s walk in the literature. In this case, Proposition 4.19 is exactly [13, Proposition 2.3].\\nIn [18], upper bounds on the mixing time of the Rook’s walk were obtained with path-coupling.\\nThe stationary distribution of the heat-bath random walk is independent of the actual\\nmass function on the Markov moves. The problem of ﬁnding the mass function which leads\\n14\\nCAPRICE STANLEY AND TOBIAS WINDISCH\\nto the fastest mixing behaviour can be formulated as the following optimization problem:\\n(4.2)\\narg min\\n(\\nλ(Hπ,f\\nF,M) : f : M →(0, 1),\\nX\\nm∈M\\nf(m) = 1\\n)\\n.\\nIt follows from Proposition 4.19 that the optimal value of (4.2) for F = [n1] × · · · × [nd],\\nM = {e1, . . . , ed}, and the uniform distribution π on F is the uniform distribution on M.\\nAnother example where the uniform distribution is the optimal solution to (4.2), but where\\nthe veriﬁcation is more involved, is presented in Example 4.21.\\nExample 4.21. Let F = [2] × [5] as in Example 4.4 and consider M = {e1, 2e1 + e2}. We\\ninvestigate for which µ ∈(0, 1), the transition matrix µHπ\\nF,e1 + (1 −µ)Hπ\\nF,2e1+e2 has the\\nsmallest second largest eigenvalue modulus. Its characteristic polynomial in Q[µ, x] is\\n−1\\n25x4(x −1)(µ + x −1)6(−5x2 + 5x + 2µ2 −2µ)(−5x2 + 5x + 4µ2 −4µ)\\nand hence its eigenvalues are\\nx1(µ) := 1,\\nx2(µ) := 1 −µ,\\nx3(µ) := 1\\n2\\n\"\\n1 +\\nr\\n1 + 8\\n5(µ2 −µ)\\n#\\n,\\nx4(µ) := 1\\n2\\n\"\\n1 −\\nr\\n1 + 8\\n5(µ2 −µ)\\n#\\n,\\nx5(µ) := 1\\n2\\nh\\n1 +\\np\\n1 + 4(µ2 −µ)\\ni\\n,\\nx6(µ) := 1\\n2\\nh\\n1 −\\np\\n1 + 4(µ2 −µ)\\ni\\n.\\nIt is straightforward to check that x5(µ) > 1\\n2 > x6(µ), x3(µ) > 1\\n2 > x4(µ). Since µ2 −µ < 0\\nfor u ∈(0, 1) and x3(µ) ≥x6(µ). We can show that x4(µ) ≥x2(µ) and thus\\nλ(µHπ\\nF,e1 + (1 −µ)Hπ\\nF,2e1+e2) = 1\\n2\\n\"\\n1 +\\nr\\n1 + 8\\n5(µ2 −µ)\\n#\\n.\\nThe fastest heat-bath random walk on F(M) which converges to uniform is thus obtained for\\nµ = 1\\n2, i.e. when the moves are selected uniformly. The second largest eigenvalue in this case\\nis\\n1\\n10(5 +\\n√\\n15) ≈0.887, which is larger than the second largest eigenvalue of the heat-bath\\nwalk that selects uniformly from {e1, e2} (see Proposition 4.19).\\n5. Augmenting Markov bases\\nIt follows from our investigation in Section 3 that the diameter of all compressed ﬁber\\ngraphs coming from a ﬁxed integer matrix A ∈Zm×d can be bounded from above by a\\nconstant. However, Markov moves can be used twice in a minimal path which can make the\\ndiameter of the compressed ﬁber graph larger than the size of the Markov basis. The next\\ndeﬁnition puts more constraints on the Markov basis and postulates the existence of a path\\nthat uses every move from the Markov basis at most once.\\nDeﬁnition 5.1. Let F ⊂Zd be a ﬁnite set and M = {m1, . . . , mk} ⊂Zd. An augmenting\\npath between distinct u, v ∈F of length r ∈N is a path in Fc(M) of the form\\nu →u + λi1mi1 →u + λi1mi1 + λi2mi2 →· · · →u +\\nr\\nX\\nk=1\\nλikmik = v\\nHEAT-BATH RANDOM WALKS WITH MARKOV BASES\\n15\\nwith distinct indices i1, . . . , ir ∈[k]. An augmenting path is minimal for u, v ∈F if there\\nexists no shorter augmenting path between u and v in Fc(M).\\nA Markov basis M for\\nF is augmenting if there is an augmenting path between any distinct nodes in F.\\nThe\\naugmentation length AM(F) of an augmenting Markov basis M is the maximum length of\\nall minimal augmenting paths in Fc(M).\\nNot every Markov basis is augmenting (see Example 3.11), but the diameter of compressed\\nﬁber graphs that use an augmenting Markov basis is at most the number of the moves. For\\nﬁber graphs coming from an integer matrix, an augmenting Markov basis for all of its ﬁbers\\ncan be computed (Remark 5.2).\\nRemark 5.2. Let A ∈Zm×d with kerZ(A) ∩Nd = {0} and let b ∈NA. The Graver basis\\nis an augmenting Markov basis for FA,b for any b ∈NA. We claim that when A is totally\\nunimodular, then AGA(FA,b) ≤d2(rank(A) + 1).\\nIn particular, the augmentation length\\nis independent of the right-hand side b.\\nLet u, v ∈FA,b be arbitrary and for i ∈N, let\\nli := min{ui, vi}, wi := max{ui, vi}, and ci := sign(ui −vi) ∈{−1, 0, 1}. Then v is the unique\\noptimal value of the linear integer optimization problem\\nmin{cT x : Ax = b, l ≤x ≤w, x ∈Zd}.\\nA discrete steepest decent as deﬁned in [5, Deﬁnition 3] using Graver moves needs at most\\n∥c∥1 · d · (rank(A) + 1) ≤d2 · (rank(A) + 1) many augmentations from u to reach the optimal\\nvalue v. We refer to [5, Corollary 8] which ensures that every Graver move is used at most\\nonce. Note that in [5], x is constrained to x ≥0 instead to x ≥l, but their argument works\\nfor any lower bound.\\nExample 5.3. Fix d ∈N and consider A and M from Example 3.5. We show that M is an\\naugmenting Markov basis for FA,b for any b ∈N. Let u, v ∈FA,b be distinct, then there exists\\ni ∈[d] such that ui > vi or ui < vi, thus, we can walk from u to u′ := u + (ui −vi)(e1 −ei)\\nor from v to v′ := v + (vi −ui)(e1 −ei). In any case, after that augmentation, the pairs\\n(u′, v) and (v′, u) coincide in the ith coordinate and thus we ﬁnd an augmenting path by\\ninduction on the dimension d. We have used at most d −1 many edges in these paths and\\nhence AM(FA,b) ≤d −1 for all b ∈N.\\nWe now show that the augmentation length is essentially bounded from below by the\\ndimension of the node set and hence the bound observed in Example 5.3 cannot be improved.\\nWe ﬁrst need the following lemma.\\nLemma 5.4. Let v1, . . . , vk ∈Qd such that any v ∈spanQ {v1, . . . , vk} can be represented by\\na linear combination of r vectors. Then dim(spanQ {v1, . . . , vk}) ≤r.\\nProof. Let B ⊂P(v1, . . . , vk) the set of all subsets of cardinality r. By our assumption,\\n∪B∈BspanQ {B} = spanQ {v1, . . . , vk}. Since dim(spanQ {B}) ≤r for all B ∈B and since B\\nis ﬁnite, the claim follows.\\n□\\nProposition 5.5. Let P ⊂Qd be polytope and let M ⊂Zd be an augmenting Markov basis\\nfor Fi := (i · P) ∩Zd for all i ∈N. Then dim(P) ≤maxi∈N AM(Fi).\\n16\\nCAPRICE STANLEY AND TOBIAS WINDISCH\\nProof. Without restricting generality, we can assume that 0 ∈P. Let V := spanQ {P} be\\nthe Q-span of P, then dim(P) = dim(V ). We must have dim(spanQ {M}) = dim(V ) since\\ndim(P) = dim(convQ(Fi)) for i suﬃciently large and since M is a Markov basis for Fi. Deﬁne\\nr := maxi∈N AM(Fi) and choose any non-zero v ∈V and u ∈relint(P) ⊂Qd. Then there\\nexists δ ∈Q>0 such that u + δv ∈P. Thus, 1\\nδu + v ∈1\\nδP. Let c ∈N≥1 such that i := c\\nδ ∈N\\nand w := c\\nδu ∈Zd. Then w + cv = c(1\\nδ u + v) ∈(i · P) ∩Zd = Fi. By assumption, there exists\\nan augmenting path from w to w + cv using only r elements from M. Put diﬀerently, the\\nelement cv from V can be represented by a linear combination of r vectors from M. Since v\\nwas chosen arbitrarily, Lemma 5.4 implies dim(P) = dim(V ) ≤r.\\n□\\nRemark 5.6. It is a consequence from Proposition 5.5 that for any matrix A ∈Zm×d with\\nkerZ(A) ∩Nd = {0} and an augmenting Markov basis M, there exists F ∈PA such that\\nAM(F) ≥dim(kerZ(A)).\\nLet us now shortly recall the framework from [23] which is necessary to prove our main\\ntheorem. Let G = (V, E) be a graph. For any ordered pair of distinct nodes (x, y) ∈V × V ,\\nlet px,y ⊆E be a path from x to y in G and let Γ := {px,y : (x, y) ∈V × V, x ̸= y} be\\nthe collection of these paths, then Γ is a set of canonical paths. Let for any edge e ∈E,\\nΓe := {p ∈Γ : e ∈p} be the set of paths from Γ that use e. Now, let H : V × V →[0, 1] be a\\nsymmetric random walk on G and deﬁne\\nρ(Γ, H) := max{|p| : p ∈Γ}\\n|V |\\n· max\\n{u,v}∈E\\n|Γ{u,v}|\\nH(u, v).\\nObserve that symmetry of H is needed to make ρ(Γ, H) well-deﬁned. This can be used to\\nprove the following upper bound on the second largest eigenvalue.\\nLemma 5.7. Let G be a graph, H be a symmetric random walk on G, and Γ be a set of\\ncanonical paths in G. Then λ2(H) ≤1 −\\n1\\nρ(Γ,H).\\nProof. The stationary distribution of H is the uniform distribution and thus the statement\\nis a direct consequence of [23, Theorem 5], since ρ(Γ, H) is an upper bound on the constant\\ndeﬁned in [23, equation 4].\\n□\\nTheorem 5.8. Let F ⊂Zd be ﬁnite and let M := {m1, . . . , mk} ⊂Zd be an augmenting\\nMarkov basis. Let π be the uniform and f be a positive distribution on F and M respectively.\\nFor i ∈[k], let ri := max{|RF,mi(u)| : u ∈F} and suppose that r1 ≥r2 ≥· · · ≥rk. Then\\nλ(Hπ,f\\nM,F) ≤1 −\\n|F| · min(f)\\nAM(F) · AM(F)! · 3AM(F)−1 · 2|M| · r1r2 · · · rAM(F)\\n.\\nProof. Choose for any distinct u, v ∈F an augmenting path pu,v of minimal length in Fc(M)\\nand let Γ be the collection of all these paths. Let u + µmk = v be an edge in Fc(M), then\\nour goal is to bound |Γ{u,v}| from above. Let S := {S ⊆[r] : |S| ≤AM(F), k ∈S} and take\\nany path px,y ∈Γ{u,v}. Then there exists S := {i1, . . . , is} with s := |S| ≤AM(F) such that\\nx + Ps\\nk=1 λikmik = y. Since px,y uses the edge {u, v}, there is j ∈[s] such that ij = k and\\nλij = µ. Since |λik| ≤rik, there are at most\\ns! · (2ri1 + 1) · · · (2rij−1 + 1) · (2rij+1 + 1) · · · (2ris + 1) ≤s! · 3s−1\\nY\\nt∈S\\\\{k}\\nrt\\nHEAT-BATH RANDOM WALKS WITH MARKOV BASES\\n17\\npaths in Γ{u,v} that uses the edge {u, v} and the moves mi1, . . . , mij−1, mij+1 . . . , mis. Since\\nall the paths are minimal, they have length at most AM(F) so indeed every path in Γ has\\nthat form.\\n|Γu,v|\\nHπ,f\\nF,M(u, v)\\n≤3AM(F)−1\\nP\\nS∈S\\n\\x10\\n|S|! Q\\nt∈S\\\\{k} rt\\n\\x11\\nf(mij) ·\\n1\\n|Rmij (u)|\\n≤3AM(F)−1 · AM(F)! · |S| · r1r2 . . . rAM(F)\\nf(mij)\\n,\\nwhere we have used the assumption r1 ≥r2 ≥· · · ≥rk. Bounding |S| rigorously from above\\nby 2|M|, the claim follows from Lemma 5.7.\\n□\\nDeﬁnition 5.9. Let F ⊂Zd and M ⊂Zd be ﬁnite sets. The longest ray through F along\\nvectors of M is RF,M := arg max{|RF,m(u)| : m ∈M, u ∈F}.\\nCorollary 5.10. Let (Fi)i∈N be a sequence of ﬁnite sets in Zd and let πi be the uniform\\ndistribution on Fi. Let M ⊂Zd be an augmenting Markov basis for Fi with AM(Fi) ≤\\ndim(Fi) and suppose that (|RFi,M|)dim(Fi))i∈N ∈O(|Fi|)i∈N.\\nThen for any positive mass\\nfunction f : M →[0, 1], there exists ǫ > 0 such that λ(Hπi,f\\nFi,M) ≤1 −ǫ for all i ∈N.\\nProof. This is a straightforward application of Theorem 5.8.\\n□\\nCorollary 5.11. Let P ⊂Zd be a polytope, Fi := (i · P) ∩Zd for i ∈N, and let πi be the\\nuniform distribution on Fi. Suppose that M ⊂Zd is an augmenting Markov basis {Fi : i ∈N}\\nsuch that AM(Fi) ≤dim(P) for all i ∈N. Then for any positive mass function f : M →[0, 1],\\nthere exists ǫ > 0 such that λ(Hπi,f\\nFi,M) ≤1 −ǫ for all i ∈N.\\nProof. Let r := dim(P). We ﬁrst show that (|RFi,M|)i∈N ∈O(i)i∈N. Write M = {m1, . . . , mk}\\nand denote by li := max{|(u + mi · Z) ∩P| : u ∈P} be the length of the longest ray through\\nthe polytope P along mi. It suﬃces to prove that i · (lk + 1) is an upper bound on the\\nlength of any ray along mk through Fi. For that, let u ∈Fi such that u + λmk ∈Fi for\\nsome λ ∈N, then 1\\ni u + λ\\ni mk ∈P and thus ⌊λ\\ni ⌋≤lk, which gives λ ≤i · (lk + 1). With\\nC := max{l1, . . . , lk} + 1 we have |RFi,M| ≤C · i. Ehrhart’s theorem [2, Theorem 3.23]\\ngives (|Fi|)i∈N ∈Ω(ir)i∈N and since |RFi,M| ≤C · i, we have (|RFi,M|r)i∈N ∈O(|Fi|)i∈N. An\\napplication of Corollary 5.10 proves the claim.\\n□\\nExample 5.12. Fix d, r ∈N and let Cd,r := {u ∈Zd : ∥u∥1 ≤r} be the set of integers of the\\nd-dimensional cross-polytope with radius r. The set Md = {e1, . . . , ed} is a Markov basis for\\nCd,r for any r ∈N. We show that Md is an augmenting Markov basis whose augmentation\\nlength is at most d. For that, let u, v ∈Cd,r distinct elements. We claim that there exists\\ni ∈[d] such that xi ̸= vi and ui + (vi −ui) ∈Cd,r. Let S ⊆[d] be the set of indices where u\\nand v diﬀer and let s = r −||u||1. If |S| = 1, then the result is clear so suppose |S| ≥2. If\\nthe result doesn’t hold then for all i ∈S, |vi| −|ui| > s. It follows that\\n∥v∥1 =\\nX\\ni/∈S\\n|ui| +\\nX\\ni∈S\\n|vi| >\\nX\\ni/∈S\\n|ui| +\\nX\\ni∈S\\ns + |ui| = |Suv| · s + ∥u∥1 = (|S| −1) · s + r.\\nBut we assumed that v ∈Cd,r. It follows that for any pair of points u, v in Cd,r, there is a\\nwalk, using the unit vectors as moves, that uses each move at most once. Corollary 5.10 yield\\nthat for any d ∈N, the second largest eigenvalue modulus of the heat-bath random walk on\\nCd,r with uniform as stationary distribution can be strictly bounded away from 1 for r →∞.\\n18\\nCAPRICE STANLEY AND TOBIAS WINDISCH\\nThe bound on the second largest eigenvalue in Theorem 5.8 is quite general and can be\\nimproved vastly, provided one has better control over the paths. For example, this can be\\nachieved for hyperrectangles intersected with a halfspace.\\nProposition 5.13. Let a ∈Nd\\n>0, b ∈N, F = {u ∈Nd : aT · u ≤b}, and M := {e1, . . . , ed}.\\nIf π and f are the uniform distributions on F and M respectively, then\\nλ(Hπ,f\\nF,M) ≤1 −|F|\\nd2\\nd\\nY\\ni=1\\nai\\nb .\\nProof. Observe that M is a Markov basis for F since all nodes are connected with 0 ∈F.\\nLet u, v ∈F be distinct. We ﬁrst show that there exists k ∈[d] such that uk ̸= vk and\\nu + (vk −uk)ek ∈F. If u ≤v, the statement trivially holds. Otherwise, there exists k ∈[d]\\nsuch that uk > vk and the vector obtained by replacing the kth coordinate of u by vk remains\\nin F. Now, consider for the following path between u and v: Choose the smallest index\\nk ∈[d] such that uk ̸= vk and such that u + (vk −uk) · ek ∈F and proceed recursively with\\nu + (vk −uk) and v. This gives a path pu,v between u and v of length at most d. Let Γ be\\nthe collection of all these paths. We want to apply Lemma 5.7. Thus, let x ∈F and consider\\nthe edge x →x + c · es. Let us count the paths pu,v that use that edge. Let u, v ∈F and let\\nk1, . . . , kr ∈[d] be distinct indices such that\\nu →u + (vk1 −uk1)ek1 →u + (vk1 −uk1)ek1 + (vk2 −uk2)ek2 →· · · →v\\nrepresents the path pu,v constructed by the upper rule.\\nAssume that pu,v uses the edge\\n{x, x + ces} and let kl = s and (vkl −ukl) = c. In particular,\\nu + (vk1 −uk1)ek1 + · · · + (vkl−1 −ukl−1)ekl−1 = x\\nx + (vkl −ukl)ekl + · · · + (vkr −ukr)ekr = v.\\nWe see that vkt = xkt for all t < l and that ukt = xkt for all t ≥l. In particular, vkl =\\nukl + c = xkl + c is also ﬁxed. The coordinates ukt and vkt are bounded from above by\\nb\\nakt\\nfor all t ∈[r], and hence there can be at most\\n l−1\\nY\\nt=1\\nb\\nakt\\n!\\n·\\n \\nr\\nY\\nt=l+1\\nb\\nakt\\n!\\n.\\nSince k1, . . . , kt are distinct coordinate indices, we have\\n|Γx,x+c·es|\\nHπ,f\\nF,M(x, x + c · es)\\n≤d ·\\nd\\nY\\ni=1\\nb\\nai\\n.\\nLemma 5.7 ﬁnishes the proof.\\n□\\nIn ﬁxed dimension, Proposition 5.13 leads to rapid mixing, but for d →∞, no statement\\ncan be made. In [19], it was shown that the simple walk with an additional halting probability\\non {u ∈Nd : atu ≤b} ∩{0, 1}d has mixing time in O(d4.5+ǫ). For zero-one polytopes, simple\\nand heat-bath walk coincide and we are conﬁdent that a similar statement holds without the\\nrestriction on zero-one polytopes.\\nHEAT-BATH RANDOM WALKS WITH MARKOV BASES\\n19\\nThe heat-bath random walk mixes rapidly when an augmenting Markov basis with a small\\naugmentation length is used.\\nWe think that it is interesting to question how might an\\naugmenting Markov bases be obtained and how their augmentation length can be improved.\\nQuestion 5.14. Let M be an augmenting Markov basis of A. Can we ﬁnd ﬁnitely many\\nmoves m1, . . . , mk such that the augmentation length of M∪{m1, . . . , mk} on FA,b is at most\\ndim(kerZ(A)) for all b ∈NA?\\nReferences\\n1. Stephen Baumert, Archis Ghate, Seksan Kiatsupaibul, Yanfang Shen, Robert L. Smith, and Zelda B.\\nZabinsky, Discrete Hit-and-Run for Sampling Points from Arbitrary Distributions Over Subsets of Integer\\nHyperrectangles, Operations Research 57 (2009), no. 3, 727–739.\\n2. Matthias Beck and Sinai Robins, Computing the Continuous Discretely, Springer, New York, 2007.\\n3. Mary Cryan, Martin Dyer, Leslie Ann Goldberg, Mark Jerrum, and Russell Martin, Rapidly Mixing\\nMarkov Chains for Sampling Contingency Tables with a Constant Number of Rows, SIAM Journal on\\nComputing 36 (2006), no. 1, 247–278.\\n4. Jes´us A. De Loera, Raymond Hemmecke, and Matthias K¨oppe, Algebraic and Geometric Ideas in the\\nTheory of Discrete Optimization, MPS-SIAM Series on Optimization, SIAM, Cambridge, 2013.\\n5. Jes´us A. De Loera, Raymond Hemmecke, and Jon Lee, On Augmentation Algorithms for Linear and\\nInteger-Linear Programming: From Edmonds–Karp to Bland and Beyond, SIAM Journal on Optimization\\n25 (2015), no. 4, 2494–2511.\\n6. Persi Diaconis and Bernd Sturmfels, Algebraic algorithms for sampling from conditional distributions, The\\nAnnals of statistics 26 (1998), no. 1, 363–397.\\n7. Mathias Drton, Bernd Sturmfels, and Seth Sullivant, Lectures on algebraic statistics, Oberwolfach Semi-\\nnars, vol. 39, Springer, Berlin, 2009, A Birkh¨auser book.\\n8. Martin Dyer, Catherine Greenhill, and Mario Ullrich, Structure and eigenvalues of heat-bath Markov\\nchains, Linear Algebra and its Applications 454 (2014), 57–71.\\n9. Giles Gardam, Expander Graphs and Kazhdan’ s Property (T), Bachelor’s thesis, University of Sidney,\\n2012.\\n10. Hisayuki Hara, Akimichi Takemura, and Ruriko Yoshida, On connectivity of ﬁbers with positive marginals\\nin multiple logistic regression, Journal of Multivariate Analysis (2010), 1–26.\\n11. Raymond Hemmecke and Peter N. Malkin, Computing generating sets of lattice ideals and Markov bases\\nof lattices, Journal of Symbolic Computation 44 (2009), no. 10, 1463–1476.\\n12. Roger A. Horn, Matrix Analysis, 2nd ed., Cambridge University Press, New York, 2013.\\n13. Steven S. Kim, Mixing Time of a Rook’s Walk, Undergraduate certiﬁcate paper (2012).\\n14. David A. Levin, Yuval Peres, and Elisabeth L. Wilmer, Markov chains and mixing times, American\\nMathematical Society, Providence, RI, 2009.\\n15. L´aszl´o Lov´asz, Hit-and-run mixes fast, Mathematical Programming 86 (1999), no. 3, 443–461.\\n16. L´aszl´o Lov´asz and Santosh Vempala, Hit-and-Run from a Corner, SIAM Journal on Computing 35 (2006),\\nno. 4, 985–1005.\\n17. Peter N. Malkin, Computing Markov bases, Gr¨obner bases, and extreme rays, Phd thesis, 2007, p. 223.\\n18. Cam McLeman, Peter T. Otto, John Rahmani, and Matthew Sutter, Mixing times for the Rook’s walk\\nvia path coupling, to appear in Involve (2016), 1–12.\\n19. Ben Morris and Alistair Sinclair, Random Walks on Truncated Cubes and Sampling 0-1 Knapsack Solutions,\\nSIAM Journal on Computing 34 (2004), no. 1, 195–226.\\n20. Samu Potka, Higher connectivity of ﬁber graphs of Gr¨obner bases, Journal of Algebraic Statistics 4 (2013),\\nno. 1, 93–107.\\n21. Johannes Rauh and Seth Sullivant, Lifting Markov bases and higher codimension toric ﬁber products,\\nJournal of Symbolic Computation 74 (2016), 276–307.\\n22. Andr´as Seb¨o, Hilbert Bases, Caratheodory’s Theorem and Combinatorial Optimization, (1990), 431–455.\\n20\\nCAPRICE STANLEY AND TOBIAS WINDISCH\\n23. Alistair Sinclair, Improved Bounds for Mixing Rates of Markov Chains and Multicommodity Flow, Com-\\nbinatorics, Probability and Computing 1 (1992), no. 4, 351–370.\\n24. Bernd Sturmfels, Gr¨obner bases and convex polytopes, American Mathematical Society, Providence, R.I.,\\n1996.\\n25. Seth Sullivant, Markov bases of binary graph models, Annals of Combinatorics 7 (2003), 441–466.\\n26. Santosh S. Vempala, Geometric Random Walks: A Survey, MSRI Combinatorial and Computational\\nGeometry 52 (2005), 573–612.\\n27. Tobias Windisch, Rapid mixing and Markov bases, preprint, arXiv:1505.03018 (2015), 1–18.\\nNC State University, Raleigh, NC 27695, USA\\nE-mail address: crstanl2@ncsu.edu\\nOtto-von-Guericke Universit¨at, Magdeburg, Germany\\nE-mail address: windisch@ovgu.de\\n')]\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# Arxiv data ingestion (research paper source)\n",
    "from langchain_community.document_loaders import ArxivLoader\n",
    "\n",
    "arxiv_loader = ArxivLoader(\n",
    "    query=\"1605.08386\"\n",
    ").load()\n",
    "print(arxiv_loader)\n",
    "print(len(arxiv_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sarthakagarwal/Dropbox/study material/python/genAI/Udemy GenAI/genai/lib/python3.11/site-packages/wikipedia/wikipedia.py:389: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"html.parser\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file /Users/sarthakagarwal/Dropbox/study material/python/genAI/Udemy GenAI/genai/lib/python3.11/site-packages/wikipedia/wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"html.parser\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "[Document(metadata={'title': 'Agentic AI', 'summary': 'Agentic AI is a class of artificial intelligence that focuses on autonomous systems that can make decisions and perform tasks without human intervention. The independent systems automatically respond to conditions, to produce process results. The field is closely linked to agentic automation, also known as agent-based process management systems, when applied to process automation. Applications include software development, customer support, cybersecurity and business intelligence.', 'source': 'https://en.wikipedia.org/wiki/Agentic_AI'}, page_content=\"Agentic AI is a class of artificial intelligence that focuses on autonomous systems that can make decisions and perform tasks without human intervention. The independent systems automatically respond to conditions, to produce process results. The field is closely linked to agentic automation, also known as agent-based process management systems, when applied to process automation. Applications include software development, customer support, cybersecurity and business intelligence.\\n\\n\\n== Core concept ==\\nThe core concept of agentic AI is the use of AI agents to perform automated tasks but without human intervention. While robotic process automation (RPA) and AI agents can be programmed to automate specific tasks or support rule-based decisions, the rules are usually fixed. Agentic AI operates independently, making decisions through continuous learning and analysis of external data and complex data sets. Functioning agents can require various AI techniques, such as natural language processing, machine learning (ML), and computer vision, depending on the environment.\\nParticularly, while reinforcement learning (RL) is essential in assissting agentic AI in making self-directed choices by supporting agents in learning best actions through trial and error method. Agents using RL continuously to explore their surroundings, will be given rewards or punishment for their actions, which  refines their decision-making capability over time. While Deep learning, as opposed to rule-based methods, supports Agentic AI through multi-layered neural networks to learn features from extensive and complex sets of data. RL combined with deep learning thus supports the use of AI agents to adjust dynamically, optimize procedures, and engage in complex behaviors with limited control from humans.\\n\\n\\n== History ==\\nSome scholars trace the conceptual roots of agentic AI to Alan Turing's mid-20th century work with machine intelligence and Norbert Wiener's work on feedback systems. The term agent-based process management system was used as far back as 1998 to describe the concept of using autonomous agents for business process management. The psychological principle of agency was also discussed in the 2008 work of sociologist Albert Bandura, who studied how humans can shape their environments. This research would shape how humans modeled and developed artificial intelligence agents. \\nSome additional milestones of agentic AI include IBM's Deep Blue, demonstrating how agency could work within a confined domain, advances in machine learning in the 2000s, AI being integrated into robotics, and the rise of generative AI such as OpenAI's GPT models and Salesforce's Agentforce platform.\\nIn the last decade, significant advances in AI have spurred the development of Agentic AI. Breakthroughs in deep learning, reinforcement learning, and neural networks allowed AI systems to learn on their own and make decision with minimal human guidance. Consilience of agentic AI across autonomous transportation, industrial automation, and tailored healthcare has also supported its viability. Self-driving cars use agentic AI to handle complex road scenarios, while AI-powered robot technology boosts productivity by adapting the changes in real time. Similarly, banks and financial institutions deploy agentic AI in predictive analytics and algorithmic trading, which illustrates its expanding applications in high-stakes decision-making.\\nIn 2025, research firm Forrester named agentic AI a top emerging technology for 2025.\\n\\n\\n== Applications ==\\nApplications using agentic AI include:\\n\\nSoftware development - AI coding agents can write large pieces of code, and review it. Agents can even perform non-code related tasks such as reverse engineering specifications from code.\\nCustomer support automation - AI agents can improve customer service by improving the ability of chatbots to answer a wider variety of questions, rather than having a limited set of answers pre-programmed by humans.\\nEnterprise wor\"), Document(metadata={'title': 'Intelligent agent', 'summary': 'In artificial intelligence, an intelligent agent is an entity that perceives its environment, takes actions autonomously to achieve goals, and may improve its performance through machine learning or by acquiring knowledge. Leading AI textbooks define artificial intelligence as the \"study and design of intelligent agents,\" emphasizing that goal-directed behavior is central to intelligence.\\nA specialized subset of intelligent agents, agentic AI (also known as an AI agent or simply agent), expands this concept by proactively pursuing goals, making decisions, and taking actions over extended periods, thereby exemplifying a novel form of digital agency.\\nIntelligent agents can range from simple to highly complex. A basic thermostat or control system is considered an intelligent agent, as is a human being, or any other system that meets the same criteria—such as a firm, a state, or a biome.\\nIntelligent agents operate based on an objective function, which encapsulates their goals. They are designed to create and execute plans that maximize the expected value of this function upon completion. For example, a reinforcement learning agent has a reward function, which allows programmers to shape its desired behavior. Similarly, an evolutionary algorithm\\'s behavior is guided by a fitness function.\\nIntelligent agents in artificial intelligence are closely related to agents in economics, and versions of the intelligent agent paradigm are studied in cognitive science, ethics, and the philosophy of practical reason, as well as in many interdisciplinary socio-cognitive modeling and computer social simulations.\\nIntelligent agents are often described schematically as abstract functional systems similar to computer programs. To distinguish theoretical models from real-world implementations, abstract descriptions of intelligent agents are called abstract intelligent agents. Intelligent agents are also closely related to software agents—autonomous computer programs that carry out tasks on behalf of users. They are also referred to using a term borrowed from economics: a \"rational agent\".\\n\\n', 'source': 'https://en.wikipedia.org/wiki/Intelligent_agent'}, page_content='In artificial intelligence, an intelligent agent is an entity that perceives its environment, takes actions autonomously to achieve goals, and may improve its performance through machine learning or by acquiring knowledge. Leading AI textbooks define artificial intelligence as the \"study and design of intelligent agents,\" emphasizing that goal-directed behavior is central to intelligence.\\nA specialized subset of intelligent agents, agentic AI (also known as an AI agent or simply agent), expands this concept by proactively pursuing goals, making decisions, and taking actions over extended periods, thereby exemplifying a novel form of digital agency.\\nIntelligent agents can range from simple to highly complex. A basic thermostat or control system is considered an intelligent agent, as is a human being, or any other system that meets the same criteria—such as a firm, a state, or a biome.\\nIntelligent agents operate based on an objective function, which encapsulates their goals. They are designed to create and execute plans that maximize the expected value of this function upon completion. For example, a reinforcement learning agent has a reward function, which allows programmers to shape its desired behavior. Similarly, an evolutionary algorithm\\'s behavior is guided by a fitness function.\\nIntelligent agents in artificial intelligence are closely related to agents in economics, and versions of the intelligent agent paradigm are studied in cognitive science, ethics, and the philosophy of practical reason, as well as in many interdisciplinary socio-cognitive modeling and computer social simulations.\\nIntelligent agents are often described schematically as abstract functional systems similar to computer programs. To distinguish theoretical models from real-world implementations, abstract descriptions of intelligent agents are called abstract intelligent agents. Intelligent agents are also closely related to software agents—autonomous computer programs that carry out tasks on behalf of users. They are also referred to using a term borrowed from economics: a \"rational agent\".\\n\\n\\n== Intelligent agents as the foundation of AI ==\\n\\nThe concept of intelligent agents provides a foundational lens through which to define and understand artificial intelligence. For instance, the influential textbook Artificial Intelligence: A Modern Approach (Russell & Norvig) describes:\\n\\nAgent: Anything that perceives its environment (using sensors) and acts upon it (using actuators). E.g., a robot with cameras and wheels, or a software program that reads data and makes recommendations.\\nRational Agent: An agent that strives to achieve the *best possible outcome* based on its knowledge and past experiences. \"Best\" is defined by a performance measure – a way of evaluating how well the agent is doing.\\nArtificial Intelligence (as a field): The study and creation of these rational agents.\\nOther researchers and definitions build upon this foundation. Padgham & Winikoff emphasize that intelligent agents should react to changes in their environment in a timely way, proactively pursue goals, and be flexible and robust (able to handle unexpected situations). Some also suggest that ideal agents should be \"rational\" in the economic sense (making optimal choices) and capable of complex reasoning, like having beliefs, desires, and intentions (BDI model). Kaplan and Haenlein offer a similar definition, focusing on a system\\'s ability to understand external data, learn from that data, and use what is learned to achieve goals through flexible adaptation.\\nDefining AI in terms of intelligent agents offers several key advantages:\\n\\nAvoids Philosophical Debates: It sidesteps arguments about whether AI is \"truly\" intelligent or conscious, like those raised by the Turing test or Searle\\'s Chinese Room. It focuses on behavior and goal achievement, not on replicating human thought.\\nObjective Testing: It provides a clear, scientific way to evaluate AI systems. Researchers can compare differen'), Document(metadata={'title': 'Manus (AI agent)', 'summary': 'Manus (hands in Latin) is an autonomous artificial intelligence agent developed by Chinese startup company Monica. The agent is designed to independently carry out complex online tasks without direct/continuous human guidance.\\n\\n', 'source': 'https://en.wikipedia.org/wiki/Manus_(AI_agent)'}, page_content='Manus (hands in Latin) is an autonomous artificial intelligence agent developed by Chinese startup company Monica. The agent is designed to independently carry out complex online tasks without direct/continuous human guidance.\\n\\n\\n== History ==\\nManus was founded to create artificial intelligence agents capable of operating independently, based on large language models (LLM). The official launch of Manus on March 6, 2025, drew international attention. Experts and media described Manus as a major advance because it could autonomously handle complex tasks, including writing and deploying code, without direct human intervention.\\n\\n\\n== Performance ==\\nManus is claimed as a fully autonomous AI agent, designed to handle tasks like website creation, stock analysis, travel planning, and schedule management. It has demonstrated performance on the GAIA benchmark, a test of real-world problem-solving skills, with reports indicating a score around 86.5%, potentially exceeding models like H2O.ai’s h2oGPTe Agent (65%) and OpenAI’s DeepResearch, though exact figures remain unverified due to limited public data. In practical use, Manus can compile lists, search properties, and nominate candidates, sometimes performing better than competitors like ChatGPT DeepResearch in specific tasks, but it requires user intervention for paywalls and captchas and can take longer, with task times ranging from 30 minutes to over an hour. Its transparency, displaying its process through a side panel, makes it accessible for users regardless of coding experience, and it operates asynchronously in the cloud, allowing tasks to proceed without constant supervision. But it encounters challenges with system stability, including crashes and server overload under high demand, leading to task failures and a higher failure rate than some alternatives. Access is limited, with fewer than 1% of waitlist users receiving invite codes, and its $2 per-task cost is positioned competitively but affected by reliability considerations. While recognized for its capabilities and seen as innovative by some, Manus has received scrutiny for potentially being a wrapper of existing models and for its marketing approach, as server issues and limited availability raise questions.\\n\\n\\n== References ==\\n\\n\\n== External links ==\\nOfficial website'), Document(metadata={'title': 'Artificial intelligence', 'summary': 'Artificial intelligence (AI) refers to the capability of computational systems to perform tasks typically associated with human intelligence, such as learning, reasoning, problem-solving, perception, and decision-making. It is a field of research in computer science that develops and studies methods and software that enable machines to perceive their environment and use learning and intelligence to take actions that maximize their chances of achieving defined goals. Such machines may be called AIs.\\nHigh-profile applications of AI include advanced web search engines (e.g., Google Search); recommendation systems (used by YouTube, Amazon, and Netflix); virtual assistants (e.g., Google Assistant, Siri, and Alexa); autonomous vehicles (e.g., Waymo); generative and creative tools (e.g., ChatGPT and AI art); and superhuman play and analysis in strategy games (e.g., chess and Go). However, many AI applications are not perceived as AI: \"A lot of cutting edge AI has filtered into general applications, often without being called AI because once something becomes useful enough and common enough it\\'s not labeled AI anymore.\"\\nVarious subfields of AI research are centered around particular goals and the use of particular tools. The traditional goals of AI research include learning, reasoning, knowledge representation, planning, natural language processing, perception, and support for robotics. General intelligence—the ability to complete any task performed by a human on an at least equal level—is among the field\\'s long-term goals. To reach these goals, AI researchers have adapted and integrated a wide range of techniques, including search and mathematical optimization, formal logic, artificial neural networks, and methods based on statistics, operations research, and economics. AI also draws upon psychology, linguistics, philosophy, neuroscience, and other fields.\\nArtificial intelligence was founded as an academic discipline in 1956, and the field went through multiple cycles of optimism throughout its history, followed by periods of disappointment and loss of funding, known as AI winters. Funding and interest vastly increased after 2012 when deep learning outperformed previous AI techniques. This growth accelerated further after 2017 with the transformer architecture, and by the early 2020s many billions of dollars were being invested in AI and the field experienced rapid ongoing progress in what has become known as the AI boom. The emergence of advanced generative AI in the midst of the AI boom and its ability to create and modify content exposed several unintended consequences and harms in the present and raised concerns about the risks of AI and its long-term effects in the future, prompting discussions about regulatory policies to ensure the safety and benefits of the technology.\\n\\n', 'source': 'https://en.wikipedia.org/wiki/Artificial_intelligence'}, page_content='Artificial intelligence (AI) refers to the capability of computational systems to perform tasks typically associated with human intelligence, such as learning, reasoning, problem-solving, perception, and decision-making. It is a field of research in computer science that develops and studies methods and software that enable machines to perceive their environment and use learning and intelligence to take actions that maximize their chances of achieving defined goals. Such machines may be called AIs.\\nHigh-profile applications of AI include advanced web search engines (e.g., Google Search); recommendation systems (used by YouTube, Amazon, and Netflix); virtual assistants (e.g., Google Assistant, Siri, and Alexa); autonomous vehicles (e.g., Waymo); generative and creative tools (e.g., ChatGPT and AI art); and superhuman play and analysis in strategy games (e.g., chess and Go). However, many AI applications are not perceived as AI: \"A lot of cutting edge AI has filtered into general applications, often without being called AI because once something becomes useful enough and common enough it\\'s not labeled AI anymore.\"\\nVarious subfields of AI research are centered around particular goals and the use of particular tools. The traditional goals of AI research include learning, reasoning, knowledge representation, planning, natural language processing, perception, and support for robotics. General intelligence—the ability to complete any task performed by a human on an at least equal level—is among the field\\'s long-term goals. To reach these goals, AI researchers have adapted and integrated a wide range of techniques, including search and mathematical optimization, formal logic, artificial neural networks, and methods based on statistics, operations research, and economics. AI also draws upon psychology, linguistics, philosophy, neuroscience, and other fields.\\nArtificial intelligence was founded as an academic discipline in 1956, and the field went through multiple cycles of optimism throughout its history, followed by periods of disappointment and loss of funding, known as AI winters. Funding and interest vastly increased after 2012 when deep learning outperformed previous AI techniques. This growth accelerated further after 2017 with the transformer architecture, and by the early 2020s many billions of dollars were being invested in AI and the field experienced rapid ongoing progress in what has become known as the AI boom. The emergence of advanced generative AI in the midst of the AI boom and its ability to create and modify content exposed several unintended consequences and harms in the present and raised concerns about the risks of AI and its long-term effects in the future, prompting discussions about regulatory policies to ensure the safety and benefits of the technology.\\n\\n\\n== Goals ==\\nThe general problem of simulating (or creating) intelligence has been broken into subproblems. These consist of particular traits or capabilities that researchers expect an intelligent system to display. The traits described below have received the most attention and cover the scope of AI research.\\n\\n\\n=== Reasoning and problem-solving ===\\nEarly researchers developed algorithms that imitated step-by-step reasoning that humans use when they solve puzzles or make logical deductions. By the late 1980s and 1990s, methods were developed for dealing with uncertain or incomplete information, employing concepts from probability and economics.\\nMany of these algorithms are insufficient for solving large reasoning problems because they experience a \"combinatorial explosion\": They become exponentially slower as the problems grow. Even humans rarely use the step-by-step deduction that early AI research could model. They solve most of their problems using fast, intuitive judgments. Accurate and efficient reasoning is an unsolved problem.\\n\\n\\n=== Knowledge representation ===\\n\\nKnowledge representation and knowledge engineering allow AI programs to answer questions in'), Document(metadata={'title': 'ChatGPT Deep Research', 'summary': 'Deep Research is an AI agent integrated into ChatGPT, which generates cited reports on a user-specified topic by autonomously browsing the web for 5 to 30 minutes.', 'source': 'https://en.wikipedia.org/wiki/ChatGPT_Deep_Research'}, page_content='Deep Research is an AI agent integrated into ChatGPT, which generates cited reports on a user-specified topic by autonomously browsing the web for 5 to 30 minutes.\\n\\n\\n== Agent ==\\nIt can interpret and analyze text, images and PDFs. It is based on a specialized version of OpenAI\\'s o3 model.\\nDeep Research scored 26.6% on the \"Humanity\\'s Last Exam\" benchmark, surpassing rivals like DeepSeek\\'s model R1 (9.4%) and GPT-4o (3.3%).\\nAccording to OpenAI, Deep Research sometimes makes factual hallucinations or incorrect inferences, can have difficulty distinguishing authoritative sources from rumors, and may not accurately convey uncertainty.\\nDeep Research is currently offered to ChatGPT Pro subscribers ($200/month), who receive 100 queries per month and for Plus, Team and Enterprise users with 10 queries per month. \\n\\n\\n== References =='), Document(metadata={'title': 'Salesforce', 'summary': \"Salesforce, Inc. is an American cloud-based software company headquartered in San Francisco, California. It provides applications focused on sales, customer service, marketing automation, e-commerce, analytics, artificial intelligence, and application development.\\nFounded by former Oracle executive Marc Benioff in March 1999, Salesforce grew quickly, making its initial public offering in 2004. As of September 2022, Salesforce is the 61st largest company in the world by market cap with a value of nearly US$153 billion. It became the world's largest enterprise applications firm in 2022. Salesforce ranked 491st on the 2023 edition of the Fortune 500, making $31.352 billion in revenues. Since 2020, Salesforce has also been a component of the Dow Jones Industrial Average.\", 'source': 'https://en.wikipedia.org/wiki/Salesforce'}, page_content='Salesforce, Inc. is an American cloud-based software company headquartered in San Francisco, California. It provides applications focused on sales, customer service, marketing automation, e-commerce, analytics, artificial intelligence, and application development.\\nFounded by former Oracle executive Marc Benioff in March 1999, Salesforce grew quickly, making its initial public offering in 2004. As of September 2022, Salesforce is the 61st largest company in the world by market cap with a value of nearly US$153 billion. It became the world\\'s largest enterprise applications firm in 2022. Salesforce ranked 491st on the 2023 edition of the Fortune 500, making $31.352 billion in revenues. Since 2020, Salesforce has also been a component of the Dow Jones Industrial Average.\\n\\n\\n== History ==\\n\\nSalesforce was founded on March 8, 1999 by former Oracle executive Marc Benioff, together with Parker Harris, Dave Moellenhoff, and Frank Dominguez as a software-as-a-service (SaaS) company. The first prototype of Salesforce was launched in November 1999.\\nTwo of Salesforce\\'s earliest investors were Larry Ellison, the co-founder and first CEO of Oracle, and Halsey Minor, the founder of CNET.\\nSalesforce was severely affected by the dot-com bubble bursting at the beginning of the new millennium, resulting in the company laying off 20% of its workforce. Despite its losses, Salesforce continued strong during the early 2000s. Salesforce also gained notability during this period for its \"the end of software\" tagline and marketing campaign, and even hired actors to hold up signs with its slogan outside a Siebel Systems conference. Salesforce\\'s revenue continued to increase from 2000 to 2003, with 2003\\'s revenue skyrocketing from $5.4 million in the fiscal year 2001 to over $100 million by December 2003.\\nIn 2003, Salesforce held its first annual Dreamforce conference in San Francisco.\\nIn June 2004, the company had its initial public offering on the New York Stock Exchange under the stock symbol CRM and raised US$110 million. In 2006, Salesforce launched Idea Exchange, a platform that allows customers to connect with company product managers.\\nIn 2009, Salesforce passed $1 billion in annual revenue. Also, in 2009, the company launched Service Cloud, an application that helps companies manage service conversations about their products and services.\\nIn 2014, the company released Trailhead, a free online learning platform. In October 2014, Salesforce announced the development of its Customer Success Platform. In September 2016, Salesforce announced the launch of Einstein, an artificial intelligence platform that supports several of Salesforce\\'s cloud services. It reportedly acquired a 20-year license to be the exclusive business-oriented software company allowed to use Albert Einstein\\'s likeness for $20 million.\\nSalesforce launched the Sustainability Cloud (Net Zero Cloud as of 2022), which is used by companies to track progress towards achieving their net zero emissions goals.\\nIn 2020, Salesforce joined the Dow Jones Industrial Average, replacing energy giant and Standard Oil-descendant ExxonMobil. Salesforce\\'s ascension to the Dow Jones was concurrent with that of Amgen and Honeywell. Because the Dow Jones factors its components by market price, Salesforce was the largest technology component of the index at its accession.\\nAcross 2020 and 2021, Salesforce saw some notable leadership changes; in February 2020, co-chief executive officer Keith Block stepped down from his position in the company. Marc Benioff remained as chairman and chief executive officer. In February 2021, Amy Weaver, previously the chief legal officer, became CFO. Former CFO Mark Hawkins announced that he would be retiring in October. In November 2021, Bret Taylor was named vice chair and co-CEO of the company.\\nIn December 2020, it was announced that Salesforce would acquire Slack for $27.7 billion, its largest acquisition to date. The acquisition closed in July 2021. Journalists covering th'), Document(metadata={'title': 'Meredith Whittaker', 'summary': \"Meredith Whittaker is the president of the Signal Foundation and serves on its board of directors. She was formerly the Minderoo Research Professor at New York University (NYU), and chief advisor and former faculty director & co founder of the AI Now Institute. She also served as a senior advisor on AI to Chair Lina Khan at the Federal Trade Commission and was listed among the 100 most influential people in AI by TIME magazine in 2023. Whittaker was employed at Google for 13 years, where she founded Google's Open Research group and co-founded the M-Lab. In 2018, she was a core organizer of the Google Walkouts and resigned from the company in July 2019.\\n\\n\", 'source': 'https://en.wikipedia.org/wiki/Meredith_Whittaker'}, page_content='Meredith Whittaker is the president of the Signal Foundation and serves on its board of directors. She was formerly the Minderoo Research Professor at New York University (NYU), and chief advisor and former faculty director & co founder of the AI Now Institute. She also served as a senior advisor on AI to Chair Lina Khan at the Federal Trade Commission and was listed among the 100 most influential people in AI by TIME magazine in 2023. Whittaker was employed at Google for 13 years, where she founded Google\\'s Open Research group and co-founded the M-Lab. In 2018, she was a core organizer of the Google Walkouts and resigned from the company in July 2019.\\n\\n\\n== Early life and education ==\\nWhittaker completed her bachelor\\'s degree in rhetoric and English literature at University of California, Berkeley.\\n\\n\\n== Research and career ==\\nWhittaker is the president of the Signal foundation and serves on their board of directors. She was formerly the Minderoo Research Professor at NYU, and the Faculty Director of NYU’s AI Now Institute. \\nWhittaker was a speaker at the 2018 World Summit on AI. She has written for the American Civil Liberties Union.\\nWhittaker co-founded M-Lab, a globally distributed network measurement system that provides the world’s largest source of open data on Internet performance. She has also worked extensively on issues of data validation, privacy, the social implications of artificial intelligence, the political economy of tech, and labor movements in the context of tech and the tech industry. She has spoken out about the need for privacy and against weakening encryption. She has advised the White House, the FCC, the FTC, the City of New York, the European Parliament, and many other governments and civil society organizations on artificial intelligence, Internet policy, measurement, privacy, and security.\\n\\n\\n=== Google ===\\nShe joined Google in 2006. She founded Google Open Research which collaborated with the open source and academic communities on issues related to net neutrality measurement, privacy, security, and the social consequences of artificial intelligence.\\nIn 2018, Whittaker was one of the core organizers of the Google Walkouts, with over 20,000 Google employees walking out internationally to protest Google\\'s culture when it comes to claims of sexual misconduct and citizen surveillance. They released a series of demands, some of which were met by Google.\\nThe walkout was prompted by Google\\'s reported $90 million payout to vice president Andy Rubin who had been accused of sexual misconduct, and the company\\'s involvement with Project Maven, against which more than three thousand Google employees signed a petition. Τhe project was established by a contract between the US military and Google, through which Google was to develop machine vision technologies for the US drone program. Following the protests, Google did not renew the Maven contract.\\nWhittaker was part of the movement that called for Google to rethink their AI ethics council after the appointment of Kay Coles James, the president of The Heritage Foundation who has fought against LGBT protections and advocated for Donald Trump’s proposed border wall. Whittaker claimed that she faced retaliation from Google, and wrote in an open letter that she had been told by the company to \"abandon her work\" on enforcing ethics in technology at the AI Now Institute.\\n\\n\\n=== AI Now ===\\nWhittaker is current chief advisor and former faculty director & co-founder of the AI Now Institute at NYU, a leading university institute dedicated to researching the social implications of artificial intelligence and related technologies which she started with Kate Crawford in 2017 after a symposium hosted by the White House. AI Now is partnered with the New York University Tandon School of Engineering, New York University Center for Data Science and Partnership on AI. They have produced annual reports that examine the social implications of artificial intelligence, including bias, righ'), Document(metadata={'title': 'Perplexity AI', 'summary': 'Perplexity AI, or simply Perplexity, is a web search engine that uses a large language model to process queries and synthesize responses based on web search results. With a conversational approach, Perplexity allows users to ask follow-up questions and receive answers with citations to its sources from the internet.\\nPerplexity AI, Inc. was founded as a privately held company in 2022 by Aravind Srinivas, Denis Yarats, Johnny Ho, and Andy Konwinski. It launched its flagship search engine on December 7, 2022, and has since released a Google Chrome extension and an app for iOS and Android. As of December 2024, the company was valued at US$9 billion. It currently has around 100 employees, and is headquartered in San Francisco, California, United States.\\nBuilt on Microsoft Azure infrastructure, Perplexity AI uses Microsoft Bing to research sources on the web. A free and public version of Perplexity employs the GPT-3.5 large language model by OpenAI, while the paid Pro subscription allows users to choose from a variety of more advanced models, among other features.\\n\\n', 'source': 'https://en.wikipedia.org/wiki/Perplexity_AI'}, page_content=\"Perplexity AI, or simply Perplexity, is a web search engine that uses a large language model to process queries and synthesize responses based on web search results. With a conversational approach, Perplexity allows users to ask follow-up questions and receive answers with citations to its sources from the internet.\\nPerplexity AI, Inc. was founded as a privately held company in 2022 by Aravind Srinivas, Denis Yarats, Johnny Ho, and Andy Konwinski. It launched its flagship search engine on December 7, 2022, and has since released a Google Chrome extension and an app for iOS and Android. As of December 2024, the company was valued at US$9 billion. It currently has around 100 employees, and is headquartered in San Francisco, California, United States.\\nBuilt on Microsoft Azure infrastructure, Perplexity AI uses Microsoft Bing to research sources on the web. A free and public version of Perplexity employs the GPT-3.5 large language model by OpenAI, while the paid Pro subscription allows users to choose from a variety of more advanced models, among other features.\\n\\n\\n== History ==\\n\\nIn August 2022, Perplexity AI, Inc. was founded by Aravind Srinivas, Denis Yarats, Johnny Ho, and Andy Konwinski, engineers with backgrounds in back-end systems, artificial intelligence (AI) and machine learning.\\nIn February 2023, Perplexity reported 2 million unique visitors. By April 2024, Perplexity had raised $165 million in funding, valuing the company at over $1 billion. As of December 2024, Perplexity closed a $500 million round of funding that elevated its valuation to $9 billion. Investors in Perplexity AI have included Jeff Bezos, Tobias Lütke, Nat Friedman, Nvidia, and Databricks. In July 2024, Perplexity announced the launch of a new publishers' program to share ad revenue with partners.\\nIn October 2024, The New York Times sent a cease-and-desist notice to Perplexity to stop accessing and using NYT content, claiming that Perplexity is violating its copyright by scraping data from its website.\\nOn January 18, 2025, the day before the impending U.S. ban on Chinese social media app TikTok, Perplexity submitted a proposal for a merger with TikTok US.\\n\\n\\n== Services ==\\n\\nPerplexity works on a freemium model. It also offers a paid enterprise version.\\nThe free model used the company's standalone LLM based on GPT-3.5 with browsing. Perplexity summarizes the search results and produces text with inline citations and also enables users to use Pages to generate customizable web pages and research presentations based on user prompts.\\nThe subscription-based Pro version provides access to an API where users can search both internal files and web content. It also has access to the OpenAI family models (including OpenAI o3-mini, GPT-4o and GPT-4.5), Claude 3.7 Sonnet, Gemini Flash 2.0, Llama 3, Deepseek R1. Its developer, Perplexity AI, Inc., is based in San Francisco, California.\\n\\n\\n=== Shopping hub ===\\nOn November 18, 2024, Perplexity launched its shopping hub to attract users, backed by Amazon and leading AI chipmaker Nvidia. This will give users product cards showing relevant items in response to asked questions about shopping.\\n\\n\\n=== Internal Knowledge Search ===\\nInternal Knowledge Search enables Pro and Enterprise Pro users to simultaneously search across web content and internal documents. Users can upload and search through Excel, Word, PDF, and other common file formats. Enterprise Pro users have a limit of 500 files for upload and indexing.\\n\\n\\n=== Finance ===\\nIn October 2024, Perplexity AI introduced new finance-related features, including looking up stock prices and company earnings data. The tool provides real-time stock quotes and price tracking, industry peer comparisons and basic financial analysis tools. The platform sources its financial data from Financial Modeling Prep (FMP) to ensure accuracy.\\n\\n\\n=== Assistant for Android ===\\nIn January 2025, Perplexity launched the Perplexity Assistant, an AI-powered tool designed to enhance the functionality of \"), Document(metadata={'title': 'OpenAI', 'summary': 'OpenAI, Inc. is an American artificial intelligence (AI) research organization founded in December 2015 and headquartered in San Francisco, California. It aims to develop \"safe and beneficial\" artificial general intelligence (AGI), which it defines as \"highly autonomous systems that outperform humans at most economically valuable work\". As a leading organization in the ongoing AI boom, OpenAI is known for the GPT family of large language models, the DALL-E series of text-to-image models, and a text-to-video model named Sora. Its release of ChatGPT in November 2022 has been credited with catalyzing widespread interest in generative AI.\\nThe organization consists of the non-profit OpenAI, Inc., registered in Delaware, and its for-profit subsidiary introduced in 2019, OpenAI Global, LLC. Its stated mission is to ensure that AGI \"benefits all of humanity\". Microsoft owns roughly 49% of OpenAI\\'s equity, having invested US$13 billion. It also provides computing resources to OpenAI through its cloud platform, Microsoft Azure.\\nIn 2023 and 2024, OpenAI faced multiple lawsuits for alleged copyright infringement against authors and media companies whose work was used to train some of OpenAI\\'s products. In November 2023, OpenAI\\'s board removed Sam Altman as CEO, citing a lack of confidence in him, but reinstated him five days later following a reconstruction of the board. Throughout 2024, roughly half of then-employed AI safety researchers left OpenAI, citing the company\\'s prominent role in an industry-wide problem.', 'source': 'https://en.wikipedia.org/wiki/OpenAI'}, page_content='OpenAI, Inc. is an American artificial intelligence (AI) research organization founded in December 2015 and headquartered in San Francisco, California. It aims to develop \"safe and beneficial\" artificial general intelligence (AGI), which it defines as \"highly autonomous systems that outperform humans at most economically valuable work\". As a leading organization in the ongoing AI boom, OpenAI is known for the GPT family of large language models, the DALL-E series of text-to-image models, and a text-to-video model named Sora. Its release of ChatGPT in November 2022 has been credited with catalyzing widespread interest in generative AI.\\nThe organization consists of the non-profit OpenAI, Inc., registered in Delaware, and its for-profit subsidiary introduced in 2019, OpenAI Global, LLC. Its stated mission is to ensure that AGI \"benefits all of humanity\". Microsoft owns roughly 49% of OpenAI\\'s equity, having invested US$13 billion. It also provides computing resources to OpenAI through its cloud platform, Microsoft Azure.\\nIn 2023 and 2024, OpenAI faced multiple lawsuits for alleged copyright infringement against authors and media companies whose work was used to train some of OpenAI\\'s products. In November 2023, OpenAI\\'s board removed Sam Altman as CEO, citing a lack of confidence in him, but reinstated him five days later following a reconstruction of the board. Throughout 2024, roughly half of then-employed AI safety researchers left OpenAI, citing the company\\'s prominent role in an industry-wide problem.\\n\\n\\n== History ==\\n\\n\\n=== 2015–2018: Non-profit beginnings ===\\n\\nIn December 2015, OpenAI was founded by Sam Altman, Elon Musk, Ilya Sutskever, Greg Brockman, Trevor Blackwell, Vicki Cheung, Andrej Karpathy, Durk Kingma, John Schulman, Pamela Vagata, and Wojciech Zaremba, with Sam Altman and Elon Musk as the co-chairs. A total of $1 billion in capital was pledged by Sam Altman, Greg Brockman, Elon Musk, Reid Hoffman, Jessica Livingston, Peter Thiel, Amazon Web Services (AWS), Infosys, and YC Research. The actual collected total amount of contributions was only $130 million until 2019. According to an investigation led by TechCrunch, while YC Research never contributed any funds, Open Philanthropy contributed $30 million and another $15 million in verifiable donations were traced back to Musk. OpenAI later stated that Musk\\'s contributions totaled less than $45 million. The organization stated it would \"freely collaborate\" with other institutions and researchers by making its patents and research open to the public. OpenAI was initially run from Brockman\\'s living room. It was later headquartered at the Pioneer Building in the Mission District, San Francisco.\\nAccording to Wired, Brockman met with Yoshua Bengio, one of the \"founding fathers\" of deep learning, and drew up a list of the \"best researchers in the field\". Brockman was able to hire nine of them as the first employees in December 2015. In 2016, OpenAI paid corporate-level (rather than nonprofit-level) salaries, but did not pay AI researchers salaries comparable to those of Facebook or Google.\\nMicrosoft\\'s Peter Lee stated that the cost of a top AI researcher exceeds the cost of a top NFL quarterback prospect. OpenAI\\'s potential and mission drew these researchers to the firm; a Google employee said he was willing to leave Google for OpenAI \"partly because of the very strong group of people and, to a very large extent, because of its mission.\" Brockman stated that \"the best thing that I could imagine doing was moving humanity closer to building real AI in a safe way.\" OpenAI co-founder Wojciech Zaremba stated that he turned down \"borderline crazy\" offers of two to three times his market value to join OpenAI instead.\\nIn April 2016, OpenAI released a public beta of \"OpenAI Gym\", its platform for reinforcement learning research. Nvidia gifted its first DGX-1 supercomputer to OpenAI in August 2016 to help it train larger and more complex AI models with the capability of reducing proc')]\n"
     ]
    }
   ],
   "source": [
    "# Wikipedia loader\n",
    "from langchain_community.document_loaders import WikipediaLoader\n",
    "wiki_docs = WikipediaLoader(\n",
    "    query = \"Agentic AI\",\n",
    "    load_max_docs= 10\n",
    ").load()\n",
    "print(len(wiki_docs))\n",
    "print(wiki_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
